### Simulation with sensitivity analysis
# Load necessary libraries
library(nnet)     # For multinomial logistic regression
library(keras)    # For deep learning models
library(tensorflow)  # For tensorflow backend
library(MatchIt)   # For matching techniques
library(survival)
library(dplyr)
library(MASS)      # For correlated data generation
library(survminer)
library(ggplot2)
library(survcomp)  # For C-index calculation
library(kableExtra)
library(copula)    # For Gaussian copula transformation

# 1. Generate Highly Correlated Synthetic Data
set.seed(42)

n_samples <- 1000  # Number of data points (rows)
n_features <- 6    # Number of features (columns)

base_corr <- 0.8  # Base correlation between features
corr_matrix <- matrix(base_corr, nrow = n_features, ncol = n_features)  # Create correlation matrix
diag(corr_matrix) <- 1  # Set diagonal elements to 1 (no self-correlation)

mean_values <- rep(0, n_features)  # Mean of 0 for all features
x_data <- mvrnorm(n = n_samples, mu = mean_values, Sigma = corr_matrix)  # Generate synthetic correlated data

# Convert to data frame and label columns
x_data <- as.data.frame(x_data)
colnames(x_data) <- paste0("X", 1:n_features)

# 2. Add Nonlinear Transformations to Some Columns
x_data$X101 <- sin(x_data$X1) * cos(x_data$X2)
x_data$X102 <- log(abs(x_data$X3) + 1) * exp(-x_data$X4)
x_data$X103 <- (x_data$X5)^2 + (x_data$X6)^3

# 3. Generate Survival Data with Weibull Distribution
shape_param <- 2  # Shape parameter for Weibull distribution
scale_param <- exp(0.5 * x_data$X1 + 0.3 * x_data$X2)  # Scale parameter depends on X1 and X2

survival_times <- rweibull(n_samples, shape = shape_param, scale = scale_param)  # Generate survival times
event_prob <- runif(n_samples)  # Generate random probabilities for event type
event_type <- ifelse(event_prob < 0.5, 1, 2)  # Assign event types (1 or 2)

censoring_times <- runif(n_samples, min = 0, max = max(survival_times) * 0.8)  # Simulate censoring times
observed_times <- pmin(survival_times, censoring_times)  # Observed time is the minimum of survival and censoring times
observed_event <- ifelse(survival_times <= censoring_times, event_type, 0)  # Event indicator (0 if censored, 1 or 2 for events)

synthetic_data <- cbind(x_data, time = observed_times, event = observed_event)  # Combine features and time/event data
synthetic_data <- as.data.frame(synthetic_data)

# 4. Multinomial Logistic Regression for Propensity Scores
synthetic_data$event <- as.factor(synthetic_data$event)  # Ensure event column is a factor

psm_model_multi <- nnet::multinom(event ~ ., data = synthetic_data)  # Fit multinomial logistic regression for propensity scores
prop_scores <- predict(psm_model_multi, type = "probs")  # Predict the propensity scores

# 5. Inverse Probability Treatment Weights (IPTW) and Horvitz-Thompson (HT) Weights
iptw_weights_1 <- 1 / prop_scores[, 1]
iptw_weights_2 <- 1 / prop_scores[, 2]

ht_weights <- 1 / (prop_scores[cbind(1:n_samples, synthetic_data$event)])  # HT weights

sample_weights <- ifelse(synthetic_data$event == 1, iptw_weights_1, iptw_weights_2)  # Sample weights for IPTW

# 6. Prepare Data for Training
X_train <- as.matrix(synthetic_data[, 1:n_features])  # Features
y_train <- synthetic_data$time  # Survival times

# 7. Gaussian Copula Transformation
copula_model <- normalCopula(dim = n_features, dispstr = "un")  # Define Gaussian copula
pseudo_obs <- pobs(X_train)  # Transform features to pseudo-observations (uniform margins)

fit_result <- fitCopula(copula_model, pseudo_obs, method = "ml")  # Fit the copula

# Simulate Copula-Transformed Features
copula_transformed_features <- rCopula(n_samples, fit_result@copula)  # Simulate from the fitted copula

# Reshape for LSTM input
X_train_rnn <- array(copula_transformed_features, dim = c(n_samples, 1, n_features))  # Timesteps = 1 for RNN

# 8. Define and Train LSTM Model with HT Weights (Horvitz-Thompson Weights)
rnn_model_ht <- keras_model_sequential() %>%
  layer_lstm(units = 64, return_sequences = FALSE, input_shape = c(1, n_features)) %>%
  layer_dense(units = 1, activation = "linear")

rnn_model_ht %>% compile(loss = "mean_squared_error", optimizer = optimizer_adam(), metrics = c("mae"))

# Train the LSTM model using the copula-transformed features and HT weights
rnn_model_ht %>% fit(X_train_rnn, y_train, epochs = 10, batch_size = 64, sample_weight = ht_weights)

# Get Treatment Effect Estimate from LSTM (HT)
rnn_pred_ht <- predict(rnn_model_ht, X_train_rnn)
rnn_ate_ht <- mean(rnn_pred_ht)  # Average of the predictions as the treatment effect estimate

# 9. Define and Train LSTM Model with IPTW Weights (Inverse Probability of Treatment Weights)
rnn_model_iptw <- keras_model_sequential() %>%
  layer_lstm(units = 64, return_sequences = FALSE, input_shape = c(1, n_features)) %>%
  layer_dense(units = 1, activation = "linear")

rnn_model_iptw %>% compile(loss = "mean_squared_error", optimizer = optimizer_adam(), metrics = c("mae"))

# Train the LSTM model using the copula-transformed features and IPTW weights
rnn_model_iptw %>% fit(X_train_rnn, y_train, epochs = 10, batch_size = 64, sample_weight = sample_weights)

# Get Treatment Effect Estimate from LSTM (IPTW)
rnn_pred_iptw <- predict(rnn_model_iptw, X_train_rnn)
rnn_ate_iptw <- mean(rnn_pred_iptw)  # Average of the predictions as the treatment effect estimate

# 10. Propensity Score Matching (PSM) for Treatment Effect Estimate
# Modify event variable to be binary (e.g., 1 for event type 1, and 0 for event type 2)
synthetic_data$event_binary <- ifelse(synthetic_data$event == 1, 1, 0)  # Convert event to binary

# Apply nearest neighbor matching
psm_model <- matchit(event_binary ~ ., data = synthetic_data, method = "nearest")  # Apply nearest neighbor matching
psm_matched_data <- match.data(psm_model)  # Get the matched data
psm_ate <- mean(psm_matched_data$time)  # Average treatment effect from PSM

# Function to calculate Bootstrap CI
bootstrap_ci <- function(model_preds, n_bootstrap = 1000, conf_level = 0.95) {
  bootstrap_samples <- replicate(n_bootstrap, sample(model_preds, replace = TRUE))
  bootstrap_means <- apply(bootstrap_samples, 2, mean)
  ci_lower <- quantile(bootstrap_means, (1 - conf_level) / 2)
  ci_upper <- quantile(bootstrap_means, 1 - (1 - conf_level) / 2)
  return(c(ci_lower, ci_upper))
}

# Calculate Confidence Intervals for the models
rnn_ate_ht_ci <- bootstrap_ci(rnn_pred_ht)
rnn_ate_iptw_ci <- bootstrap_ci(rnn_pred_iptw)
psm_ate_ci <- bootstrap_ci(psm_matched_data$time)
logit_preds <- predict(psm_model_multi, type = "probs")
logit_ate_ci <- bootstrap_ci(logit_preds[, 1])  # For the first event


# Function to perturb the data
perturb_data <- function(data, perturb_rate = 0.1) {
  n_samples <- nrow(data)
  n_features <- ncol(data)
  perturbation <- rnorm(n_samples * n_features, mean = 0, sd = perturb_rate)
  perturbed_data <- data + matrix(perturbation, nrow = n_samples, ncol = n_features)
  return(perturbed_data)
}

# Perturb the data with 10% perturbation
X_train_perturbed <- perturb_data(X_train, perturb_rate = 0.1)
X_train_rnn_perturbed <- array(perturb_data(copula_transformed_features, perturb_rate = 0.1), dim = c(n_samples, 1, n_features))

# Train LSTM Model with HT Weights on perturbed data
rnn_model_ht %>% fit(X_train_rnn_perturbed, y_train, epochs = 10, batch_size = 64, sample_weight = ht_weights)
rnn_pred_ht_perturbed <- predict(rnn_model_ht, X_train_rnn_perturbed)
rnn_ate_ht_perturbed <- mean(rnn_pred_ht_perturbed)  # Average of the predictions as the treatment effect estimate

# Train LSTM Model with IPTW Weights on perturbed data
rnn_model_iptw %>% fit(X_train_rnn_perturbed, y_train, epochs = 10, batch_size = 64, sample_weight = sample_weights)
rnn_pred_iptw_perturbed <- predict(rnn_model_iptw, X_train_rnn_perturbed)
rnn_ate_iptw_perturbed <- mean(rnn_pred_iptw_perturbed)  # Average of the predictions as the treatment effect estimate

# Propensity Score Matching (PSM) on perturbed data
synthetic_data_perturbed <- synthetic_data
synthetic_data_perturbed[, 1:n_features] <- perturb_data(synthetic_data[, 1:n_features], perturb_rate = 0.1)
psm_model_perturbed <- matchit(event_binary ~ ., data = synthetic_data_perturbed, method = "nearest")
psm_matched_data_perturbed <- match.data(psm_model_perturbed)
psm_ate_perturbed <- mean(psm_matched_data_perturbed$time)  # Average treatment effect from PSM

# Logistic Regression on perturbed data
psm_model_multi_perturbed <- nnet::multinom(event ~ ., data = synthetic_data_perturbed)
logit_preds_perturbed <- predict(psm_model_multi_perturbed, type = "probs")
logit_ate_perturbed <- mean(logit_preds_perturbed[, 1])  # For the first event

# Calculate Confidence Intervals for the perturbed models
rnn_ate_ht_perturbed_ci <- bootstrap_ci(rnn_pred_ht_perturbed)
rnn_ate_iptw_perturbed_ci <- bootstrap_ci(rnn_pred_iptw_perturbed)
psm_ate_perturbed_ci <- bootstrap_ci(psm_matched_data_perturbed$time)
logit_ate_perturbed_ci <- bootstrap_ci(logit_preds_perturbed[, 1])  # For the first event

# Print Results for Perturbed Models
ate_results_perturbed <- data.frame(
  Method = c("Copula RNN with HT (Perturbed)", "Copula RNN with IPTW (Perturbed)", 
             "PSM (Perturbed)", "Logistic Regression (Perturbed)"),
  Estimate = c(rnn_ate_ht_perturbed, rnn_ate_iptw_perturbed, psm_ate_perturbed, logit_ate_perturbed),
  LowerCI = c(rnn_ate_ht_perturbed_ci[1], rnn_ate_iptw_perturbed_ci[1], psm_ate_perturbed_ci[1], logit_ate_perturbed_ci[1]),
  UpperCI = c(rnn_ate_ht_perturbed_ci[2], rnn_ate_iptw_perturbed_ci[2], psm_ate_perturbed_ci[2], logit_ate_perturbed_ci[2])
)

# Print Perturbed Results
print(ate_results_perturbed)

# Visualize Perturbed Results with Error Bars for CIs
ggplot(ate_results_perturbed, aes(x = Method, y = Estimate)) +
  geom_point() +  # Plot the point estimates
  geom_errorbar(aes(ymin = LowerCI, ymax = UpperCI), width = 0.2) +  # Add error bars for the CI
  labs(title = "Comparison of Treatment Effect Estimates (Perturbed Models) with Confidence Intervals",
       y = "Estimated Treatment Effect (ATE)",
       x = "Method") +
theme_minimal()


# Combine original and perturbed results into one data frame for tabular display
ate_results_combined <- data.frame(
  Method = c("Copula RNN with HT", "Copula RNN with HT (Perturbed)", 
             "Copula RNN with IPTW", "Copula RNN with IPTW (Perturbed)", 
             "PSM", "PSM (Perturbed)", 
             "Logistic Regression", "Logistic Regression (Perturbed)"),
  Estimate = c(rnn_ate_ht, rnn_ate_ht_perturbed, 
               rnn_ate_iptw, rnn_ate_iptw_perturbed, 
               psm_ate, psm_ate_perturbed, 
               logit_ate, logit_ate_perturbed),
  LowerCI = c(rnn_ate_ht_ci[1], rnn_ate_ht_perturbed_ci[1], 
              rnn_ate_iptw_ci[1], rnn_ate_iptw_perturbed_ci[1], 
              psm_ate_ci[1], psm_ate_perturbed_ci[1], 
              logit_ate_ci[1], logit_ate_perturbed_ci[1]),
  UpperCI = c(rnn_ate_ht_ci[2], rnn_ate_ht_perturbed_ci[2], 
              rnn_ate_iptw_ci[2], rnn_ate_iptw_perturbed_ci[2], 
              psm_ate_ci[2], psm_ate_perturbed_ci[2], 
              logit_ate_ci[2], logit_ate_perturbed_ci[2])
)

# Display the table
ate_results_combined %>%
  kable("html", caption = "Comparison of Treatment Effect Estimates (Original vs. Perturbed Models)") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
ate_results_combined


### Real Data with sensitivity analysis
library(nnet)      # For multinomial logistic regression
library(keras)     # For deep learning models
library(tensorflow)  # For tensorflow backend
library(MatchIt)   # For matching techniques
library(survival)
library(dplyr)
library(MASS)      # For correlated data generation
library(survminer)
library(ggplot2)
library(survcomp)  # For C-index calculation
library(kableExtra)
library(copula)    # For Gaussian copula transformation
library(boot)      # For bootstrapping

# 1. Load the compas-scores.csv dataset
compas_data <- read.csv("compas-scores.csv")

# Preview the dataset
head(compas_data)

# 2. Preprocess the data
# Select features and the treatment variable (e.g., 'is_recid' for recidivism prediction)
# Assume 'is_recid' is the treatment indicator and 'decile_score' is the outcome
# Add more variables as necessary based on the dataset columns

# Define features for the model
features <- c("age", "sex", "race", "priors_count", "decile_score")
compas_data <- compas_data[complete.cases(compas_data[, features]), ]  # Remove missing values

# Convert 'is_recid' to a factor for logistic regression
compas_data$is_recid <- as.factor(compas_data$is_recid)

# 3. Generate Propensity Scores using Multinomial Logistic Regression
psm_model_multi <- nnet::multinom(is_recid ~ age + sex + race + priors_count + decile_score, data = compas_data)
prop_scores <- predict(psm_model_multi, type = "probs")

# 4. Calculate Inverse Probability Treatment Weights (IPTW) and Horvitz-Thompson (HT) Weights
iptw_weights_1 <- 1 / prop_scores[, 1]
iptw_weights_2 <- 1 / prop_scores[, 2]
ht_weights <- 1 / (prop_scores[cbind(1:nrow(compas_data), as.numeric(compas_data$is_recid))])

# Sample weights for IPTW
sample_weights <- ifelse(compas_data$is_recid == 1, iptw_weights_1, iptw_weights_2)

# 5. Prepare Data for Training
X_train <- as.matrix(compas_data[, features])  # Features
y_train <- compas_data$decile_score  # Outcome variable

# 6. Gaussian Copula Transformation
copula_model <- normalCopula(dim = length(features), dispstr = "un")  # Define Gaussian copula
pseudo_obs <- pobs(X_train)  # Transform features to pseudo-observations (uniform margins)

fit_result <- fitCopula(copula_model, pseudo_obs, method = "ml")  # Fit the copula

# Simulate Copula-Transformed Features
copula_transformed_features <- rCopula(nrow(compas_data), fit_result@copula)  # Simulate from the fitted copula

# Reshape for LSTM input
X_train_rnn <- array(copula_transformed_features, dim = c(nrow(compas_data), 1, length(features)))  # Timesteps = 1 for RNN

# 7. Define and Train LSTM Model with HT Weights (Horvitz-Thompson Weights)
rnn_model_ht <- keras_model_sequential() %>%
  layer_lstm(units = 64, return_sequences = FALSE, input_shape = c(1, length(features))) %>%
  layer_dense(units = 1, activation = "linear")

rnn_model_ht %>% compile(loss = "mean_squared_error", optimizer = optimizer_adam(), metrics = c("mae"))

# Train the LSTM model using the copula-transformed features and HT weights
rnn_model_ht %>% fit(X_train_rnn, y_train, epochs = 10, batch_size = 64, sample_weight = ht_weights)

# Get Treatment Effect Estimate from LSTM (HT)
rnn_pred_ht <- predict(rnn_model_ht, X_train_rnn)
rnn_ate_ht <- mean(rnn_pred_ht)  # Average of the predictions as the treatment effect estimate

# 8. Propensity Score Matching (PSM) for Treatment Effect Estimate
# Modify event variable to be binary (e.g., 1 for event type 1, and 0 for event type 2)
compas_data$event_binary <- ifelse(compas_data$is_recid == 1, 1, 0)  # Convert event to binary

# Apply nearest neighbor matching
psm_model <- matchit(event_binary ~ age + sex + race + priors_count + decile_score, data = compas_data, method = "nearest")  # Apply nearest neighbor matching
psm_matched_data <- match.data(psm_model)  # Get the matched data
psm_ate <- mean(psm_matched_data$decile_score)  # Average treatment effect from PSM

# 9. Calculate Confidence Intervals for the models using Bootstrap
bootstrap_ci <- function(model_preds, n_bootstrap = 1000, conf_level = 0.95) {
  bootstrap_samples <- replicate(n_bootstrap, sample(model_preds, replace = TRUE))
  bootstrap_means <- apply(bootstrap_samples, 2, mean)
  ci_lower <- quantile(bootstrap_means, (1 - conf_level) / 2)
  ci_upper <- quantile(bootstrap_means, 1 - (1 - conf_level) / 2)
  return(c(ci_lower, ci_upper))
}

# Bootstrap CI for the models
rnn_ate_ht_ci <- bootstrap_ci(rnn_pred_ht)
psm_ate_ci <- bootstrap_ci(psm_matched_data$decile_score)
logit_preds <- predict(psm_model_multi, type = "probs")
logit_ate_ci <- bootstrap_ci(logit_preds[, 1])  # For the first event

# 10. Perturbation Factor (5% Perturbed Rate)
perturbation_factor <- 0.05

# Apply Perturbation to the Original Features
perturbed_X_train <- X_train + rnorm(length(X_train), mean = 0, sd = perturbation_factor)

# Convert Perturbed Data into Copula-Transformed Features for Copula RNN with HT Weights
pseudo_obs_perturbed <- pobs(perturbed_X_train)
fit_result_perturbed <- fitCopula(copula_model, pseudo_obs_perturbed, method = "ml")
copula_transformed_features_perturbed <- rCopula(nrow(compas_data), fit_result_perturbed@copula)
X_train_rnn_perturbed <- array(copula_transformed_features_perturbed, dim = c(nrow(compas_data), 1, length(features)))

# 11. Train Copula RNN with Perturbed Data (HT Weights)
rnn_model_ht %>% fit(X_train_rnn_perturbed, y_train, epochs = 10, batch_size = 64, sample_weight = ht_weights)
rnn_pred_ht_perturbed <- predict(rnn_model_ht, X_train_rnn_perturbed)
rnn_ate_ht_perturbed <- mean(rnn_pred_ht_perturbed)  # ATE for Perturbed Data

# 12. Apply PSM on Perturbed Data
psm_model_perturbed <- matchit(event_binary ~ perturbed_X_train, data = compas_data, method = "nearest")
psm_matched_data_perturbed <- match.data(psm_model_perturbed)
psm_ate_perturbed <- mean(psm_matched_data_perturbed$decile_score)

# 13. Apply Logistic Regression on Perturbed Data
logit_preds_perturbed <- predict(psm_model_multi, newdata = compas_data, type = "probs")
logit_ate_perturbed <- mean(logit_preds_perturbed[, 1])  # ATE for Perturbed Data

# 14. Combine Results into a Table
ate_results_perturbed <- data.frame(
  Method = c("Copula RNN with HT (Original)", "PSM (Original)", "Multinomial Logit (Original)", 
             "Copula RNN with HT (Perturbed)", "PSM (Perturbed)", "Multinomial Logit (Perturbed)"),
  Estimate = c(rnn_ate_ht, psm_ate, mean(logit_preds[, 1]), 
               rnn_ate_ht_perturbed, psm_ate_perturbed, logit_ate_perturbed),
  LowerCI = c(rnn_ate_ht_ci[1], psm_ate_ci[1], logit_ate_ci[1], 
              bootstrap_ci(rnn_pred_ht_perturbed)[1], bootstrap_ci(psm_matched_data_perturbed$decile_score)[1], 
              bootstrap_ci(logit_preds_perturbed[, 1])[1]),
  UpperCI = c(rnn_ate_ht_ci[2], psm_ate_ci[2], logit_ate_ci[2], 
              bootstrap_ci(rnn_pred_ht_perturbed)[2], bootstrap_ci(psm_matched_data_perturbed$decile_score)[2], 
              bootstrap_ci(logit_preds_perturbed[, 1])[2])
)

# Print the Results in a Clean Table
print(kable(ate_results_perturbed, caption = "Sensitivity Analysis Results for Treatment Effect Estimates"))

# 15. Visualization of ATE with Perturbed Data
ggplot(ate_results_perturbed, aes(x = Method, y = Estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = LowerCI, ymax = UpperCI), width = 0.2) + 
  labs(title = "Comparison of Treatment Effect Estimates with Confidence Intervals (Original vs Perturbed)",
       y = "Estimated Treatment Effect (ATE)", x = "Method") +
  theme_minimal()


### Real Data


# Load Necessary Libraries
library(nnet)      # For multinomial logistic regression
library(keras)     # For deep learning models
library(tensorflow)  # For tensorflow backend
library(MatchIt)   # For matching techniques
library(survival)
library(dplyr)
library(MASS)      # For correlated data generation
library(survminer)
library(ggplot2)
library(survcomp)  # For C-index calculation
library(kableExtra)
library(copula)    # For Gaussian copula transformation
library(boot)      # For bootstrapping

# 1. Load the compas-scores.csv dataset
compas_data <- read.csv("compas-scores.csv")

# Preview the dataset
head(compas_data)

# 2. Preprocess the data
# Select features and the treatment variable (e.g., 'is_recid' for recidivism prediction)
# Assume 'is_recid' is the treatment indicator and 'decile_score' is the outcome
# Add more variables as necessary based on the dataset columns

# Define features for the model
features <- c("age", "sex", "race", "priors_count", "decile_score")
compas_data <- compas_data[complete.cases(compas_data[, features]), ]  # Remove missing values

# Convert 'is_recid' to a factor for logistic regression
compas_data$is_recid <- as.factor(compas_data$is_recid)

# 3. Generate Propensity Scores using Multinomial Logistic Regression
psm_model_multi <- nnet::multinom(is_recid ~ age + sex + race + priors_count + decile_score, data = compas_data)
prop_scores <- predict(psm_model_multi, type = "probs")

# 4. Calculate Inverse Probability Treatment Weights (IPTW) and Horvitz-Thompson (HT) Weights
iptw_weights_1 <- 1 / prop_scores[, 1]
iptw_weights_2 <- 1 / prop_scores[, 2]
ht_weights <- 1 / (prop_scores[cbind(1:nrow(compas_data), as.numeric(compas_data$is_recid))])

# Sample weights for IPTW
sample_weights <- ifelse(compas_data$is_recid == 1, iptw_weights_1, iptw_weights_2)

# 5. Prepare Data for Training
X_train <- as.matrix(compas_data[, features])  # Features
y_train <- compas_data$decile_score  # Outcome variable

# 6. Gaussian Copula Transformation
copula_model <- normalCopula(dim = length(features), dispstr = "un")  # Define Gaussian copula
pseudo_obs <- pobs(X_train)  # Transform features to pseudo-observations (uniform margins)

fit_result <- fitCopula(copula_model, pseudo_obs, method = "ml")  # Fit the copula

# Simulate Copula-Transformed Features
copula_transformed_features <- rCopula(nrow(compas_data), fit_result@copula)  # Simulate from the fitted copula

# Reshape for LSTM input
X_train_rnn <- array(copula_transformed_features, dim = c(nrow(compas_data), 1, length(features)))  # Timesteps = 1 for RNN

# 7. Define and Train LSTM Model with HT Weights (Horvitz-Thompson Weights)
rnn_model_ht <- keras_model_sequential() %>%
  layer_lstm(units = 64, return_sequences = FALSE, input_shape = c(1, length(features))) %>%
  layer_dense(units = 1, activation = "linear")

rnn_model_ht %>% compile(loss = "mean_squared_error", optimizer = optimizer_adam(), metrics = c("mae"))

# Train the LSTM model using the copula-transformed features and HT weights
rnn_model_ht %>% fit(X_train_rnn, y_train, epochs = 10, batch_size = 64, sample_weight = ht_weights)

# Get Treatment Effect Estimate from LSTM (HT)
rnn_pred_ht <- predict(rnn_model_ht, X_train_rnn)
rnn_ate_ht <- mean(rnn_pred_ht)  # Average of the predictions as the treatment effect estimate

# 8. Define and Train LSTM Model with IPTW Weights (Inverse Probability of Treatment Weights)
rnn_model_iptw <- keras_model_sequential() %>%
  layer_lstm(units = 64, return_sequences = FALSE, input_shape = c(1, length(features))) %>%
  layer_dense(units = 1, activation = "linear")

rnn_model_iptw %>% compile(loss = "mean_squared_error", optimizer = optimizer_adam(), metrics = c("mae"))

# Train the LSTM model using the copula-transformed features and IPTW weights
rnn_model_iptw %>% fit(X_train_rnn, y_train, epochs = 10, batch_size = 64, sample_weight = sample_weights)

# Get Treatment Effect Estimate from LSTM (IPTW)
rnn_pred_iptw <- predict(rnn_model_iptw, X_train_rnn)
rnn_ate_iptw <- mean(rnn_pred_iptw)  # Average of the predictions as the treatment effect estimate

# 9. Propensity Score Matching (PSM) for Treatment Effect Estimate
# Modify event variable to be binary (e.g., 1 for event type 1, and 0 for event type 2)
compas_data$event_binary <- ifelse(compas_data$is_recid == 1, 1, 0)  # Convert event to binary

# Apply nearest neighbor matching
psm_model <- matchit(event_binary ~ age + sex + race + priors_count + decile_score, data = compas_data, method = "nearest")  # Apply nearest neighbor matching
psm_matched_data <- match.data(psm_model)  # Get the matched data
psm_ate <- mean(psm_matched_data$decile_score)  # Average treatment effect from PSM

# 10. Calculate Confidence Intervals for the models using Bootstrap
bootstrap_ci <- function(model_preds, n_bootstrap = 1000, conf_level = 0.95) {
  bootstrap_samples <- replicate(n_bootstrap, sample(model_preds, replace = TRUE))
  bootstrap_means <- apply(bootstrap_samples, 2, mean)
  ci_lower <- quantile(bootstrap_means, (1 - conf_level) / 2)
  ci_upper <- quantile(bootstrap_means, 1 - (1 - conf_level) / 2)
  return(c(ci_lower, ci_upper))
}

# Bootstrap CI for the models
rnn_ate_ht_ci <- bootstrap_ci(rnn_pred_ht)
psm_ate_ci <- bootstrap_ci(psm_matched_data$decile_score)
logit_preds <- predict(psm_model_multi, type = "probs")
logit_ate_ci <- bootstrap_ci(logit_preds[, 1])  # For the first event

# 11. Print Results
ate_results <- data.frame(
  Method = c("Copula RNN with HT",  
             "PSM", "Logistic Regression"),
  Estimate = c(rnn_ate_ht, psm_ate, mean(logit_preds[, 1])),
  LowerCI = c(rnn_ate_ht_ci[1], psm_ate_ci[1], logit_ate_ci[1]),
  UpperCI = c(rnn_ate_ht_ci[2], psm_ate_ci[2], logit_ate_ci[2])
)

# Print Results
print(ate_results)

# 12. Visualization with Error Bars for CI
ggplot(ate_results, aes(x = Method, y = Estimate)) +
  geom_point() +  # Plot the point estimates
  geom_errorbar(aes(ymin = LowerCI, ymax = UpperCI), width = 0.2) +  # Add error bars for the CI
  labs(title = "Comparison of Treatment Effect Estimates with Confidence Intervals",
       y = "Estimated Treatment Effect (ATE)",
       x = "Method") +
  theme_minimal()
