# ==============================================================================
# Full R Code: CNN-LSTM with Copula NLL Losses for Joint Modeling
# ==============================================================================

# 0️⃣ Library Setup and Configuration
# ================================
library(keras)
library(keras3)
library(tensorflow)
library(dplyr)
library(tidyr)
library(ggplot2)
library(copula)
library(gridExtra)
library(kableExtra)
library(viridis)
library(JM) # For pbc2.id data in the real data section
set.seed(123)
# Prevent TensorFlow from using too much memory on some systems
# tf$compat$v1$disable_eager_execution()
# tf$compat$v1$set_random_seed(123)

# --- Helper function: Transform N(0,1) predictions to U(0,1) ---
to_uniform_tf <- function(x) {
  # Standard normal CDF (Phi) approximation using the error function (erf)
  u <- 0.5 * (1 + tf$math$erf(tf$cast(x, tf$float32) / tf$math$sqrt(2.0)))
  # Clip to (0, 1) for copula stability
  tf$clip_by_value(u, 1e-6, 1 - 1e-6)
}

# --- Shared Copula Loss Functions (3D: Y1, Y2, S) ---

copula_gaussian_loss <- function(y_true, y_pred) {
  y_true <- tf$cast(y_true, tf$float32)
  y_pred <- tf$cast(y_pred, tf$float32)
 
  # The Gaussian copula NLL is equivalent to the NLL of a multivariate normal distribution
  # of the *predictions* (which are the pseudo-observations in N(0,1) space).
  phi_centered <- y_pred - tf$reduce_mean(y_pred, axis = as.integer(0L), keepdims = TRUE)
  n_samples <- tf$cast(tf$shape(phi_centered)[1], tf$float32)
 
  # Covariance matrix calculation (unbiased estimate)
  cov_mat <- tf$matmul(tf$transpose(phi_centered), phi_centered) / (n_samples - 1.0)
  epsilon <- 1e-6
  cov_mat_stable <- cov_mat + epsilon * tf$eye(tf$shape(cov_mat)[1])
 
  # Inverse and Determinant
  inv_cov <- tf$linalg$inv(cov_mat_stable)
  det_cov <- tf$linalg$det(cov_mat_stable)
 
  # Quadratic term (y' * inv_cov * y)
  quad_term <- tf$reduce_sum(tf$matmul(y_true, inv_cov) * y_true, axis = as.integer(1L))
  log_det_term <- tf$math$log(det_cov + epsilon)
 
  # NLL = 0.5 * (log(det) + y' * inv_cov * y + d*log(2*pi)) (ignoring constant for optimization)
  nll <- 0.5 * (log_det_term + quad_term)
  tf$reduce_mean(nll, axis = as.integer(0L))
}


copula_clayton_loss <- function(y_true, y_pred, theta_init = 1.0) {
  y_true <- tf$cast(y_true, tf$float32)
  y_pred <- tf$cast(y_pred, tf$float32)
 
  # Ensure theta > 0
  theta_init <- tf$cast(theta_init, tf$float32)
  theta <- tf$abs(theta_init) + 1e-3
 
  # Transform N(0,1) predictions to U(0,1) margins
  u_pred <- to_uniform_tf(y_pred)
 
  # S = sum(u_j^(-theta))
  S <- tf$reduce_sum(tf$math$pow(u_pred, -theta), axis = as.integer(1L))
 
  # Log-density for 3D Clayton Copula:
  # log(c(u)) = log(1 + 2*theta) - (theta + 1) * sum(log(u_j)) - (3 + 1/theta) * log(S - 2)
  log_term_const <- tf$math$log(1 + 2 * theta)
  log_term_prod <- - (theta + 1) * tf$reduce_sum(tf$math$log(u_pred), axis = as.integer(1L))
  log_term_power <- - (3 + 1/theta) * tf$math$log(S - 2 + 1e-6)
 
  log_C_density <- log_term_const + log_term_prod + log_term_power
 
  nll <- -log_C_density
  tf$reduce_mean(nll)
}


copula_gumbel_loss <- function(y_true, y_pred, theta_init = 2.0) {
  y_true <- tf$cast(y_true, tf$float32)
  y_pred <- tf$cast(y_pred, tf$float32)
 
  # Ensure theta >= 1
  theta_init <- tf$cast(theta_init, tf$float32)
  theta <- tf$maximum(theta_init, 1.0 + 1e-3)
 
  u_pred <- to_uniform_tf(y_pred)
 
  # Gumbel generator: phi(t) = (-log t)^theta
  log_u <- -tf$math$log(u_pred)
  phi_u <- tf$math$pow(log_u, theta)
  sum_phi_u <- tf$reduce_sum(phi_u, axis = as.integer(1L))
 
  # Log-density for 3D Gumbel Copula:
  # log(c(u)) = log( (1/theta) * sum_phi_u^(3/theta - 3) ) + ...
  # Simplified NLL (using log-likelihood for the 3D Gumbel copula):
 
  # Term 1: log of the second derivative component
  log_deriv <- tf$math$log(tf$math$pow(sum_phi_u, 1/theta - 3))
 
  # Term 2: log of the copula function and first derivative components
  log_remaining <- (3 - 1/theta) * tf$math$log(sum_phi_u) +
    tf$reduce_sum(
      (theta - 1) * tf$math$log(log_u) - theta * tf$math$log(u_pred),
      axis = as.integer(1L)
    )
 
  log_C_density <- log_deriv + log_remaining
 
  nll <- -log_C_density
  tf$reduce_mean(nll)
}


# --- Custom Keras Layers (Placeholders for Copula Activations) ---
LearnableClaytonActivation <- new_layer_class(
  classname = "LearnableClaytonActivation",
  initialize = function(self, theta_init = 1.0) {
    super$initialize()
    self$theta <- self$add_weight(shape = list(), initializer = initializer_constant(theta_init), trainable = TRUE, name = "theta")
  },
  call = function(self, inputs) {
    # The actual copula transformation is *not* the activation,
    # so we use identity for the output layer's input.
    tf$identity(inputs)
  }
)

LearnableGumbelActivation <- new_layer_class(
  classname = "LearnableGumbelActivation",
  initialize = function(self, theta_init = 2.0) {
    super$initialize()
    self$theta <- self$add_weight(shape = list(), initializer = initializer_constant(theta_init), trainable = TRUE, name = "theta")
  },
  call = function(self, inputs) { tf$identity(inputs) }
)

LearnableClaytonGumbelHybrid <- new_layer_class(
  classname = "LearnableClaytonGumbelHybrid",
  initialize = function(self, theta_c_init = 1.0, theta_g_init = 2.0, alpha_init = 0.5) {
    super$initialize()
    self$theta_c <- self$add_weight(shape = list(), initializer = initializer_constant(theta_c_init), trainable = TRUE, name = "theta_c")
    self$theta_g <- self$add_weight(shape = list(), initializer = initializer_constant(theta_g_init), trainable = TRUE, name = "theta_g")
    self$alpha <- self$add_weight(shape = list(), initializer = initializer_constant(alpha_init), trainable = TRUE, name = "alpha")
  },
  call = function(self, inputs) { tf$identity(inputs) }
)

# --- CNN-LSTM Model Builder ---
create_cnn_lstm <- function(type = c("clayton", "gumbel", "hybrid", "relu", "sigmoid"), input_features) {
  type <- match.arg(type)
 
  model <- keras_model_sequential() %>%
    layer_conv_1d(filters = 32, kernel_size = 3, activation = "relu",
                  input_shape = c(timesteps, input_features)) %>%
    layer_max_pooling_1d(pool_size = 2) %>%
    layer_lstm(units = 64, return_sequences = FALSE) %>%
    layer_dropout(0.3)
 
  # The activation layers are added but effectively use identity in this architecture
  # because the copula loss operates on the final linear output.
  if (type == "clayton") model <- model %>% LearnableClaytonActivation()
  if (type == "gumbel") model <- model %>% LearnableGumbelActivation()
  if (type == "hybrid") model <- model %>% LearnableClaytonGumbelHybrid()
  if (type == "relu") model <- model %>% layer_activation("relu")
  if (type == "sigmoid") model <- model %>% layer_activation("sigmoid")
 
  # Final linear layer for 3 N(0,1) outcomes (Y1, Y2, S)
  model %>% layer_dense(units = 3, activation = "linear")
}

# --- Shared Training Function ---
train_and_evaluate <- function(x_scaled, y_numeric, timesteps, features, model_types, model_names, loss_types) {
 
  all_results <- list()
  all_residuals <- list()
 
  for (i in seq_along(model_types)) {
    for (loss in loss_types) {
      k_clear_session()
      cat("Training", model_names[i], "with", loss, "loss\n")
     
      m <- create_cnn_lstm(model_types[i], input_features = dim(x_scaled)[3])
     
      # Select the appropriate loss function
      current_loss_fn <- switch(
        loss,
        "mse" = "mse",
        "gauss_copula" = copula_gaussian_loss,
        "clayton_copula" = copula_clayton_loss,
        "gumbel_copula" = copula_gumbel_loss
      )
     
      m %>% compile(
        optimizer = "adam",
        loss = current_loss_fn,
        metrics = "mae"
      )
     
      # Train the model
      m %>% fit(
        x_scaled, y_numeric,
        epochs = 10, batch_size = 32, verbose = 0, validation_split = 0.2
      )
     
      # Predict and calculate residuals
      preds <- m %>% predict(x_scaled)
      residuals <- y_numeric - preds
     
      res_df <- data.frame(
        Model = model_names[i], Loss = loss,
        ResidualY1 = residuals[,1], ResidualY2 = residuals[,2], ResidualS = residuals[,3]
      )
      all_residuals[[paste0(model_names[i], "_", loss)]] <- res_df
     
      all_results[[paste0(model_names[i], "_", loss)]] <- data.frame(
        Model = model_names[i], Loss = loss,
        MeanResidual1 = mean(residuals[,1]), SDResidual1 = sd(residuals[,1]),
        MeanResidual2 = mean(residuals[,2]), SDResidual2 = sd(residuals[,2]),
        MeanResidualS = mean(residuals[,3]), SDResidualS = sd(residuals[,3])
      )
    }
  }
 
  results_df <- do.call(rbind, all_results)
  residuals_df <- do.call(rbind, all_residuals)
 
  return(list(results = results_df, residuals = residuals_df))
}


# ==============================================================================
# PART 1: SIMULATION STUDY
# ==============================================================================

cat("================================\n")
cat("SIMULATION STUDY\n")
cat("================================\n")

# 1️⃣ Simulation Settings
timesteps <- 10
features  <- 30
n         <- 500
weibull_shape <- 1.5
weibull_scale <- 5

# 2️⃣ Generate Continuous Outcomes and Survival Time
y_continuous <- data.frame(Y1 = rnorm(n, mean = 0, sd = 1), Y2 = rnorm(n, mean = 1, sd = 2))
Time <- rweibull(n, shape = weibull_shape, scale = weibull_scale)
CensorTime <- runif(n, min = 0, max = 10)
ObservedTime <- pmin(Time, CensorTime)
Event <- as.numeric(Time <= CensorTime)
S <- Time # Latent survival time is the third outcome
y_data_df <- data.frame(Y1 = y_continuous$Y1, Y2 = y_continuous$Y2, S = S)
x_data <- array(runif(n * timesteps * features), dim = c(n, timesteps, features))

# 3️⃣ Copula Transformation and Scaling
u_data_df <- as.data.frame(lapply(y_data_df, function(x) rank(x)/(length(x)+1)))
u_mat <- as.matrix(pmin(pmax(u_data_df, 1e-6), 1 - 1e-6))
cop_fit0 <- normalCopula(dim = 3)
fit0 <- fitCopula(cop_fit0, data = u_mat, method = "ml")
rho <- coef(fit0) # Use rho for initial theta
y_copula <- as.data.frame(sapply(1:3, function(j) qnorm(u_data_df[[j]])))
colnames(y_copula) <- c("Y1", "Y2", "S")
y_numeric <- as.matrix(y_copula)
x_scaled <- (x_data - mean(x_data)) / sd(x_data)

# 4️⃣ Train and Evaluate
model_types <- c("clayton", "gumbel", "hybrid", "relu", "sigmoid")
model_names <- c("Clayton", "Gumbel", "Clayton-Gumbel", "ReLU", "Sigmoid")
loss_types <- c("mse", "gauss_copula", "clayton_copula", "gumbel_copula")

sim_results <- train_and_evaluate(x_scaled, y_numeric, timesteps, features, model_types, model_names, loss_types)
sim_residuals_df <- sim_results$residuals
sim_results_df <- sim_results$results

# 5️⃣ Simulation Residual Diagnostics
sim_residuals_long <- sim_residuals_df %>%
  tidyr::pivot_longer(
    cols = c(ResidualY1, ResidualY2, ResidualS), names_to = "Outcome", values_to = "Residual") %>%
  mutate(Outcome = gsub("Residual", "", Outcome), Dataset = "Simulated")

# Plot
p_sim <- ggplot(sim_residuals_long, aes(x = Residual, fill = Loss)) +
  geom_density(alpha = 0.55, adjust = 1.2) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "black") +
  facet_grid(Model ~ Outcome, scales = "free") +
  theme_minimal(base_size = 10) +
  scale_fill_viridis_d(option = "C", end = 0.8) +
  labs(title = "Simulation: Residual Density Comparison",
       subtitle = "Faceted by Model, Outcome, and Loss Function",
       x = "Residual", y = "Density", fill = "Loss Function") +
  theme(plot.title = element_text(face = "bold", size = 12), legend.position = "bottom")

print(p_sim)

cat("Simulation Residual Summary:\n")
print(kable(sim_results_df, digits = 3, caption = "Simulation Residual Summary") %>%
        kable_styling(full_width = FALSE))


# ==============================================================================
# PART 2: REAL DATA (PBC Study)
# ==============================================================================

cat("\n\n================================\n")
cat("REAL DATA: PBC Study\n")
cat("================================\n")

# 1️⃣ Load and Inspect Data
data("pbc2.id", package = "JM")
pbc <- pbc2.id
n_desired <- 500
timesteps <- 10
max_features <- 30

id_col <- "id"
time_col <- "years"
status_col <- "status"

# Convert factor/character columns to numeric
pbc[[time_col]] <- as.numeric(as.character(pbc[[time_col]]))
pbc[[status_col]] <- as.numeric(as.character(pbc[[status_col]]))
pbc$serBilir <- as.numeric(as.character(pbc$serBilir))
pbc$albumin <- as.numeric(as.character(pbc$albumin))

# 2️⃣ Build Per-Subject Survival Summary
surv_summary <- pbc %>%
  group_by_at(id_col) %>%
  summarise(
    surv_time = max(.data[[time_col]], na.rm = TRUE),
    # Combine 1 (dead) and 2 (transplanted) as event=1
    event = as.integer(any(.data[[status_col]] %in% c(1, 2)))
  ) %>% ungroup()

# Filter to desired subjects
subject_ids <- head(surv_summary[[id_col]], n_desired)
surv_summary <- surv_summary %>% filter(.data[[id_col]] %in% subject_ids)
pbc <- pbc %>% filter(.data[[id_col]] %in% subject_ids)

# 3️⃣ Prepare Longitudinal Feature Data
excluded_cols <- c(id_col, time_col, status_col, "drug", "sex", "ascites", "hepatomegaly",
                   "spiders", "edema", "histol")
candidate_features <- setdiff(names(pbc), excluded_cols)
numeric_features <- candidate_features[sapply(pbc[candidate_features], is.numeric)]
numeric_features <- head(numeric_features, max_features)

long_data <- pbc[, c(id_col, time_col, numeric_features)]
long_data <- long_data[order(long_data[[id_col]], long_data[[time_col]]), ]

# 4️⃣ Pad to Fixed Timesteps (X-array)
subject_list <- split(long_data, long_data[[id_col]])
pad_subject <- function(df, features, tsteps) {
  mat <- as.matrix(df[, features, drop = FALSE])
  if (nrow(mat) >= tsteps) mat[1:tsteps, ] else rbind(mat, matrix(0, nrow = tsteps - nrow(mat), ncol = ncol(mat)))
}

x_array <- array(
  data = unlist(lapply(subject_list, pad_subject, numeric_features, timesteps)),
  dim = c(length(subject_list), timesteps, length(numeric_features))
)

# 5️⃣ Impute Missing Values and Scale
for (j in 1:dim(x_array)[3]) {
  col_data <- x_array[,,j]
  col_data[is.na(col_data)] <- mean(col_data[col_data != 0], na.rm = TRUE)
  x_array[,,j] <- col_data
}
x_scaled <- (x_array - mean(x_array, na.rm=TRUE)) / sd(x_array, na.rm=TRUE)
x_scaled[is.na(x_scaled)] <- 0 # Final safeguard

# 6️⃣ Prepare Copula-Transformed Target (Y1: serBilir, Y2: albumin, S: surv_time)
long_outcomes <- c("serBilir", "albumin")
y_long_summary <- long_data %>%
  group_by_at(id_col) %>%
  summarise(
    across(all_of(long_outcomes), ~ last(.[!is.na(.)]), .names = "Y_{.col}")
  ) %>% ungroup()

y_data_df <- surv_summary %>%
  left_join(y_long_summary, by = id_col) %>%
  dplyr::select(Y_serBilir, Y_albumin, S = surv_time) %>%
  na.omit()

# Match subject order and align x_scaled
id_order <- names(subject_list)
y_data_df <- y_data_df[match(id_order, surv_summary[[id_col]]), ]
y_data_df <- y_data_df[!is.na(y_data_df$S), ] # Ensure only subjects with complete Y data are kept
x_scaled <- x_scaled[1:nrow(y_data_df), , ]

# Copula transformation
u_data_df <- as.data.frame(lapply(y_data_df, function(x) rank(x, na.last = "keep") / (sum(!is.na(x)) + 1)))
u_mat <- as.matrix(pmin(pmax(u_data_df, 1e-6), 1 - 1e-6))
u_mat[!is.finite(u_mat)] <- 0.5

cop_fit0 <- normalCopula(dim = ncol(u_mat))
suppressWarnings(fit0 <- try(fitCopula(cop_fit0, data = u_mat, method = "ml"), silent = TRUE))
rho <- if (inherits(fit0, "try-error")) rep(0.5, ncol(u_mat)) else as.vector(coef(fit0))

y_copula <- as.data.frame(lapply(u_data_df, function(u) qnorm(u)))
colnames(y_copula) <- colnames(y_data_df)
y_numeric <- as.matrix(y_copula)

n_final <- nrow(y_numeric)
x_scaled <- x_scaled[1:n_final, , ]

# 7️⃣ Train and Evaluate Real Data
real_results <- train_and_evaluate(x_scaled, y_numeric, timesteps, features, model_types, model_names, loss_types)
real_residuals_df <- real_results$residuals
real_results_df <- real_results$results

# 8️⃣ Real Data Residual Diagnostics
real_residuals_long <- real_residuals_df %>%
  tidyr::pivot_longer(
    cols = c(ResidualY1, ResidualY2, ResidualS), names_to = "Outcome", values_to = "Residual") %>%
  mutate(Outcome = gsub("ResidualY1", "serBilir", Outcome),
         Outcome = gsub("ResidualY2", "albumin", Outcome),
         Outcome = gsub("ResidualS", "Survival", Outcome),
         Dataset = "Real PBC")

# Plot
p_real <- ggplot(real_residuals_long, aes(x = Residual, fill = Loss)) +
  geom_density(alpha = 0.55, adjust = 1.2) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "black") +
  facet_grid(Model ~ Outcome, scales = "free") +
  theme_minimal(base_size = 10) +
  scale_fill_viridis_d(option = "C", end = 0.8) +
  labs(title = "Real PBC Data: Residual Density Comparison",
       subtitle = "Faceted by Model, Outcome (serBilir, albumin, Survival), and Loss Function",
       x = "Residual", y = "Density", fill = "Loss Function") +
  theme(plot.title = element_text(face = "bold", size = 12), legend.position = "bottom")

print(p_real)

cat("\nReal PBC Residual Summary:\n")
print(kable(real_results_df, digits = 3, caption = "Real PBC Residual Summary") %>%
        kable_styling(full_width = FALSE))

# ==============================================================================
# Final Visualization (Combined)
# ==============================================================================
# Combine the two residual data frames for a final comparison
combined_residuals_long <- bind_rows(sim_residuals_long, real_residuals_long)

p_combined_box <- ggplot(combined_residuals_long, aes(x = Loss, y = Residual, fill = Loss)) +
  geom_boxplot(alpha = 0.6, outlier.size = 0.5, width = 0.6) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  facet_grid(Dataset + Model ~ Outcome, scales = "free_y") +
  theme_minimal(base_size = 10) +
  scale_fill_viridis_d(option = "C", end = 0.8) +
  labs(title = "Residual Spread: Simulation vs. Real PBC Data",
       subtitle = "Faceted by Dataset, Model, and Outcome",
       x = "Loss Function", y = "Residual") +
  theme(plot.title = element_text(face = "bold", size = 14),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
        strip.text = element_text(size = 9, face = "bold"),
        legend.position = "none")

print(p_combined_box)

##### Model fitting for CGD data with cluster embeddings #####
library(JM)
library(dplyr)
library(tensorflow)
library(keras)

data("cgd")  # Load CGD dataset

# --- 1️⃣ Prepare time, event, and covariates ---
time <- cgd$tstop - cgd$tstart
event <- cgd$status
treat <- as.integer(cgd$treat) - 1
sex <- as.integer(cgd$sex) - 1

X <- model.matrix(~0 + treat + sex)       # design matrix
p <- ncol(X)                               # number of covariates
n <- nrow(X)                               # total sample size
q <- length(unique(cgd$id))                # number of clusters
id_numeric <- as.integer(factor(cgd$id)) - 1  # 0-indexed cluster IDs

# --- 2️⃣ Prepare numeric features for CNN-LSTM ---
timesteps <- 1
x_numeric_df <- as.data.frame(X)
x_array <- array(as.matrix(x_numeric_df), dim=c(n, timesteps, p))
x_scaled <- (x_array - mean(x_array, na.rm=TRUE)) / sd(x_array, na.rm=TRUE)

# --- 3️⃣ Prepare cluster embeddings ---
embedding_dim <- min(10, ceiling(q/2))  # embedding dimension
id_input <- layer_input(shape = 1, dtype = "int32", name = "id_input")
id_embed <- id_input %>%
  layer_embedding(input_dim = q, output_dim = embedding_dim, input_length = 1) %>%
  layer_flatten()

# --- 4️⃣ Prepare response variables (multivariate survival) ---
y_data_df <- data.frame(Y1=time, Y2=event, S=time)
u_data_df <- as.data.frame(lapply(y_data_df, function(x) rank(x)/(length(x)+1)))
y_numeric <- as.matrix(sapply(u_data_df, qnorm))

# --- 5️⃣ Updated CNN-LSTM model builder with cluster embeddings ---
create_cnn_lstm_with_cluster <- function(type=c("clayton","gumbel","hybrid","relu","sigmoid"), 
                                         input_features, timesteps, n_clusters, embed_dim) {
  type <- match.arg(type)
  
  # Feature input
  feature_input <- layer_input(shape = c(timesteps, input_features), name="feature_input")
  x <- feature_input %>%
    layer_conv_1d(filters = 32, kernel_size = min(3, timesteps), activation = "relu", padding="same")
  if(timesteps > 1) x <- x %>% layer_max_pooling_1d(pool_size = 2)
  x <- x %>% layer_lstm(units = 64, return_sequences = FALSE) %>% layer_dropout(0.3)
  
  # Cluster embedding input
  id_input <- layer_input(shape = 1, dtype = "int32", name="id_input")
  id_embed <- id_input %>%
    layer_embedding(input_dim = n_clusters, output_dim = embed_dim, input_length = 1) %>%
    layer_flatten()
  
  # Concatenate feature representation + cluster embedding
  concat <- layer_concatenate(list(x, id_embed))
  
  if(type=="clayton") concat <- concat %>% LearnableClaytonActivation()
  if(type=="gumbel") concat <- concat %>% LearnableGumbelActivation()
  if(type=="hybrid") concat <- concat %>% LearnableClaytonGumbelHybrid()
  if(type=="relu") concat <- concat %>% layer_activation("relu")
  if(type=="sigmoid") concat <- concat %>% layer_activation("sigmoid")
  
  output <- concat %>% layer_dense(units = 3, activation="linear")
  
  model <- keras_model(inputs = list(feature_input, id_input), outputs = output)
  return(model)
}

# --- 6️⃣ Train and Evaluate Function for Cluster Model ---
train_and_evaluate_cluster <- function(x_scaled, id_numeric, y_numeric, timesteps, features,
                                       n_clusters, embed_dim, model_types, model_names, loss_types) {
  
  all_results <- list()
  all_residuals <- list()
  
  for(i in seq_along(model_types)) {
    for(loss in loss_types) {
      k_clear_session()
      cat("Training", model_names[i], "with", loss, "loss\n")
      
      m <- create_cnn_lstm_with_cluster(model_types[i], features, timesteps, n_clusters, embed_dim)
      
      # Select loss
      current_loss_fn <- switch(loss,
                                "mse" = "mse",
                                "gauss_copula" = copula_gaussian_loss,
                                "clayton_copula" = copula_clayton_loss,
                                "gumbel_copula" = copula_gumbel_loss)
      
      m %>% compile(optimizer = "adam", loss = current_loss_fn, metrics = "mae")
      
      m %>% fit(
        list(feature_input = x_scaled, id_input = array(id_numeric, dim=c(length(id_numeric),1))),
        y_numeric,
        epochs = 10, batch_size = 32, verbose=0, validation_split=0.2
      )
      
      # Predictions & residuals
      preds <- m %>% predict(list(feature_input = x_scaled, id_input = array(id_numeric, dim=c(length(id_numeric),1))))
      residuals <- y_numeric - preds
      res_df <- data.frame(Model=model_names[i], Loss=loss,
                           ResidualY1=residuals[,1], ResidualY2=residuals[,2], ResidualS=residuals[,3])
      all_residuals[[paste0(model_names[i],"_",loss)]] <- res_df
      
      all_results[[paste0(model_names[i],"_",loss)]] <- data.frame(
        Model=model_names[i], Loss=loss,
        MeanResidual1 = mean(residuals[,1]), SDResidual1 = sd(residuals[,1]),
        MeanResidual2 = mean(residuals[,2]), SDResidual2 = sd(residuals[,2]),
        MeanResidualS = mean(residuals[,3]), SDResidualS = sd(residuals[,3])
      )
    }
  }
  
  results_df <- do.call(rbind, all_results)
  residuals_df <- do.call(rbind, all_residuals)
  
  return(list(results=results_df, residuals=residuals_df))
}

# --- 7️⃣ Train on CGD data ---
cgd_results <- train_and_evaluate_cluster(
  x_scaled, id_numeric, y_numeric, timesteps, p, n_clusters=q, embed_dim=embedding_dim,
  model_types, model_names, loss_types
)

# =======================================================================
# PART 4: CGD DATA RESIDUAL VISUALIZATION (Corrected)
# =======================================================================

library(ggplot2)
library(dplyr)
library(tidyr)
library(viridis)
library(kableExtra)


# 1️⃣ Transform CGD residuals into long format for plotting
cgd_residuals_long <- cgd_results$residuals %>%
    pivot_longer(
        cols = c(ResidualY1, ResidualY2, ResidualS),
        names_to = "Outcome",
        values_to = "Residual"
    ) %>%
    mutate(
        Outcome = gsub("ResidualY1", "Time", Outcome),
        Outcome = gsub("ResidualY2", "Event", Outcome),
        Outcome = gsub("ResidualS", "Survival", Outcome),
        Dataset = "CGD"
    )

# 2️⃣ Combine with available datasets (Simulation, PBC, CGD)
# The missing 'kidney_residuals_long' has been removed.
combined_residuals_long <- bind_rows(
    sim_residuals_long,
    real_residuals_long,
    cgd_residuals_long
)

# --- The rest of the visualization code can now run ---

# 3️⃣ Density Plot: Residuals by Model and Outcome
p_combined_density <- ggplot(combined_residuals_long, aes(x = Residual, fill = Loss)) +
    geom_density(alpha = 0.55, adjust = 1.2) +
    geom_vline(xintercept = 0, linetype = "dashed", color = "black") +
    facet_grid(Dataset + Model ~ Outcome, scales = "free") +
    theme_minimal(base_size = 10) +
    scale_fill_viridis_d(option = "C", end = 0.8) +
    labs(
        title = "Residual Density Comparison Across Datasets (Sim, PBC, CGD)",
        subtitle = "Faceted by Dataset, Model, Outcome, and Loss Function",
        x = "Residual", y = "Density", fill = "Loss Function"
    ) +
    theme(
        plot.title = element_text(face = "bold", size = 12),
        legend.position = "bottom",
        strip.text = element_text(size = 9)
    )

print(p_combined_density)

# 4️⃣ Boxplot: Residual spread comparison
p_combined_box <- ggplot(combined_residuals_long, aes(x = Loss, y = Residual, fill = Loss)) +
    geom_boxplot(alpha = 0.6, outlier.size = 0.5, width = 0.6) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
    facet_grid(Dataset + Model ~ Outcome, scales = "free_y") +
    theme_minimal(base_size = 10) +
    scale_fill_viridis_d(option = "C", end = 0.8) +
    labs(
        title = "Residual Spread Across Datasets (Sim, PBC, CGD)",
        subtitle = "Faceted by Dataset, Model, and Outcome",
        x = "Loss Function", y = "Residual"
    ) +
    theme(
        plot.title = element_text(face = "bold", size = 14),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
        strip.text = element_text(size = 9, face = "bold"),
        legend.position = "none"
    )

print(p_combined_box)

# Combine all residuals with dataset labels
all_residuals_long <- bind_rows(
  sim_residuals_long,
  real_residuals_long,
  cgd_residuals_long
)

# Function to create individual residual density plots per outcome per dataset
plot_individual_residuals_all <- function(residuals_long) {
  datasets <- unique(residuals_long$Dataset)
  outcomes <- unique(residuals_long$Outcome)
  
  plots <- list()
  
  for (ds in datasets) {
    for (out in outcomes) {
      df <- residuals_long %>% filter(Dataset == ds, Outcome == out)
      
      p <- ggplot(df, aes(x = Residual, fill = Loss)) +
        geom_density(alpha = 0.55, adjust = 1.2) +
        geom_vline(xintercept = 0, linetype = "dashed", color = "black") +
        facet_wrap(~Model, scales = "free") +
        theme_minimal(base_size = 12) +
        scale_fill_viridis_d(option = "C", end = 0.8) +
        labs(
          title = paste(ds, "-", out, "Residuals"),
          x = "Residual", y = "Density", fill = "Loss Function"
        ) +
        theme(
          plot.title = element_text(face = "bold", size = 14),
          legend.position = "bottom",
          strip.text = element_text(size = 10)
        )
      
      plots[[paste(ds, out, sep = "_")]] <- p
    }
  }
  
  return(plots)
}

# Generate individual plots for all datasets
individual_residual_plots <- plot_individual_residuals_all(all_residuals_long)

# Example: Display plots for Simulation dataset
print(individual_residual_plots[["Simulated_Y1"]])
print(individual_residual_plots[["Simulated_Y2"]])
print(individual_residual_plots[["Simulated_S"]])

# Example: Display plots for Real PBC dataset
print(individual_residual_plots[["Real PBC_serBilir"]])
print(individual_residual_plots[["Real PBC_albumin"]])
print(individual_residual_plots[["Real PBC_Survival"]])

# Example: Display plots for CGD dataset
print(individual_residual_plots[["CGD_Time"]])
print(individual_residual_plots[["CGD_Event"]])
print(individual_residual_plots[["CGD_Survival"]])

library(dplyr)
library(kableExtra)

# Combine all residuals with dataset labels
all_residuals_long <- bind_rows(
  sim_residuals_long,
  real_residuals_long,
  cgd_residuals_long
)

# Function to summarize residuals per outcome
summarize_residuals_per_outcome <- function(residuals_long) {
  residuals_long %>%
    group_by(Dataset, Model, Outcome, Loss) %>%
    summarise(
      MeanResidual = mean(Residual, na.rm = TRUE),
      SDResidual = sd(Residual, na.rm = TRUE),
      .groups = "drop"
    ) %>%
    arrange(Dataset, Outcome, Model, Loss)
}

# Generate the summary table
residual_summary_table <- summarize_residuals_per_outcome(all_residuals_long)

# Display tables per dataset and outcome
datasets <- unique(residual_summary_table$Dataset)
outcomes <- unique(residual_summary_table$Outcome)

for (ds in datasets) {
  cat("\n====================================\n")
  cat("Residual Summary: ", ds, "\n")
  cat("====================================\n")
  
  for (out in outcomes) {
    df <- residual_summary_table %>% filter(Dataset == ds, Outcome == out)
    if (nrow(df) > 0) {
      cat("\n--- Outcome: ", out, " ---\n")
      print(
        kable(df, digits = 3, caption = paste(ds, "-", out, "Residual Summary")) %>%
          kable_styling(full_width = FALSE)
      )
    }
  }
}
