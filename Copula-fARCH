# ============================================
#  fARCH vs Copula-fARCH Simulation (100 Iter)
# ============================================

# --- Clean Environment ---
rm(list = ls()); gc()

# --- Load Libraries ---
library(sde)
library(fda)
library(fdapace)
library(MASS)
library(ggplot2)
library(copula)
library(forecast)
library(DEoptim)
library(Matrix)
library(tseries)
library(reshape2)
library(parallel)
library(patchwork)

# --- Parameters ---
L <- 350          # Intraday points per day
N <- 100          # Number of days
p <- 2            # Number of FPCs
nbasis <- 15      # Number of B-spline basis functions
norder <- 4       # Basis order
lambda <- 1e-4    # Smoothing parameter
itermax <- 200
n_iter <- 100    # Number of simulation iterations


# ====================================
# --- fARCH Estimation Function ---
# ====================================
estimate_farch <- function(y_old, L, p, nbasis = 15, norder = 4) {
  N <- ncol(y_old)
  sigma_out_squared <- matrix(0, N - 2, L)
  basis <- create.bspline.basis(rangeval = c(0, 1), nbasis = nbasis, norder = norder)
  fd_par_obj <- fdPar(basis, lambda = 1e-4, Lfdobj = 2)
  times <- seq(0, 1, length.out = L)
  
  for (out in 2:(N - 1)) {
    y <- y_old[, -out, drop = FALSE]
    y_sq_fdsmooth <- smooth.basis(times, y^2, fdParobj = fd_par_obj)
    functional_y_squared <- y_sq_fdsmooth$fd
    principal_components <- pca.fd(fdobj = functional_y_squared, nharm = p)
    eigenfunctions <- principal_components$harmonics
    mean_fd <- mean.fd(functional_y_squared)
    
    Z <- matrix(0, p, ncol(y))
    for (i in 1:p) {
      for (k in 1:ncol(y)) {
        Z[i, k] <- inprod(fd(coef = functional_y_squared$coefs[, k, drop = FALSE],
                             basisobj = basis), eigenfunctions[i]) -
          inprod(mean_fd, eigenfunctions[i])
      }
    }
    
    part1 <- part2 <- matrix(0, p, p)
    for (k in 2:ncol(Z)) {
      part1 <- part1 + Z[, k] %*% t(Z[, k - 1])
      part2 <- part2 + Z[, k - 1] %*% t(Z[, k - 1])
    }
    M <- part1 %*% solve(part2)
    
    eigenvector <- lapply(1:p, function(i) eval.fd(times, eigenfunctions[i]))
    beta_matrix_estimated <- Reduce(`+`, lapply(1:p, function(k) {
      Reduce(`+`, lapply(1:p, function(ell) {
        M[k, ell] * eigenvector[[k]] %*% t(eigenvector[[ell]])
      }))
    }))
    
    beta_hat <- function(x) t(beta_matrix_estimated %*% t(x)) / L
    m_hat <- t(as.matrix(eval.fd(times, mean_fd)))
    estimated_delta <- m_hat - beta_hat(m_hat)
    
    y_test <- matrix(y_old[, out - 1]^2, nrow = 1)
    sigma_out_squared[out - 1, ] <- estimated_delta + beta_hat(y_test)
  }
  return(sigma_out_squared)
}


# =========================================
# --- Copula-fARCH Estimation Function ---
# =========================================
estimate_cfarch <- function(y_old, L, p, nbasis = 15, norder = 4, lambda = 1e-4) {
  N <- ncol(y_old)
  sigma_out_squared <- matrix(0, N - 2, L)
  basis <- create.bspline.basis(c(0, 1), nbasis = nbasis, norder = norder)
  fd_par_obj <- fdPar(basis, lambda = lambda, Lfdobj = 2)
  times <- seq(0, 1, length.out = L)
  
  for (out in 2:(N - 1)) {
    y <- y_old[, -out, drop = FALSE]
    
    y_sq_fdsmooth <- smooth.basis(times, y^2, fdParobj = fd_par_obj)
    functional_y_squared <- y_sq_fdsmooth$fd
    mean_fd <- mean.fd(functional_y_squared)
    pca_obj <- pca.fd(fdobj = functional_y_squared, nharm = p)
    eigenfunctions <- pca_obj$harmonics
    
    Z <- matrix(0, p, ncol(y))
    for (i in 1:p) {
      for (k in 1:ncol(y)) {
        Z[i, k] <- inprod(fd(coef = functional_y_squared$coefs[, k, drop = FALSE],
                             basisobj = basis), eigenfunctions[i]) -
          inprod(mean_fd, eigenfunctions[i])
      }
    }
    
    u_data <- pobs(t(Z))
    sims_list <- list()
    n_sim <- ncol(Z)
    
    for (cop_type in c("gaussian", "clayton")) {
      fit_cop <- tryCatch({
        cop_obj <- switch(cop_type,
                          "gaussian" = normalCopula(dim = p, dispstr = "un"),
                          "clayton"  = claytonCopula(dim = p))
        fitCopula(cop_obj, u_data, method = "ml")
      }, error = function(e) NULL)
      
      if (!is.null(fit_cop)) {
        sim_cop <- rCopula(n_sim, fit_cop@copula)
        sims_list[[length(sims_list) + 1]] <- qnorm(sim_cop)
      }
    }
    
    Z_used <- if (length(sims_list) > 0) {
      t(Reduce("+", sims_list) / length(sims_list))
    } else {
      t(Z)
    }
    
    part1 <- part2 <- matrix(0, p, p)
    for (k in 2:ncol(Z_used)) {
      part1 <- part1 + Z_used[, k] %*% t(Z_used[, k - 1])
      part2 <- part2 + Z_used[, k - 1] %*% t(Z_used[, k - 1])
    }
    M <- part1 %*% solve(part2)
    
    eigenvector <- lapply(1:p, function(i) eval.fd(times, eigenfunctions[i]))
    beta_matrix_estimated <- Reduce(`+`, lapply(1:p, function(k) {
      Reduce(`+`, lapply(1:p, function(ell) {
        M[k, ell] * eigenvector[[k]] %*% t(eigenvector[[ell]])
      }))
    }))
    
    beta_hat <- function(x) t(beta_matrix_estimated %*% t(x)) / L
    m_hat <- t(as.matrix(eval.fd(times, mean_fd)))
    estimated_delta <- m_hat - beta_hat(m_hat)
    
    y_test <- matrix(y_old[, out - 1]^2, nrow = 1)
    sigma_out_squared[out - 1, ] <- estimated_delta + beta_hat(y_test)
  }
  
  return(sigma_out_squared)
}


# ====================================
# --- Utility Functions ---
# ====================================
log_stabilize <- function(mat, epsilon = 1e-6) {
  log_mat <- log(mat + epsilon)
  exp(log_mat)
}

evaluate_model <- function(est, proxy) {
  error <- est - proxy
  rmse <- sqrt(mean(error^2, na.rm = TRUE))
  mae  <- mean(abs(error), na.rm = TRUE)
  mape <- mean(abs((proxy - est)/ifelse(proxy==0, NA, proxy)), na.rm = TRUE) * 100
  log_est <- log1p(pmax(est, 0))
  log_proxy <- log1p(pmax(proxy, 0))
  msle <- mean((log_est - log_proxy)^2, na.rm = TRUE)
  corr <- mean(sapply(1:nrow(est), function(i) cor(est[i, ], proxy[i, ], use = "complete.obs")), na.rm = TRUE)
  return(list(RMSE=rmse, MAE=mae, MAPE=mape, MSLE=msle, Corr=corr))
}


# ====================================
# --- Single Simulation Function ---
# ====================================
run_single_sim <- function(iter) {
  set.seed(100 + iter)
  
  # Simulate volatility and returns
  volatility <- numeric(L + 1)
  volatility[1] <- 0.5
  alpha0 <- 0.001; alpha1 <- 0.25; beta1 <- 0.74; shock_scale <- 0.6
  for (t in 2:(L + 1)) {
    shock <- rnorm(1, mean = 0, sd = shock_scale)
    volatility[t] <- sqrt(pmax(alpha0 + alpha1 * volatility[t - 1]^2 + beta1 * shock^2, 1e-6))
  }
  ar_coef <- 0.95
  returns <- matrix(0, nrow = L + 1, ncol = N)
  for (i in 1:N) {
    ar_process <- numeric(L + 1)
    ar_process[1] <- rnorm(1, sd = volatility[1])
    for (t in 2:(L + 1)) {
      epsilon <- rnorm(1, sd = volatility[t])
      ar_process[t] <- ar_coef * ar_process[t - 1] + epsilon
    }
    returns[, i] <- ar_process
  }
  returns_use <- returns[-1, ]
  
  # Estimate models
  sigma_farch <- estimate_farch(returns_use, L, p, nbasis, norder)
  sigma_cfarch <- estimate_cfarch(returns_use, L, p, nbasis, norder)
  
  # Log-stabilize
  pred_farch <- log_stabilize(sigma_farch)
  pred_cfarch <- log_stabilize(sigma_cfarch)
  true_vol_eval <- log_stabilize(t(returns_use^2))
  true_vol_eval <- true_vol_eval[1:nrow(pred_farch), ]
  
  # Evaluate
  eval_farch  <- evaluate_model(pred_farch, true_vol_eval)
  eval_cfarch <- evaluate_model(pred_cfarch, true_vol_eval)
  
  return(data.frame(
    Iter = iter,
    RMSE_fARCH = eval_farch$RMSE,
    RMSE_CfARCH = eval_cfarch$RMSE,
    Corr_fARCH = eval_farch$Corr,
    Corr_CfARCH = eval_cfarch$Corr
  ))
}


# ====================================
# --- Parallel Simulation (100x) ---
# ====================================
n_cores <- max(1, detectCores() - 1)
cl <- makeCluster(n_cores)

# Export everything needed
clusterExport(cl, list(
  "estimate_farch", "estimate_cfarch", "log_stabilize", "evaluate_model",
  "L", "N", "p", "nbasis", "norder", "lambda"
))
clusterEvalQ(cl, {
  library(sde); library(fda); library(fdapace); library(MASS)
  library(ggplot2); library(copula); library(forecast)
  library(DEoptim); library(Matrix); library(tseries); library(reshape2)
})

# Run
results_list <- parLapply(cl, 1:n_iter, run_single_sim)
stopCluster(cl)

# Combine Results
results_df <- do.call(rbind, results_list)
print(summary(results_df))

# --- Visualization of RMSE comparison ---
results_melt <- melt(results_df[, c("RMSE_fARCH", "RMSE_CfARCH")])
ggplot(results_melt, aes(x = variable, y = value, fill = variable)) +
  geom_boxplot(outlier.shape = NA) +
  labs(title = "RMSE Comparison: fARCH vs Copula-fARCH",
       x = "Model", y = "RMSE") +
  theme_minimal() +
  theme(legend.position = "none")


# ==========================
# --- Summary Table ---
# ==========================
library(dplyr)
library(knitr)

summary_table <- results_df %>%
  summarise(
    Mean_RMSE_fARCH = mean(RMSE_fARCH, na.rm = TRUE),
    SD_RMSE_fARCH   = sd(RMSE_fARCH, na.rm = TRUE),
    Mean_RMSE_CfARCH = mean(RMSE_CfARCH, na.rm = TRUE),
    SD_RMSE_CfARCH   = sd(RMSE_CfARCH, na.rm = TRUE),
    Mean_Corr_fARCH = mean(Corr_fARCH, na.rm = TRUE),
    SD_Corr_fARCH   = sd(Corr_fARCH, na.rm = TRUE),
    Mean_Corr_CfARCH = mean(Corr_CfARCH, na.rm = TRUE),
    SD_Corr_CfARCH   = sd(Corr_CfARCH, na.rm = TRUE)
  )

print(summary_table)



# ==========================
# --- Visualization ---
# ==========================
library(ggplot2)
library(reshape2)

# Melt the RMSE and Corr values for comparison
results_melt <- results_df %>%
  select(RMSE_fARCH, RMSE_CfARCH, Corr_fARCH, Corr_CfARCH) %>%
  melt(variable.name = "Metric", value.name = "Value")

# Add group columns for easier labeling
results_melt$Type <- ifelse(grepl("RMSE", results_melt$Metric), "RMSE", "Correlation")
results_melt$Model <- ifelse(grepl("CfARCH", results_melt$Metric), "Copula-fARCH", "fARCH")

# --- RMSE Boxplot ---
ggplot(filter(results_melt, Type == "RMSE"),
       aes(x = Model, y = Value, fill = Model)) +
  geom_boxplot(outlier.shape = NA) +
  coord_cartesian(ylim = quantile(results_melt$Value[results_melt$Type == "RMSE"], c(0.05, 0.95))) +
  labs(title = "RMSE Comparison (fARCH vs Copula-fARCH)",
       x = "", y = "RMSE") +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none",
        plot.title = element_text(face = "bold", size = 14))

# --- Correlation Boxplot ---
ggplot(filter(results_melt, Type == "Correlation"),
       aes(x = Model, y = Value, fill = Model)) +
  geom_boxplot(outlier.shape = NA) +
  coord_cartesian(ylim = quantile(results_melt$Value[results_melt$Type == "Correlation"], c(0.05, 0.95))) +
  labs(title = "Correlation Comparison (fARCH vs Copula-fARCH)",
       x = "", y = "Correlation") +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none",
        plot.title = element_text(face = "bold", size = 14))


# --- Combined Summary Plot ---
summary_long <- results_melt %>%
  group_by(Model, Type) %>%
  summarise(
    Mean = mean(Value, na.rm = TRUE),
    SD = sd(Value, na.rm = TRUE)
  )

ggplot(summary_long, aes(x = Model, y = Mean, fill = Type)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.7)) +
  geom_errorbar(aes(ymin = Mean - SD, ymax = Mean + SD),
                position = position_dodge(width = 0.7), width = 0.2) +
  labs(title = "Mean ± SD of RMSE and Correlation across Models",
       x = "Model", y = "Value", fill = "Metric Type") +
  theme_minimal(base_size = 12) +
  theme(plot.title = element_text(face = "bold", size = 14))

# --- Real Data Analysis ---
rm(list = ls()); gc()

library(sde)
library(fda)
library(fdapace)
library(MASS)
library(copula)
library(ggplot2)
library(forecast)
library(DEoptim)
library(Matrix)
library(tseries)  # for dm.test
library(reshape2)

# Parameters (use your original choices)
L <- 381     # Intraday observations per day
# Original N (216) will be overridden by number of usable days read from file
h <- 1
p <- 2
nbasis <- 32
norder <- 4
lambda <- 1e-4
itermax <- 200

# --- Load and Process Data ---
# NOTE: ensure "newkospi.csv" has no zeros in prices (log) or handle them explicitly.
kospi <- read.csv("newkospi.csv", header = FALSE)
data <- as.matrix(kospi)[,-(1:176)]

# Attempt to coerce into (L+h) x ncol format only if file is raw; otherwise skip this.
# If file is already rows = times and cols = days, you can omit dim() override.
if(nrow(data) == (L + h)) {
  # OK as-is: columns are days
} else if(ncol(data) == (L + h)) {
  # transpose so rows correspond to times
  data <- t(data)
} else {
  warning("Input dimensions don't match L+h. Check newkospi.csv formatting.")
}

# safe log: if any non-positive entries exist, stop and inspect.
if(any(data <= 0)) stop("Non-positive price(s) found; log() not possible. Clean data first.")

log_data <- 100 * log(data)
# compute intraday returns: rows = days, cols = intraday timepoints
returns_mat <- t(log_data[(h + 1):(L + h), ] - log_data[1:L, ])
# remove days (columns) with any NA in original (if any)
returns_mat <- returns_mat[, colSums(is.na(returns_mat)) == 0]

# Now returns_mat: rows = days, cols = L
N_days <- nrow(returns_mat)
cat("Usable days:", N_days, "Intraday points:", ncol(returns_mat), "\n")

# proxy volatility (squared returns) - keep the same shape (days x L)
proxy_vol <- returns_mat^2

# --- Corrected fARCH Estimation Function ---
estimate_farch <- function(y_old, L, p, nbasis = 32, norder = 4, lambda = 1e-6) {
  # y_old: matrix (days x L)
  N <- nrow(y_old)
  times <- seq(0, 1, length.out = L)
  basis <- create.bspline.basis(rangeval = c(0, 1), nbasis = nbasis, norder = norder)
  fd_par <- fdPar(basis, Lfd = 2, lambda = lambda)

  sigma_out_squared <- matrix(NA, N - 2, L)  # will store rows for out = 2:(N-1)

  for (out in 2:(N - 1)) {
    cat("Estimating fARCH Day:", out, "/", N - 1, "\n")
    # leave-one-out set of observations
    y <- y_old[-out, , drop = FALSE]   # (n_obs x L) where n_obs = N-1
    n_obs <- nrow(y)

    # smooth squared functions: smooth.basis expects rows = timepoints, cols = obs
    smooth_res <- smooth.basis(argvals = times, y = t(y^2), fdParobj = fd_par)
    fd_y2 <- smooth_res$fd

    # FPCA: pca.fd returns scores with rows = observations
    pca_obj <- pca.fd(fdobj = fd_y2, nharm = p, center = TRUE)
    # scores: n_obs x p; we want Z as p x n_obs
    Z <- t(pca_obj$scores)   # correct shape: p x n_obs

    # Estimate transition matrix M using Z (lag-1)
    part1 <- matrix(0, p, p)
    part2 <- matrix(0, p, p)
    for (k in 2:ncol(Z)) {
      part1 <- part1 + Z[, k] %*% t(Z[, k - 1])
      part2 <- part2 + Z[, k - 1] %*% t(Z[, k - 1])
    }
    # regularize solve if needed
    M <- tryCatch(part1 %*% solve(part2), error = function(e) part1 %*% ginv(part2))

    # reconstruct beta matrix on time grid using eigenfunctions (harmonics)
    eigenfun_list <- lapply(1:p, function(i) eval.fd(times, pca_obj$harmonics[i]))
    beta_matrix_estimated <- Reduce(`+`, lapply(1:p, function(k) {
      Reduce(`+`, lapply(1:p, function(ell) {
        M[k, ell] * (eigenfun_list[[k]] %*% t(eigenfun_list[[ell]]))
      }))
    }))

    beta_hat <- function(x) t(beta_matrix_estimated %*% t(x)) / L   # same form you used
    mean_fd_vals <- t(as.matrix(eval.fd(times, pca_obj$meanfd)))

    # estimated_delta and prediction (note using y_old[out-1, ]^2 as last observed)
    estimated_delta <- mean_fd_vals - beta_hat(mean_fd_vals)
    sigma_out_squared[out - 1, ] <- as.numeric(estimated_delta + beta_hat(t(y_old[out - 1, ]^2)))
  }

  return(sigma_out_squared)
}

# --- Corrected Copula-fARCH Estimation Function ---
estimate_cfarch <- function(y_old, L, p, nbasis = 32, norder = 4, lambda = 1e-4) {
  set.seed(123)
  N <- nrow(y_old)
  times <- seq(0, 1, length.out = L)
  basis <- create.bspline.basis(rangeval = c(0, 1), nbasis = nbasis, norder = norder)
  fd_par <- fdPar(basis, Lfd = 2, lambda = lambda)

  sigma_out_squared <- matrix(NA, N - 2, L)

  for (out in 2:(N - 1)) {
    cat("Estimating Copula-fARCH Day:", out, "/", N - 1, "\n")
    y <- y_old[-out, , drop = FALSE]   # n_obs x L
    n_obs <- nrow(y)

    # Smooth and FPCA
    smooth_res <- smooth.basis(argvals = times, y = t(y^2), fdParobj = fd_par)
    fd_y2 <- smooth_res$fd
    pca_obj <- pca.fd(fdobj = fd_y2, nharm = p, center = TRUE)

    # Z: p x n_obs
    Z <- t(pca_obj$scores)

    # --- Copula modeling on pseudo-observations ---
    # pobs expects obs in rows; we pass matrix n_obs x p
    u_data <- pobs(t(Z)) # returns n_obs x p (pobs expects columns as variables)
    sims_list <- list()

    # fit Gaussian and Clayton (if possible)
    for (cop_type in c("gaussian", "clayton")) {
      fit_cop <- tryCatch({
        cop_obj <- switch(cop_type,
                          "gaussian" = normalCopula(dim = p, dispstr = "un"),
                          "clayton" = claytonCopula(dim = p))
        fitCopula(cop_obj, data = u_data, method = "ml")
      }, error = function(e) NULL)

      if (!is.null(fit_cop)) {
        sim_cop <- rCopula(n = n_obs, fit_cop@copula)  # n_obs x p
        sims_list[[length(sims_list) + 1]] <- qnorm(sim_cop)  # transform to Gaussian scores
      }
    }

    # If we have simulated copula-improved scores, average them; otherwise fallback to Z (original)
    if (length(sims_list) > 0) {
      avg_sim <- Reduce(`+`, sims_list) / length(sims_list)  # n_obs x p
      Z_used <- t(avg_sim)   # p x n_obs
    } else {
      Z_used <- Z
    }

    # Estimate M using Z_used
    part1 <- matrix(0, p, p)
    part2 <- matrix(0, p, p)
    for (k in 2:ncol(Z_used)) {
      part1 <- part1 + Z_used[, k] %*% t(Z_used[, k - 1])
      part2 <- part2 + Z_used[, k - 1] %*% t(Z_used[, k - 1])
    }
    M <- tryCatch(part1 %*% solve(part2), error = function(e) part1 %*% ginv(part2))

    # reconstruct volatility surface
    eigenfun_list <- lapply(1:p, function(i) eval.fd(times, pca_obj$harmonics[i]))
    beta_matrix_estimated <- Reduce(`+`, lapply(1:p, function(k) {
      Reduce(`+`, lapply(1:p, function(ell) {
        M[k, ell] * (eigenfun_list[[k]] %*% t(eigenfun_list[[ell]]))
      }))
    }))

    beta_hat <- function(x) t(beta_matrix_estimated %*% t(x)) / L
    mean_fd_vals <- t(as.matrix(eval.fd(times, pca_obj$meanfd)))
    estimated_delta <- mean_fd_vals - beta_hat(mean_fd_vals)

    sigma_out_squared[out - 1, ] <- as.numeric(estimated_delta + beta_hat(t(y_old[out - 1, ]^2)))
  }

  return(sigma_out_squared)
}

# --- Run Estimations ---
sigma_farch  <- estimate_farch(returns_mat, L, p, nbasis = nbasis, norder = norder, lambda = 1e-6)
sigma_cfarch <- estimate_cfarch(returns_mat, L, p, nbasis = nbasis, norder = norder, lambda = lambda)

# --- Evaluation Function (unchanged but aligned indices) ---
evaluate_model <- function(est, proxy) {
  proxy_sub <- proxy[1:nrow(est), , drop = FALSE]
  error <- est - proxy_sub

  rmse <- sqrt(mean(error^2, na.rm = TRUE))
  mse  <- mean(error^2, na.rm = TRUE)
  mae  <- mean(abs(error), na.rm = TRUE)
  mape <- mean(abs((proxy_sub - est) / ifelse(proxy_sub == 0, NA, proxy_sub)), na.rm = TRUE) * 100

  log_est <- log1p(pmax(est, 0))
  log_proxy <- log1p(pmax(proxy_sub, 0))
  msle <- mean((log_est - log_proxy)^2, na.rm = TRUE)

  corr <- mean(sapply(1:nrow(est), function(i) cor(est[i, ], proxy_sub[i, ], use = "complete.obs")), na.rm = TRUE)

  return(list(RMSE = rmse, MSE = mse, MAE = mae, MAPE = mape, MSLE = msle, Corr = corr))
}

# --- Evaluate ---
eval_cfarch <- evaluate_model(sigma_cfarch, proxy_vol)
eval_farch  <- evaluate_model(sigma_farch, proxy_vol)

# Log-likelihood (Gaussian errors)
loglik_fun <- function(est, proxy) {
  resid <- as.vector(proxy[1:nrow(est), ] - est)
  sigma_eps <- sd(resid, na.rm = TRUE)
  sum(dnorm(resid, mean = 0, sd = sigma_eps, log = TRUE), na.rm = TRUE)
}

loglik_cfarch <- loglik_fun(sigma_cfarch, proxy_vol)
loglik_farch  <- loglik_fun(sigma_farch, proxy_vol)

k <- p
n <- nrow(proxy_vol)
aic <- function(ll, k) -2 * ll + 2 * k
bic <- function(ll, k, n) -2 * ll + log(n) * k

aic_cfarch <- aic(loglik_cfarch, k)
bic_cfarch <- bic(loglik_cfarch, k, n)
aic_farch  <- aic(loglik_farch, k)
bic_farch  <- bic(loglik_farch, k, n)

result_table <- data.frame(
  Model = c("Copula-fARCH", "fARCH"),
  RMSE = c(eval_cfarch$RMSE, eval_farch$RMSE),
  MSE = c(eval_cfarch$MSE, eval_farch$MSE),
  MAE = c(eval_cfarch$MAE, eval_farch$MAE),
  MAPE = c(eval_cfarch$MAPE, eval_farch$MAPE),
  MSLE = c(eval_cfarch$MSLE, eval_farch$MSLE),
  Correlation = c(eval_cfarch$Corr, eval_farch$Corr),
  LogLikelihood = c(loglik_cfarch, loglik_farch),
  AIC = c(aic_cfarch, aic_farch),
  BIC = c(bic_cfarch, bic_farch)
)

print(result_table)

# --- Align for plotting ---
true_vol <- returns_mat^2
pred_farch  <- sigma_farch   # (N-2) x L
pred_cfarch <- sigma_cfarch
true_vol_eval <- true_vol[2:(nrow(true_vol) - 1), , drop = FALSE]  # align length


# --- Choose sample days for visualization ---
sample_days <- c(5, 20)
sample_days <- sample_days[sample_days <= nrow(pred_farch)]  # ensure valid indices

# --- Prepare plotting data frame ---
library(reshape2)
library(dplyr)
library(ggplot2)
library(scales)

df_list <- lapply(sample_days, function(day_idx) {
  data.frame(
    Time = 1:L,
    True = true_vol_eval[day_idx, ],
    fARCH = pred_farch[day_idx, ],
    Copula_fARCH = pred_cfarch[day_idx, ],
    Day = paste("Day", day_idx)
  )
})

df_plot <- bind_rows(df_list)
df_plot_melt <- melt(df_plot, id.vars = c("Time", "Day"))
df_plot_melt$variable <- factor(df_plot_melt$variable, 
                                levels = c("True", "fARCH", "Copula_fARCH"))

ggplot(df_plot_melt, aes(x = Time, y = value, 
                         color = variable, 
                         linetype = variable)) +
  geom_line(size = 1.2, alpha = 0.95) +
  scale_color_manual(values = c(
    "True"          = "#FFD700",  # Golden yellow for True
    "fARCH"         = "#E63946",  # Strong red
    "Copula_fARCH"  = "#1D3557"   # Deep navy blue
  )) +
  scale_linetype_manual(values = c(
    "True"          = "dashed",
    "fARCH"         = "dotdash",
    "Copula_fARCH"  = "solid"
  )) +
  facet_wrap(~ Day, ncol = 2, scales = "free_y") +
  labs(
    title = "KOSPI Functional Volatility Surface Comparison Across Sample Days",
    subtitle = "True vs Estimated Volatility Functions (σ²)",
    x = "Intraday Time Index",
    y = expression("Estimated Volatility" ~ (σ^2))
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
    plot.subtitle = element_text(size = 12, color = "gray40", hjust = 0.5),
    axis.title = element_text(face = "bold", size = 13),
    strip.text = element_text(face = "bold", size = 13),
    legend.position = "top",
    legend.title = element_blank(),
    legend.text = element_text(size = 12),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.major.y = element_line(color = "gray85", size = 0.3),
    plot.margin = margin(10, 20, 10, 20)
  ) +
  guides(color = guide_legend(override.aes = list(size = 1.6, alpha = 1)))

# --- Diebold-Mariano test for MSE comparison ---
mse_cfarch_vec <- rowMeans((pred_cfarch - proxy_vol[1:nrow(pred_cfarch), ])^2, na.rm = TRUE)
mse_farch_vec  <- rowMeans((pred_farch  - proxy_vol[1:nrow(pred_farch), ])^2, na.rm = TRUE)

# dm.test expects two numeric vectors
dm_cfarch_farch <- dm.test(mse_cfarch_vec, mse_farch_vec, alternative = "two.sided", h = 1)

cat("\nDiebold-Mariano Test (Copula-fARCH vs fARCH):\n")
print(dm_cfarch_farch)

