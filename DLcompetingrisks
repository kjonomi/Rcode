# ================================================================
# 0️⃣ Libraries
# ================================================================
library(survival)
library(dplyr)
library(tidyr)
library(keras)
library(tensorflow)
library(ggplot2)
library(MASS)
library(cmprsk)
library(survminer)
library(boot)
library(Hmisc)

# ================================================================
# 1️⃣ Simulation parameters
# ================================================================
n_sim <- 100    # number of iterations
n <- 1000        # sample size per iteration
B_boot <- 20    # bootstrap resamples (short for speed)
covariates <- c("age","sex","serChol","albumin","SGOT","serBilir",
                "spiders","ascites","hepatomegaly")

# ================================================================
# 2️⃣ Helper functions: Copula / CIF transformation
# ================================================================
to_uniform_tf <- function(x){
  u <- tf$math$sigmoid(tf$cast(x, tf$float32))
  tf$clip_by_value(u, 1e-6, 1-1e-6)
}

u_to_z <- function(u){
  tf$sqrt(2.0) * tf$math$erfinv(2.0*tf$clip_by_value(u,1e-6,1-1e-6) - 1.0)
}

copula_gaussian_loss <- function(y_true, y_pred){
  rho <- tf$constant(0.5, dtype=tf$float32)
  rho <- tf$clip_by_value(rho, -0.99, 0.99)
  u <- to_uniform_tf(y_pred)
  z <- u_to_z(u)
  z1 <- z[,1]; z2 <- z[,2]
  detR <- 1 - rho^2
  invR <- 1 / detR
  quad <- invR * (z1^2 + z2^2 - 2*rho*z1*z2)
  loss_term <- -0.5*quad - 0.5*tf$math$log(detR)
  -tf$reduce_mean(loss_term)
}

copula_clayton_loss <- function(y_true, y_pred){
  theta <- tf$constant(1.5, dtype=tf$float32)
  theta <- tf$maximum(theta, 0.01)
  u <- to_uniform_tf(y_pred)
  S <- tf$reduce_sum(u^(-theta), axis=1L)
  log_c <- tf$math$log(1+theta) - (theta+1)*tf$reduce_sum(tf$math$log(u), axis=1L) - (2 + 1/theta)*tf$math$log(S - 1 + 1e-6)
  -tf$reduce_mean(log_c)
}

copula_gumbel_loss <- function(y_true, y_pred){
  theta <- tf$constant(2.0, dtype=tf$float32)
  theta <- tf$maximum(theta, 1.001)
  u <- to_uniform_tf(y_pred)
  log_u <- -tf$math$log(u)
  phi <- log_u^theta
  S <- tf$reduce_sum(phi, axis=1L)
  log_C <- -S^(1/theta)
  log_c <- log_C +
    (1/theta - 1) * tf$math$log(S + 1e-6) +
    tf$reduce_sum((theta-1)*tf$math$log(log_u + 1e-6) - tf$math$log(u + 1e-6), axis=1L) +
    tf$math$log(1 + (theta-1)*S^(-1/theta))
  -tf$reduce_mean(log_c)
}

competing_risks_loss <- function(y_true, y_pred){
  event_mask <- tf$reduce_sum(y_true[,2:3], axis=1L)
  cause_mask1 <- y_true[,2]; cause_mask2 <- y_true[,3]
  u <- to_uniform_tf(y_pred)
  cif1 <- u[,1]; cif2 <- u[,2]
  ll1 <- cause_mask1 * tf$math$log(cif1 + 1e-6)
  ll2 <- cause_mask2 * tf$math$log(cif2 + 1e-6)
  surv_prob <- 1 - cif1 - cif2
  ll_cens <- (1 - event_mask) * tf$math$log(surv_prob + 1e-6)
  -tf$reduce_mean(ll1 + ll2 + ll_cens)
}

# ================================================================
# 3️⃣ CNN-LSTM model builder
# ================================================================
create_model <- function(loss_fn, n_causes=2){
  model <- keras_model_sequential() %>%
    layer_conv_1d(filters=16, kernel_size=1, activation="relu", input_shape=c(1,length(covariates))) %>%
    layer_conv_1d(filters=16, kernel_size=1, activation="relu") %>%
    layer_lstm(16) %>%
    layer_dense(16, activation="relu") %>%
    layer_dense(n_causes, activation=ifelse(n_causes==1,"softmax","sigmoid"))
  
  model %>% compile(loss=loss_fn, optimizer=optimizer_adam(0.001), metrics="accuracy")
  return(model)
}

# ================================================================
# 4️⃣ Simulation loop
# ================================================================
set.seed(123)
cindex_results_list <- list()
maxrisk_results_list <- list()

for(sim in 1:n_sim){
  cat("Simulation:", sim, "\n")
  
  # ---------- Generate synthetic data ----------
  df <- data.frame(
    id = 1:n,
    age = rnorm(n, 50, 10),
    sex = rbinom(n, 1, 0.6),
    serChol = rnorm(n, 250, 40),
    albumin = rnorm(n, 3.5, 0.5),
    SGOT = rnorm(n, 30, 10),
    serBilir = rnorm(n, 1, 0.5),
    spiders = rbinom(n, 1, 0.3),
    ascites = rbinom(n, 1, 0.2),
    hepatomegaly = rbinom(n, 1, 0.25)
  )
  df$time <- rexp(n, rate = 0.05)
  df$event <- sample(0:2, n, replace=TRUE, prob=c(0.4,0.35,0.25))
  
  # ---------- Train/test split ----------
  idx_train <- sample(seq_len(nrow(df)), floor(0.8*nrow(df)))
  df_train <- df[idx_train, ]
  df_test  <- df[-idx_train, ]
  
  X_train <- array(as.matrix(df_train[,covariates]), dim=c(nrow(df_train),1,length(covariates)))
  X_test  <- array(as.matrix(df_test[,covariates]), dim=c(nrow(df_test),1,length(covariates)))
  
  df_train$keras_event <- ifelse(df_train$event>0,1,0)
  df_test$keras_event  <- ifelse(df_test$event>0,1,0)
  y_train <- to_categorical(df_train$keras_event,2)
  y_test  <- to_categorical(df_test$keras_event,2)
  
  df_train$event1 <- ifelse(df_train$event==1,1,0)
  df_train$event2 <- ifelse(df_train$event==2,1,0)
  y_train_cr <- as.matrix(cbind(1-df_train$keras_event, df_train$event1, df_train$event2))
  
  df_test$event1 <- ifelse(df_test$event==1,1,0)
  df_test$event2 <- ifelse(df_test$event==2,1,0)
  y_test_cr <- as.matrix(cbind(1-df_test$keras_event, df_test$event1, df_test$event2))
  
  # ---------- Build and train models ----------
  model_ce <- create_model("categorical_crossentropy")
  model_cr_gauss <- create_model(competing_risks_loss, n_causes=2)
  model_cr_clayton <- create_model(competing_risks_loss, n_causes=2)
  model_cr_gumbel <- create_model(competing_risks_loss, n_causes=2)
  
  model_ce %>% fit(X_train, y_train, epochs=3, batch_size=32, validation_split=0.2, verbose=0)
  model_cr_gauss %>% fit(X_train, y_train_cr, epochs=3, batch_size=32, validation_split=0.2, verbose=0)
  model_cr_clayton %>% fit(X_train, y_train_cr, epochs=3, batch_size=32, validation_split=0.2, verbose=0)
  model_cr_gumbel %>% fit(X_train, y_train_cr, epochs=3, batch_size=32, validation_split=0.2, verbose=0)
  
  pretrained_all <- list(
    cox=NULL, ce=model_ce,
    copula_cr_gauss=model_cr_gauss,
    copula_cr_clayton=model_cr_clayton,
    copula_cr_gumbel=model_cr_gumbel
  )
  
  # ---------- Bootstrap C-index ----------
  cindex_cause_boot_fn <- function(data, indices, model_type, cause=1){
    d <- data[indices, ]
    y_true <- d$event
    if(sum(y_true==cause)<2) return(NA)
    
    if(model_type=="cox"){
      fm <- coxph(Surv(time,event)~., data=df_train[,c("time","event",covariates)])
      score <- predict(fm, newdata=d)
    } else {
      m <- pretrained_all[[model_type]]
      Xd <- array(as.matrix(d[,covariates]), dim=c(nrow(d),1,length(covariates)))
      p <- m %>% predict(Xd)
      score <- p[,min(cause,ncol(p))]
    }
    rcorr.cens(score, Surv(d$time, ifelse(y_true==cause,1,0)))["C Index"]
  }
  
  ci_safe <- function(b){ if(all(is.na(b$t))||sd(b$t,na.rm=TRUE)==0) c(NA,NA) else boot.ci(b,type="perc")$percent[4:5] }
  
  boot_models <- c("cox","ce","copula_cr_gauss","copula_cr_clayton","copula_cr_gumbel")
  boot_results_c1 <- lapply(boot_models, function(m) boot(df_test,function(d,i) cindex_cause_boot_fn(d,i,m,cause=1),R=B_boot))
  boot_results_c2 <- lapply(boot_models, function(m) boot(df_test,function(d,i) cindex_cause_boot_fn(d,i,m,cause=2),R=B_boot))
  
  summary_cause_ci <- data.frame(
    Simulation=sim,
    Model=c("Cox","CE","Copula-CR Gaussian","Copula-CR Clayton","Copula-CR Gumbel"),
    C_index_Cause1 = sapply(boot_results_c1,function(b) mean(b$t,na.rm=TRUE)),
    CI_lower_Cause1 = sapply(boot_results_c1,function(b) ci_safe(b)[1]),
    CI_upper_Cause1 = sapply(boot_results_c1,function(b) ci_safe(b)[2]),
    C_index_Cause2 = sapply(boot_results_c2,function(b) mean(b$t,na.rm=TRUE)),
    CI_lower_Cause2 = sapply(boot_results_c2,function(b) ci_safe(b)[1]),
    CI_upper_Cause2 = sapply(boot_results_c2,function(b) ci_safe(b)[2])
  )
  
  cindex_results_list[[sim]] <- summary_cause_ci
  
  # ---------- Max-risk predictions ----------
  extract_risk <- function(pred_matrix){
    data.frame(cause1=pred_matrix[,1], cause2=pred_matrix[,2], survival=1-rowSums(pred_matrix))
  }
  
  max_risk_cause <- function(risk_df_model){
    apply(risk_df_model,1,function(x){ c("Cause1","Cause2","Survival")[which.max(x)] })
  }
  
  risk_df <- data.frame(
    risk_gauss=max_risk_cause(extract_risk(model_cr_gauss %>% predict(X_test))),
    risk_clayton=max_risk_cause(extract_risk(model_cr_clayton %>% predict(X_test))),
    risk_gumbel=max_risk_cause(extract_risk(model_cr_gumbel %>% predict(X_test))),
    Simulation=sim
  )
  
  maxrisk_results_list[[sim]] <- risk_df
}

# ================================================================
# 5️⃣ Aggregate results
# ================================================================
cindex_all <- do.call(rbind, cindex_results_list)
maxrisk_all <- do.call(rbind, maxrisk_results_list)

# ---------- Preview ----------
head(cindex_all)
head(maxrisk_all)

# ================================================================
# 6️⃣ & 7️⃣ Visualize simulation results: max-risk and C-index
# ================================================================

library(dplyr)
library(ggplot2)
library(tidyr)
library(boot)

# ---------- Function to compute bootstrap CI ----------
bootstrap_ci <- function(x, R=200, conf=0.95){
  if(length(unique(x)) < 2) return(c(NA, NA))  # not enough variation
  b <- boot(x, statistic=function(data, i) mean(data[i]), R=R)
  ci <- boot.ci(b, type="perc")$percent[4:5]
  return(ci)
}

# ---------- 6.1 Prepare long format for C-index ----------
cindex_long <- cindex_all %>%
  pivot_longer(
    cols = starts_with("C_index"),
    names_to = "Cause",
    values_to = "C_index"
  ) %>%
  mutate(
    Cause = ifelse(Cause == "C_index_Cause1", "Cause 1", "Cause 2"),
    Model = factor(Model, levels = c("Cox","CE","Copula-CR Gaussian","Copula-CR Clayton","Copula-CR Gumbel"))
  )

# ---------- 6.2 Summarize C-index with bootstrap CI ----------
cindex_summary <- cindex_long %>%
  group_by(Model, Cause) %>%
  summarise(
    Mean_C_index = mean(C_index, na.rm=TRUE),
    CI_lower = bootstrap_ci(C_index)[1],
    CI_upper = bootstrap_ci(C_index)[2],
    .groups="drop"
  )

# ---------- 6.3 Plot C-index with 95% CI ----------
ggplot(cindex_summary, aes(x = Model, y = Mean_C_index, fill = Cause)) +
  geom_col(position = position_dodge(0.8)) +
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper),
                width = 0.2,
                position = position_dodge(0.8)) +
  ylim(0,1) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  labs(
    title = "Cause-specific C-index with 95% Bootstrap CI",
    y = "C-index",
    x = "",
    fill = "Cause"
  )

# ---------- 6.4 Prepare max-risk summary ----------
maxrisk_long <- maxrisk_all %>%
  pivot_longer(
    cols = c(risk_gauss, risk_clayton, risk_gumbel),
    names_to = "Model",
    values_to = "Max_Risk"
  ) %>%
  mutate(
    Model = factor(Model,
                   levels = c("risk_gauss","risk_clayton","risk_gumbel"),
                   labels = c("Copula-CR Gaussian","Copula-CR Clayton","Copula-CR Gumbel"))
  )

maxrisk_summary <- maxrisk_long %>%
  group_by(Model, Max_Risk) %>%
  summarise(Count = n(),
            Percent = 100*Count/n(),
            .groups="drop")

# ---------- 6.5 Plot max-risk counts ----------
ggplot(maxrisk_summary, aes(x = Model, y = Percent, fill = Max_Risk)) +
  geom_col(position = "dodge") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  labs(
    title = "Predicted Maximum Risk Cause Across Simulations",
    y = "Percentage (%)",
    x = "",
    fill = "Max Risk Cause"
  )


# ================================================================
# 0️⃣ Libraries
# ================================================================
library(survival)
library(dplyr)
library(tidyr)
library(keras)
library(tensorflow)
library(ggplot2)
library(MASS)
library(cmprsk)
library(survminer)
library(boot)

# ================================================================
# 1️⃣ Generate synthetic PBC-like data
# ================================================================
set.seed(123)
n <- 300  # sample size

df <- data.frame(
  id = 1:n,
  age = rnorm(n, 50, 10),
  sex = rbinom(n, 1, 0.6),
  serChol = rnorm(n, 250, 40),
  albumin = rnorm(n, 3.5, 0.5),
  SGOT = rnorm(n, 30, 10),
  serBilir = rnorm(n, 1, 0.5),
  spiders = rbinom(n, 1, 0.3),
  ascites = rbinom(n, 1, 0.2),
  hepatomegaly = rbinom(n, 1, 0.25)
)

# Simulate competing-risk times
baseline_hazard <- 0.05
df$time <- rexp(n, rate = baseline_hazard)
df$event <- sample(0:2, n, replace = TRUE, prob = c(0.4, 0.35, 0.25))

covariates <- c("age","sex","serChol","albumin","SGOT","serBilir",
                "spiders","ascites","hepatomegaly")

# Train/test split
set.seed(123)
idx_train <- sample(seq_len(nrow(df)), floor(0.8*nrow(df)))
df_train <- df[idx_train, ]
df_test  <- df[-idx_train, ]

# Keras input arrays
X_train <- array(as.matrix(df_train[, covariates]), dim = c(nrow(df_train), 1, length(covariates)))
X_test  <- array(as.matrix(df_test[, covariates]), dim = c(nrow(df_test), 1, length(covariates)))

# One-hot encoding for binary event
df_train$keras_event <- ifelse(df_train$event > 0, 1, 0)
df_test$keras_event  <- ifelse(df_test$event > 0, 1, 0)
y_train <- to_categorical(df_train$keras_event, 2)
y_test  <- to_categorical(df_test$keras_event, 2)

# Cause-specific one-hot for competing risks
df_train$event1 <- ifelse(df_train$event==1,1,0)
df_train$event2 <- ifelse(df_train$event==2,1,0)
y_train_cr <- as.matrix(cbind(1 - df_train$keras_event, df_train$event1, df_train$event2))

df_test$event1 <- ifelse(df_test$event==1,1,0)
df_test$event2 <- ifelse(df_test$event==2,1,0)
y_test_cr <- as.matrix(cbind(1 - df_test$keras_event, df_test$event1, df_test$event2))

# ================================================================
# 2️⃣ Helper functions: Copula / CIF transformation
# ================================================================
to_uniform_tf <- function(x){
  u <- tf$math$sigmoid(tf$cast(x, tf$float32))
  tf$clip_by_value(u, 1e-6, 1-1e-6)
}

u_to_z <- function(u){
  tf$sqrt(2.0) * tf$math$erfinv(2.0*tf$clip_by_value(u,1e-6,1-1e-6) - 1.0)
}

# Copula-based losses
copula_gaussian_loss <- function(y_true, y_pred){
  rho <- tf$constant(0.5, dtype=tf$float32)
  rho <- tf$clip_by_value(rho, -0.99, 0.99)
  u <- to_uniform_tf(y_pred)
  z <- u_to_z(u)
  z1 <- z[,1]; z2 <- z[,2]
  detR <- 1 - rho^2
  invR <- 1 / detR
  quad <- invR * (z1^2 + z2^2 - 2*rho*z1*z2)
  loss_term <- -0.5*quad - 0.5*tf$math$log(detR)
  -tf$reduce_mean(loss_term)
}

copula_clayton_loss <- function(y_true, y_pred){
  theta <- tf$constant(1.5, dtype=tf$float32)
  theta <- tf$maximum(theta, 0.01)
  u <- to_uniform_tf(y_pred)
  S <- tf$reduce_sum(u^(-theta), axis=1L)
  log_c <- tf$math$log(1+theta) - (theta+1)*tf$reduce_sum(tf$math$log(u), axis=1L) - (2 + 1/theta)*tf$math$log(S - 1 + 1e-6)
  -tf$reduce_mean(log_c)
}

copula_gumbel_loss <- function(y_true, y_pred){
  theta <- tf$constant(2.0, dtype=tf$float32)
  theta <- tf$maximum(theta, 1.001)
  u <- to_uniform_tf(y_pred)
  log_u <- -tf$math$log(u)
  phi <- log_u^theta
  S <- tf$reduce_sum(phi, axis=1L)
  log_C <- -S^(1/theta)
  log_c <- log_C +
    (1/theta - 1) * tf$math$log(S + 1e-6) +
    tf$reduce_sum((theta-1)*tf$math$log(log_u + 1e-6) - tf$math$log(u + 1e-6), axis=1L) +
    tf$math$log(1 + (theta-1)*S^(-1/theta))
  -tf$reduce_mean(log_c)
}

# CIF-based competing risks loss
competing_risks_loss <- function(y_true, y_pred){
  event_mask <- tf$reduce_sum(y_true[,2:3], axis=1L)
  cause_mask1 <- y_true[,2]; cause_mask2 <- y_true[,3]
  u <- to_uniform_tf(y_pred)
  cif1 <- u[,1]; cif2 <- u[,2]
  ll1 <- cause_mask1 * tf$math$log(cif1 + 1e-6)
  ll2 <- cause_mask2 * tf$math$log(cif2 + 1e-6)
  surv_prob <- 1 - cif1 - cif2
  ll_cens <- (1 - event_mask) * tf$math$log(surv_prob + 1e-6)
  -tf$reduce_mean(ll1 + ll2 + ll_cens)
}

# ================================================================
# 3️⃣ CNN-LSTM model builder
# ================================================================
create_model <- function(loss_fn, n_causes=2){
  model <- keras_model_sequential() %>%
    layer_conv_1d(filters=16, kernel_size=1, activation="relu", input_shape=c(1,length(covariates))) %>%
    layer_conv_1d(filters=16, kernel_size=1, activation="relu") %>%
    layer_lstm(16) %>%
    layer_dense(16, activation="relu") %>%
    layer_dense(n_causes, activation=ifelse(n_causes==1,"softmax","sigmoid"))
  
  model %>% compile(loss=loss_fn, optimizer=optimizer_adam(0.001), metrics="accuracy")
  return(model)
}

# ================================================================
# 4️⃣ Train models
# ================================================================
model_ce      <- create_model("categorical_crossentropy")
model_gauss   <- create_model(copula_gaussian_loss)
model_clayton <- create_model(copula_clayton_loss)
model_gumbel  <- create_model(copula_gumbel_loss)
model_cr_gauss   <- create_model(competing_risks_loss, n_causes=2)
model_cr_clayton <- create_model(competing_risks_loss, n_causes=2)
model_cr_gumbel  <- create_model(competing_risks_loss, n_causes=2)

history_ce      <- model_ce %>% fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2, verbose=1)
history_gauss   <- model_gauss %>% fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2, verbose=1)
history_clayton <- model_clayton %>% fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2, verbose=1)
history_gumbel  <- model_gumbel %>% fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2, verbose=1)
history_cr_gauss   <- model_cr_gauss %>% fit(X_train, y_train_cr, epochs=5, batch_size=32, validation_split=0.2, verbose=1)
history_cr_clayton <- model_cr_clayton %>% fit(X_train, y_train_cr, epochs=5, batch_size=32, validation_split=0.2, verbose=1)
history_cr_gumbel  <- model_cr_gumbel %>% fit(X_train, y_train_cr, epochs=5, batch_size=32, validation_split=0.2, verbose=1)

# ================================================================
# 5️⃣ Predict on test set
# ================================================================
pred_cr_gauss   <- model_cr_gauss %>% predict(X_test)
pred_cr_clayton <- model_cr_clayton %>% predict(X_test)
pred_cr_gumbel  <- model_cr_gumbel %>% predict(X_test)

# ---------- Function to extract predicted risk ----------
extract_risk <- function(pred_matrix) {
  data.frame(
    cause1   = pred_matrix[,1],
    cause2   = pred_matrix[,2],
    survival = 1 - rowSums(pred_matrix)
  )
}

risk_gauss   <- extract_risk(pred_cr_gauss)
risk_clayton <- extract_risk(pred_cr_clayton)
risk_gumbel  <- extract_risk(pred_cr_gumbel)

# ---------- Combine into a single data.frame ----------
risk_df <- data.frame(
  id = df_test$id,
  cause1_gauss     = risk_gauss$cause1,
  cause2_gauss     = risk_gauss$cause2,
  survival_gauss   = risk_gauss$survival,
  cause1_clayton   = risk_clayton$cause1,
  cause2_clayton   = risk_clayton$cause2,
  survival_clayton = risk_clayton$survival,
  cause1_gumbel    = risk_gumbel$cause1,
  cause2_gumbel    = risk_gumbel$cause2,
  survival_gumbel  = risk_gumbel$survival
)

# ---------- Function to get predicted highest risk ----------
max_risk_cause <- function(risk_df_model) {
  apply(risk_df_model, 1, function(x) {
    cause_names <- c("Cause1", "Cause2", "Survival")
    cause_names[which.max(x)]
  })
}

risk_df$max_risk_gauss   <- max_risk_cause(risk_gauss)
risk_df$max_risk_clayton <- max_risk_cause(risk_clayton)
risk_df$max_risk_gumbel  <- max_risk_cause(risk_gumbel)

# ---------- Function to summarize max-risk counts ----------
summarize_max_risk <- function(risk_col) {
  df <- as.data.frame.table(risk_col)
  colnames(df) <- c("Cause", "Count")
  df$Percent <- round(100 * df$Count / sum(df$Count), 1)
  return(df)
}

summary_gauss   <- summarize_max_risk(risk_df$max_risk_gauss)
summary_clayton <- summarize_max_risk(risk_df$max_risk_clayton)
summary_gumbel  <- summarize_max_risk(risk_df$max_risk_gumbel)

summary_gauss$Model   <- "Copula-CR Gaussian"
summary_clayton$Model <- "Copula-CR Clayton"
summary_gumbel$Model  <- "Copula-CR Gumbel"

summary_max_risk <- rbind(summary_gauss, summary_clayton, summary_gumbel)
summary_max_risk <- summary_max_risk[, c("Model", "Cause", "Count", "Percent")]

# ---------- Preview results ----------
print(summary_max_risk)


# ================================================================
# 6️⃣ Cause-specific C-index + Bootstrap 95% CI
# ================================================================
library(Hmisc)  # for rcorr.cens

# Safe bootstrap CI function
ci_safe <- function(b){
  if(all(is.na(b$t)) || sd(b$t, na.rm=TRUE)==0){
    return(c(NA, NA))
  } else {
    return(boot.ci(b,type="perc")$percent[4:5])
  }
}

# Cause-specific C-index function with event check
cindex_cause_boot_fn <- function(data, indices, model_type, cause=1, pretrained_models=NULL){
  d <- data[indices, ]
  y_true <- d$event
  
  # Not enough events → return NA
  if(sum(y_true==cause) < 2) return(NA)
  
  # Cox model
  if(model_type=="cox"){
    fm <- coxph(Surv(time,event)~., data=df_train[, c("time","event",covariates)])
    score <- predict(fm, newdata=d)
    
  # Keras-based models
  } else {
    m <- pretrained_models[[model_type]]
    Xd <- array(as.matrix(d[,covariates]), dim=c(nrow(d),1,length(covariates)))
    p <- m %>% predict(Xd)
    # If model outputs multiple causes, select appropriate cause
    score <- p[,min(cause,ncol(p))]
  }
  
  # Compute C-index using Hmisc::rcorr.cens
  rcorr.cens(score, Surv(d$time, ifelse(y_true==cause,1,0)))["C Index"]
}

# Pretrained model list
pretrained_all <- list(
  cox = NULL,
  ce = model_ce,
  copula_cr_gauss = model_cr_gauss,
  copula_cr_clayton = model_cr_clayton,
  copula_cr_gumbel = model_cr_gumbel
)

# Bootstrap
library(boot)
B_boot <- 50
set.seed(2025)
boot_models <- c("cox","ce","copula_cr_gauss","copula_cr_clayton","copula_cr_gumbel")

boot_results_c1 <- lapply(boot_models, function(m) boot(df_test, function(d,i) cindex_cause_boot_fn(d,i,m,cause=1,pretrained_all), R=B_boot))
boot_results_c2 <- lapply(boot_models, function(m) boot(df_test, function(d,i) cindex_cause_boot_fn(d,i,m,cause=2,pretrained_all), R=B_boot))
names(boot_results_c1) <- names(boot_results_c2) <- boot_models

# Summarize C-index + 95% CI
summary_cause_ci <- data.frame(
  Model = c("Cox","CE","Copula-CR Gaussian","Copula-CR Clayton","Copula-CR Gumbel"),
  C_index_Cause1 = sapply(boot_results_c1, function(b) mean(b$t, na.rm=TRUE)),
  CI_lower_Cause1 = sapply(boot_results_c1, function(b) ci_safe(b)[1]),
  CI_upper_Cause1 = sapply(boot_results_c1, function(b) ci_safe(b)[2]),
  C_index_Cause2 = sapply(boot_results_c2, function(b) mean(b$t, na.rm=TRUE)),
  CI_lower_Cause2 = sapply(boot_results_c2, function(b) ci_safe(b)[1]),
  CI_upper_Cause2 = sapply(boot_results_c2, function(b) ci_safe(b)[2])
)

print(summary_cause_ci)

# ================================================================
# 7️⃣ Optional: Barplot of Cause-specific C-index
# ================================================================
df_plot <- summary_cause_ci %>%
  pivot_longer(cols=c(C_index_Cause1,C_index_Cause2,CI_lower_Cause1,CI_lower_Cause2,CI_upper_Cause1,CI_upper_Cause2),
               names_to=c(".value","Cause"),
               names_pattern="(.*)_(Cause[12])") %>%
  mutate(Cause = ifelse(Cause=="Cause1","Cause 1","Cause 2"))

ggplot(df_plot, aes(x=Model, y=C_index, fill=Cause)) +
  geom_col(position="dodge") +
  geom_errorbar(aes(ymin=CI_lower, ymax=CI_upper), width=0.2, position=position_dodge(0.9)) +
  ylim(0,1) +
  theme(axis.text.x=element_text(angle=45,hjust=1)) +
  labs(title="Cause-specific C-index with 95% Bootstrap CI",
       y="C-index", x="", fill="Cause")


# ================================================================
# Real Dara (PBC)
# ================================================================
library(JM)
library(survival)
library(dplyr)
library(tidyr)
library(keras)
library(tensorflow)
library(ggplot2)
library(MASS)
library(cmprsk)
library(survminer)
library(boot)

# ================================================================
# 1️⃣ Load & Prepare PBC dataset
# ================================================================
data(pbc2.id)

df <- pbc2.id %>%
  dplyr::select(id, years, status2,
                age, sex,
                serChol, albumin, SGOT, serBilir,
                spiders, ascites, hepatomegaly) %>%
  dplyr::mutate(
    sex = as.numeric(sex == "male"),
    spiders = as.numeric(spiders == "Yes"),
    ascites = as.numeric(ascites == "Yes"),
    hepatomegaly = as.numeric(hepatomegaly == "Yes"),
    event = status2,        # 0=censor, 1=death, 2=transplant
    time  = years
  ) %>%
  tidyr::drop_na()

covariates <- c("age","sex","serChol","albumin","SGOT","serBilir",
                "spiders","ascites","hepatomegaly")

# Train/test split
set.seed(123)
idx_train <- sample(seq_len(nrow(df)), floor(0.8*nrow(df)))
df_train <- df[idx_train, ]
df_test  <- df[-idx_train, ]

# Keras input arrays
X_train <- array(as.matrix(df_train[, covariates]), dim = c(nrow(df_train), 1, length(covariates)))
X_test  <- array(as.matrix(df_test[, covariates]), dim = c(nrow(df_test), 1, length(covariates)))

# One-hot encoding for binary event
df_train$keras_event <- ifelse(df_train$event > 0, 1, 0)
df_test$keras_event  <- ifelse(df_test$event > 0, 1, 0)
y_train <- to_categorical(df_train$keras_event, 2)
y_test  <- to_categorical(df_test$keras_event, 2)

# Cause-specific one-hot for competing risks
df_train$event1 <- ifelse(df_train$event==1,1,0)
df_train$event2 <- ifelse(df_train$event==2,1,0)
y_train_cr <- as.matrix(cbind(1 - df_train$keras_event, df_train$event1, df_train$event2))

df_test$event1 <- ifelse(df_test$event==1,1,0)
df_test$event2 <- ifelse(df_test$event==2,1,0)
y_test_cr <- as.matrix(cbind(1 - df_test$keras_event, df_test$event1, df_test$event2))

# ================================================================
# 2️⃣ Helper functions: Copula / CIF transformation
# ================================================================
to_uniform_tf <- function(x){
  u <- tf$math$sigmoid(tf$cast(x, tf$float32))
  tf$clip_by_value(u, 1e-6, 1-1e-6)
}

u_to_z <- function(u){
  tf$sqrt(2.0) * tf$math$erfinv(2.0*tf$clip_by_value(u,1e-6,1-1e-6) - 1.0)
}

# Copula-based losses
copula_gaussian_loss <- function(y_true, y_pred){
  rho <- tf$constant(0.5, dtype=tf$float32)
  rho <- tf$clip_by_value(rho, -0.99, 0.99)
  
  u <- to_uniform_tf(y_pred)
  z <- u_to_z(u)
  
  z1 <- z[,1]; z2 <- z[,2]
  detR <- 1 - rho^2
  invR <- 1 / detR
  quad <- invR * (z1^2 + z2^2 - 2*rho*z1*z2)
  loss_term <- -0.5*quad - 0.5*tf$math$log(detR)
  -tf$reduce_mean(loss_term)
}

copula_clayton_loss <- function(y_true, y_pred){
  theta <- tf$constant(1.5, dtype=tf$float32)
  theta <- tf$maximum(theta, 0.01)
  
  u <- to_uniform_tf(y_pred)
  S <- tf$reduce_sum(u^(-theta), axis=1L)
  log_c <- tf$math$log(1+theta) - (theta+1)*tf$reduce_sum(tf$math$log(u), axis=1L) - (2 + 1/theta)*tf$math$log(S - 1 + 1e-6)
  -tf$reduce_mean(log_c)
}

copula_gumbel_loss <- function(y_true, y_pred){
  theta <- tf$constant(2.0, dtype=tf$float32)
  theta <- tf$maximum(theta, 1.001)
  
  u <- to_uniform_tf(y_pred)
  log_u <- -tf$math$log(u)
  phi <- log_u^theta
  S <- tf$reduce_sum(phi, axis=1L)
  log_C <- -S^(1/theta)
  
  log_c <- log_C +
    (1/theta - 1) * tf$math$log(S + 1e-6) +
    tf$reduce_sum((theta-1)*tf$math$log(log_u + 1e-6) - tf$math$log(u + 1e-6), axis=1L) +
    tf$math$log(1 + (theta-1)*S^(-1/theta))
  
  -tf$reduce_mean(log_c)
}

# CIF-based competing risks loss
competing_risks_loss <- function(y_true, y_pred){
  event_mask <- tf$reduce_sum(y_true[,2:3], axis=1L)
  cause_mask1 <- y_true[,2]; cause_mask2 <- y_true[,3]
  
  u <- to_uniform_tf(y_pred)
  cif1 <- u[,1]; cif2 <- u[,2]
  
  ll1 <- cause_mask1 * tf$math$log(cif1 + 1e-6)
  ll2 <- cause_mask2 * tf$math$log(cif2 + 1e-6)
  surv_prob <- 1 - cif1 - cif2
  ll_cens <- (1 - event_mask) * tf$math$log(surv_prob + 1e-6)
  
  -tf$reduce_mean(ll1 + ll2 + ll_cens)
}

# ================================================================
# 3️⃣ CNN-LSTM model builders
# ================================================================
create_model <- function(loss_fn, n_causes=2){
  model <- keras_model_sequential() %>%
    layer_conv_1d(filters=16, kernel_size=1, activation="relu", input_shape=c(1,length(covariates))) %>%
    layer_conv_1d(filters=16, kernel_size=1, activation="relu") %>%
    layer_lstm(16) %>%
    layer_dense(16, activation="relu") %>%
    layer_dense(n_causes, activation=ifelse(n_causes==1,"softmax","sigmoid"))
  
  model %>% compile(loss=loss_fn, optimizer=optimizer_adam(0.001), metrics="accuracy")
  return(model)
}

# ================================================================
# 4️⃣ Train models
# ================================================================
model_ce      <- create_model("categorical_crossentropy")
model_gauss   <- create_model(copula_gaussian_loss)
model_clayton <- create_model(copula_clayton_loss)
model_gumbel  <- create_model(copula_gumbel_loss)
model_cr_gauss   <- create_model(competing_risks_loss, n_causes=2)
model_cr_clayton <- create_model(competing_risks_loss, n_causes=2)
model_cr_gumbel  <- create_model(competing_risks_loss, n_causes=2)

history_ce      <- model_ce %>% fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)
history_gauss   <- model_gauss %>% fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)
history_clayton <- model_clayton %>% fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)
history_gumbel  <- model_gumbel %>% fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)
history_cr_gauss   <- model_cr_gauss %>% fit(X_train, y_train_cr, epochs=20, batch_size=32, validation_split=0.2, verbose=1)
history_cr_clayton <- model_cr_clayton %>% fit(X_train, y_train_cr, epochs=20, batch_size=32, validation_split=0.2, verbose=1)
history_cr_gumbel  <- model_cr_gumbel %>% fit(X_train, y_train_cr, epochs=20, batch_size=32, validation_split=0.2, verbose=1)

# ================================================================
# 5️⃣ Predict on test set
# ================================================================
pred_ce      <- model_ce %>% predict(X_test)
pred_gauss   <- model_gauss %>% predict(X_test)
pred_clayton <- model_clayton %>% predict(X_test)
pred_gumbel  <- model_gumbel %>% predict(X_test)
pred_cr_gauss   <- model_cr_gauss %>% predict(X_test)
pred_cr_clayton <- model_cr_clayton %>% predict(X_test)
pred_cr_gumbel  <- model_cr_gumbel %>% predict(X_test)

# ================================================================
# 6️⃣ Cause-specific C-index + Bootstrap 95% CI
# ================================================================
pretrained_all <- list(
  ce      = model_ce,
  gauss   = model_gauss,
  clayton = model_clayton,
  gumbel  = model_gumbel,
  copula_cr_gauss   = model_cr_gauss,
  copula_cr_clayton = model_cr_clayton,
  copula_cr_gumbel  = model_cr_gumbel
)

# Safe bootstrap CI function
ci_safe <- function(b){
  if(all(is.na(b$t)) || sd(b$t, na.rm=TRUE)==0){
    return(c(NA, NA))
  } else {
    return(boot.ci(b,type="perc")$percent[4:5])
  }
}

# Cause-specific C-index function with event count check
cindex_cause_boot_fn <- function(data, indices, model_type, cause=1, pretrained_models=NULL){
  d <- data[indices, ]
  y_true <- d$event
  
  # Not enough events → return NA
  if(sum(y_true==cause) < 2) return(NA)
  
  if(model_type=="cox"){
    fm <- coxph(Surv(time,event)~., data=df_train[, c("time","event",covariates)])
    score <- predict(fm, newdata=d)
  } else {
    m <- pretrained_models[[model_type]]
    Xd <- array(as.matrix(d[,covariates]), dim=c(nrow(d),1,length(covariates)))
    p <- m %>% predict(Xd)
    # Adjust for n_causes difference
    score <- p[,min(cause,ncol(p))]
  }
  
  concordance(Surv(d$time, ifelse(y_true==cause,1,0)) ~ score)$concordance
}

B_boot <- 50
set.seed(2025)
boot_models <- c("cox","ce","gauss","clayton","gumbel",
                 "copula_cr_gauss","copula_cr_clayton","copula_cr_gumbel")

boot_results_c1 <- lapply(boot_models, function(m) boot(df_test, function(d,i) cindex_cause_boot_fn(d,i,m,cause=1,pretrained_all), R=B_boot))
boot_results_c2 <- lapply(boot_models, function(m) boot(df_test, function(d,i) cindex_cause_boot_fn(d,i,m,cause=2,pretrained_all), R=B_boot))
names(boot_results_c1) <- names(boot_results_c2) <- boot_models

summary_cause_ci <- data.frame(
  Model = c("Cox","CE","Copula CE","Copula Clayton","Copula Gumbel",
            "Copula-CR Gaussian","Copula-CR Clayton","Copula-CR Gumbel"),
  C_index_Cause1 = sapply(boot_results_c1, function(b) mean(b$t, na.rm=TRUE)),
  CI_lower_Cause1 = sapply(boot_results_c1, function(b) ci_safe(b)[1]),
  CI_upper_Cause1 = sapply(boot_results_c1, function(b) ci_safe(b)[2]),
  C_index_Cause2 = sapply(boot_results_c2, function(b) mean(b$t, na.rm=TRUE)),
  CI_lower_Cause2 = sapply(boot_results_c2, function(b) ci_safe(b)[1]),
  CI_upper_Cause2 = sapply(boot_results_c2, function(b) ci_safe(b)[2])
)
print(summary_cause_ci)

# ================================================================
# 7️⃣ Barplot: Cause-specific C-index with 95% CI
# ================================================================
df_plot <- summary_cause_ci %>%
  pivot_longer(cols=c(C_index_Cause1,C_index_Cause2,CI_lower_Cause1,CI_lower_Cause2,CI_upper_Cause1,CI_upper_Cause2),
               names_to=c(".value","Cause"),
               names_pattern="(.*)_(Cause[12])") %>%
  mutate(Cause = ifelse(Cause=="Cause1","Cause 1","Cause 2"))

ggplot(df_plot, aes(x=Model, y=C_index, fill=Cause)) +
  geom_col(position="dodge") +
  geom_errorbar(aes(ymin=CI_lower, ymax=CI_upper), width=0.2, position=position_dodge(0.9)) +
  ylim(0,1) +
  theme(axis.text.x=element_text(angle=45,hjust=1)) +
  labs(title="Cause-specific C-index with 95% Bootstrap CI",
       y="C-index", x="", fill="Cause")

# ---------- Function to extract predicted risk (safe version) ----------
extract_risk <- function(pred_matrix) {
  # pred_matrix: n x 2 matrix from competing risks model
  # columns: 1=cause1, 2=cause2
  data.frame(
    cause1   = pred_matrix[,1],
    cause2   = pred_matrix[,2],
    survival = 1 - rowSums(pred_matrix)
  )
}

# ---------- Extract risks for all CR models ----------
risk_gauss   <- extract_risk(pred_cr_gauss)
risk_clayton <- extract_risk(pred_cr_clayton)
risk_gumbel  <- extract_risk(pred_cr_gumbel)

# ---------- Combine into a single data.frame ----------
risk_df <- data.frame(
  id = df_test$id,
  
  cause1_gauss     = risk_gauss$cause1,
  cause2_gauss     = risk_gauss$cause2,
  survival_gauss   = risk_gauss$survival,
  
  cause1_clayton   = risk_clayton$cause1,
  cause2_clayton   = risk_clayton$cause2,
  survival_clayton = risk_clayton$survival,
  
  cause1_gumbel    = risk_gumbel$cause1,
  cause2_gumbel    = risk_gumbel$cause2,
  survival_gumbel  = risk_gumbel$survival
)

# ---------- Preview ----------
head(risk_df)

# ---------- Function to get predicted highest risk ----------
max_risk_cause <- function(risk_df_model) {
  apply(risk_df_model, 1, function(x) {
    cause_names <- c("Cause1", "Cause2", "Survival")
    cause_names[which.max(x)]
  })
}

# ---------- Add highest-risk cause columns ----------
risk_df$max_risk_gauss   <- max_risk_cause(risk_gauss)
risk_df$max_risk_clayton <- max_risk_cause(risk_clayton)
risk_df$max_risk_gumbel  <- max_risk_cause(risk_gumbel)

# ---------- Preview ----------
head(risk_df[, c("id",
                 "cause1_gauss", "cause2_gauss", "survival_gauss", "max_risk_gauss",
                 "cause1_clayton", "cause2_clayton", "survival_clayton", "max_risk_clayton",
                 "cause1_gumbel", "cause2_gumbel", "survival_gumbel", "max_risk_gumbel")])
