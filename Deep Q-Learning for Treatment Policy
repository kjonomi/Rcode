# Boston Housing dataset
# --- Libraries ---
library(MASS)       
library(keras)
library(tensorflow)
library(tidyverse)

# --- Parameters ---
T_steps <- 10
n <- 200
epsilon <- 1.0
epsilon_decay <- 0.99
epsilon_min <- 0.1
episodes <- 150
gamma <- 0.95
alpha <- 0.001

# --- Prepare Data ---
data("Boston")
# --- Sample n rows from Boston dataset ---
df <- Boston
p <- ncol(df)
set.seed(123)  # for reproducibility
df_sampled <- df[sample(1:nrow(df), n, replace = TRUE), ]

# --- Simulate time series structure ---
X_long <- array(0, dim = c(n, T_steps, p))
for (t in 1:T_steps) {
  noise <- matrix(rnorm(n * p, 0, 0.01), nrow = n)
  X_long[, t, ] <- as.matrix(df_sampled) + noise
}

# Normalize per feature and time
X_scaled <- X_long
for (j in 1:p) {
  for (t in 1:T_steps) {
    mean_val <- mean(X_scaled[, t, j])
    sd_val <- sd(X_scaled[, t, j])
    X_scaled[, t, j] <- (X_scaled[, t, j] - mean_val) / sd_val
  }
}

# Simulate treatment
rm_avg <- apply(X_scaled[, , which(colnames(df) == "rm")], 1, mean)
W <- rbinom(n, 1, plogis(5 * (rm_avg - 0.5)))

# Simulate outcomes
base_outcome <- apply(X_scaled[, , 1:3], 1, mean) * 5 + rnorm(n, 0, 0.2)
treatment_effect <- 2 + 4 * apply(X_scaled[, , which(colnames(df) == "lstat")], 1, mean)

Y0 <- base_outcome
Y1 <- base_outcome + treatment_effect
Y_obs <- ifelse(W == 1, Y1, Y0)

# Split into train/test
train_idx <- sample(1:n, floor(0.7 * n))
test_idx <- setdiff(1:n, train_idx)

X_train <- X_scaled[train_idx, , ]
X_test <- X_scaled[test_idx, , ]
Y0_test <- Y0[test_idx]
Y1_test <- Y1[test_idx]

# --- Deep Q Network Model ---
create_dqn_model <- function(input_shape) {
  model <- keras_model_sequential() %>%
    layer_conv_1d(filters = 32, kernel_size = 3, activation = 'relu',
                  input_shape = input_shape) %>%
    layer_max_pooling_1d(pool_size = 2) %>%
    layer_lstm(units = 32, return_sequences = FALSE) %>%
    layer_dense(units = 32, activation = 'relu') %>%
    layer_dense(units = 2, activation = 'linear')  # Q-values for 2 actions
  
  model %>% compile(
    loss = 'mse',
    optimizer = optimizer_adam(learning_rate = alpha)
  )
  return(model)
}

input_shape <- c(T_steps, p)
model <- create_dqn_model(input_shape)

# --- Q-learning Loop ---
rewards <- c()
actions <- c()

for (i in 1:episodes) {
  idx <- sample(1:dim(X_train)[1], 1)
  state <- array_reshape(X_train[idx, , ], dim = c(1, T_steps, p))
  
  if (runif(1) < epsilon) {
    action <- sample(0:1, 1)
  } else {
    q_values <- model %>% predict(state, verbose = 0)
    action <- which.max(q_values) - 1
  }
  
  reward <- ifelse(action == 1, Y1[train_idx[idx]], Y0[train_idx[idx]])
  rewards <- c(rewards, reward)
  actions <- c(actions, action)
  
  # Target Q-value (single-step, no future reward here)
  target <- reward
  q_vals <- model %>% predict(state, verbose = 0)
  q_vals[1, action + 1] <- target
  
  # Train the model on this single sample
  model %>% fit(state, q_vals, verbose = 0, epochs = 1)
  
  # Epsilon decay
  epsilon <- max(epsilon * epsilon_decay, epsilon_min)
}

# --- Evaluation ---
q_preds <- model %>% predict(X_test, verbose = 0)
policy_actions <- apply(q_preds, 1, which.max) - 1
rewards_pred <- ifelse(policy_actions == 1, Y1_test, Y0_test)

cat("Total reward collected on test set:", sum(rewards_pred), "\n")
cat("Average reward on test set:", mean(rewards_pred), "\n")
cat("Action frequencies on test set:\n")
print(table(policy_actions))

# --- Summary Tables ---

# Reward summary on training episodes
reward_summary <- data.frame(
  Statistic = c("Total Reward", "Average Reward", "Min Reward", "Max Reward"),
  Value = c(sum(rewards), mean(rewards), min(rewards), max(rewards))
)
print("Reward Summary (Training Episodes):")
print(reward_summary)

# Action frequencies on training episodes
action_table <- table(actions)
action_prop <- prop.table(action_table)
action_summary <- data.frame(
  Action = as.integer(names(action_table)),
  Frequency = as.vector(action_table),
  Proportion = round(as.vector(action_prop), 3)
)
print("Action Frequencies (Training Episodes):")
print(action_summary)

# Action frequencies on test set (evaluation)
eval_action_table <- table(policy_actions)
eval_action_prop <- prop.table(eval_action_table)
eval_action_summary <- data.frame(
  Action = as.integer(names(eval_action_table)),
  Frequency = as.vector(eval_action_table),
  Proportion = round(as.vector(eval_action_prop), 3)
)
print("Action Frequencies (Test Set):")
print(eval_action_summary)

# Reward summary on test set
eval_reward_summary <- data.frame(
  Statistic = c("Total Reward", "Average Reward"),
  Value = c(sum(rewards_pred), mean(rewards_pred))
)
print("Reward Summary (Test Set):")
print(eval_reward_summary)

# Q-value statistics on test set
qvalue_summary <- data.frame(
  Statistic = c("Mean", "SD", "Min", "Max"),
  Q0 = c(mean(q_preds[,1]), sd(q_preds[,1]), min(q_preds[,1]), max(q_preds[,1])),
  Q1 = c(mean(q_preds[,2]), sd(q_preds[,2]), min(q_preds[,2]), max(q_preds[,2]))
)
print("Q-value Summary (Test Set):")
print(qvalue_summary)

# --- Q-value Plot ---
q_df <- data.frame(
  Q0 = q_preds[, 1],
  Q1 = q_preds[, 2],
  action = factor(policy_actions)
)

ggplot(q_df, aes(x = Q0, y = Q1, color = action)) +
  geom_point(alpha = 0.6, size = 3) +
  labs(title = "Q-values per state under learned policy", x = "Q0", y = "Q1") +
  theme_minimal()

# Wine Quality Dataset

# --- Libraries ---
library(data.table)
library(keras)
library(tensorflow)
library(tidyverse)

# --- Parameters ---
T_steps <- 10
n <- 200
epsilon <- 1.0
epsilon_decay <- 0.99
epsilon_min <- 0.1
episodes <- 150
gamma <- 0.95
alpha <- 0.001

# --- Load and Prepare Wine Dataset ---
wine_url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
wine <- fread(wine_url, sep = ";")
p <- ncol(wine) - 1  # Exclude 'quality' as it's the target
features <- setdiff(names(wine), "quality")
df_sampled <- wine[sample(1:nrow(wine), n, replace = TRUE), ..features]

# --- Simulate Time Series from Static Features ---
X_long <- array(0, dim = c(n, T_steps, p))
for (t in 1:T_steps) {
  noise <- matrix(rnorm(n * p, 0, 0.01), nrow = n)
  X_long[, t, ] <- as.matrix(df_sampled) + noise
}

# --- Normalize (feature-wise, time-wise) ---
X_scaled <- X_long
for (j in 1:p) {
  for (t in 1:T_steps) {
    mean_val <- mean(X_scaled[, t, j])
    sd_val <- sd(X_scaled[, t, j])
    X_scaled[, t, j] <- (X_scaled[, t, j] - mean_val) / sd_val
  }
}

# --- Simulate Treatment Assignment ---
alcohol_avg <- apply(X_scaled[, , which(features == "alcohol")], 1, mean)
W <- rbinom(n, 1, plogis(2 * (alcohol_avg - mean(alcohol_avg))))

# --- Simulate Outcomes ---
# Y0 = baseline outcome; Y1 = treatment effect
base_outcome <- apply(X_scaled[, , 1:3], 1, mean) * 5 + rnorm(n, 0, 0.2)
sulphate_avg <- apply(X_scaled[, , which(features == "sulphates")], 1, mean)
treatment_effect <- 2 + 3 * sulphate_avg

Y0 <- base_outcome
Y1 <- base_outcome + treatment_effect
Y_obs <- ifelse(W == 1, Y1, Y0)

# --- Train/Test Split ---
train_idx <- sample(1:n, floor(0.7 * n))
test_idx <- setdiff(1:n, train_idx)

X_train <- X_scaled[train_idx, , ]
X_test <- X_scaled[test_idx, , ]
Y0_test <- Y0[test_idx]
Y1_test <- Y1[test_idx]

# --- Deep Q Network ---
create_dqn_model <- function(input_shape) {
  keras_model_sequential() %>%
    layer_conv_1d(filters = 32, kernel_size = 3, activation = 'relu',
                  input_shape = input_shape) %>%
    layer_max_pooling_1d(pool_size = 2) %>%
    layer_lstm(units = 32, return_sequences = FALSE) %>%
    layer_dense(units = 32, activation = 'relu') %>%
    layer_dense(units = 2, activation = 'linear') %>%
    compile(loss = 'mse', optimizer = optimizer_adam(learning_rate = alpha))
}

input_shape <- c(T_steps, p)
model <- create_dqn_model(input_shape)

# --- Q-learning Loop ---
rewards <- c()
actions <- c()

for (i in 1:episodes) {
  idx <- sample(1:dim(X_train)[1], 1)
  state <- array_reshape(X_train[idx, , ], dim = c(1, T_steps, p))
  
  # Îµ-greedy action
  if (runif(1) < epsilon) {
    action <- sample(0:1, 1)
  } else {
    q_values <- model %>% predict(state, verbose = 0)
    action <- which.max(q_values) - 1
  }
  
  # Reward
  reward <- ifelse(action == 1, Y1[train_idx[idx]], Y0[train_idx[idx]])
  rewards <- c(rewards, reward)
  actions <- c(actions, action)
  
  # Update Q-values
  q_vals <- model %>% predict(state, verbose = 0)
  q_vals[1, action + 1] <- reward
  model %>% fit(state, q_vals, verbose = 0, epochs = 1)
  
  # Decay epsilon
  epsilon <- max(epsilon * epsilon_decay, epsilon_min)
}

# --- Evaluation ---
q_preds <- model %>% predict(X_test, verbose = 0)
policy_actions <- apply(q_preds, 1, which.max) - 1
rewards_pred <- ifelse(policy_actions == 1, Y1_test, Y0_test)

# --- Summaries ---
cat("Total reward collected on test set:", sum(rewards_pred), "\n")
cat("Average reward on test set:", mean(rewards_pred), "\n")

# Action Frequencies
print("Action Frequencies (Test Set):")
print(table(policy_actions))

# Reward summary (training episodes)
reward_summary <- data.frame(
  Statistic = c("Total", "Average", "Min", "Max"),
  Reward = c(sum(rewards), mean(rewards), min(rewards), max(rewards))
)
print(reward_summary)

# Action summary (training)
action_summary <- data.frame(
  Action = as.integer(names(table(actions))),
  Frequency = as.vector(table(actions)),
  Proportion = round(prop.table(table(actions)), 3)
)
print(action_summary)

# Action summary (test)
eval_action_summary <- data.frame(
  Action = as.integer(names(table(policy_actions))),
  Frequency = as.vector(table(policy_actions)),
  Proportion = round(prop.table(table(policy_actions)), 3)
)
print(eval_action_summary)

# Test set reward summary
eval_reward_summary <- data.frame(
  Statistic = c("Total Reward", "Average Reward"),
  Value = c(sum(rewards_pred), mean(rewards_pred))
)
print(eval_reward_summary)

# Q-value stats
qvalue_summary <- data.frame(
  Statistic = c("Mean", "SD", "Min", "Max"),
  Q0 = c(mean(q_preds[,1]), sd(q_preds[,1]), min(q_preds[,1]), max(q_preds[,1])),
  Q1 = c(mean(q_preds[,2]), sd(q_preds[,2]), min(q_preds[,2]), max(q_preds[,2]))
)
print(qvalue_summary)

# --- Q-value Plot ---
q_df <- data.frame(
  Q0 = q_preds[, 1],
  Q1 = q_preds[, 2],
  action = factor(policy_actions)
)
ggplot(q_df, aes(x = Q0, y = Q1, color = action)) +
  geom_point(alpha = 0.6, size = 3) +
  labs(title = "Q-values per state under learned policy", x = "Q0", y = "Q1") +
  theme_minimal()
