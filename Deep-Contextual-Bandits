############################################################
## Multivariate Contextual Bandit
## DR + IPS + Empirical Copula (Boston Housing)
############################################################

library(MASS)
library(keras)
library(tensorflow)
library(dplyr)

set.seed(123)

# ==========================================================
# 1Ô∏è‚É£ PARAMETERS
# ==========================================================
n <- 500
T_steps <- 10
episodes <- 400
epsilon <- 1
epsilon_decay <- 0.995
epsilon_min <- 0.05
alpha <- 0.001
rho <- 0.7          # AR(1) temporal correlation
K_out <- 4          # number of outcomes

# ==========================================================
# 2Ô∏è‚É£ Load Boston Housing Dataset
# ==========================================================
data("Boston")
df <- Boston
p <- ncol(df)

# Sample n rows with replacement if needed
df_sampled <- df[sample(1:nrow(df), n, replace = TRUE), ]
X_num <- scale(as.matrix(df_sampled)) # all numeric columns

# ==========================================================
# 3Ô∏è‚É£ AR(1) TEMPORAL PROCESS
# ==========================================================
X_long <- array(0, c(n, T_steps, p))
X_long[,1,] <- X_num

for(t in 2:T_steps){
  noise <- matrix(rnorm(n*p, 0, 0.1), n)
  X_long[,t,] <- rho*X_long[,t-1,] + noise
}

X_scaled <- array_reshape(scale(array_reshape(X_long, c(n*T_steps, p))),
                          c(n, T_steps, p))

# ==========================================================
# 4Ô∏è‚É£ MULTI-CONFOUNDER PROPENSITY MODEL
# ==========================================================
rm_avg    <- apply(X_scaled[,,which(colnames(df)=="rm")], 1, mean)
lstat_avg <- apply(X_scaled[,,which(colnames(df)=="lstat")], 1, mean)
crim_avg  <- apply(X_scaled[,,which(colnames(df)=="crim")], 1, mean)

lin_pred <- 1.2*rm_avg - 1.0*lstat_avg + 0.7*crim_avg
propensity <- plogis(lin_pred)

W <- rbinom(n, 1, propensity)

# ==========================================================
# 5Ô∏è‚É£ NONLINEAR MULTIVARIATE POTENTIAL OUTCOMES
# ==========================================================
Y0_mat <- matrix(0, n, K_out)
Y1_mat <- matrix(0, n, K_out)

for(k in 1:K_out){
  base_k <- sin(rm_avg + k*0.2) +
    log(abs(lstat_avg)+1) +
    rnorm(n,0,0.2)
  tau_k <- 1.5 + 0.5*k + 0.8*(rm_avg^2) - 0.3*lstat_avg
  Y0_mat[,k] <- base_k
  Y1_mat[,k] <- base_k + tau_k
}

# ==========================================================
# 6Ô∏è‚É£ EMPIRICAL COPULA TRANSFORMATION
# ==========================================================
empirical_copula <- function(Y){
  apply(Y, 2, function(col){
    rank(col, ties.method="average")/length(col)
  })
}

Y0_cop <- empirical_copula(Y0_mat)
Y1_cop <- empirical_copula(Y1_mat)

# Observed outcomes
Y_obs_mat <- Y0_cop
Y_obs_mat[W==1, ] <- Y1_cop[W==1, ]

reward_scalar <- rowMeans(Y_obs_mat)

# ==========================================================
# 7Ô∏è‚É£ TRAIN / TEST SPLIT
# ==========================================================
train_idx <- sample(1:n, 0.7*n)
test_idx  <- setdiff(1:n, train_idx)

X_train <- X_scaled[train_idx,,]
X_test  <- X_scaled[test_idx,,]

# ==========================================================
# 8Ô∏è‚É£ CONTEXTUAL BANDIT NETWORK
# ==========================================================
create_model <- function(input_shape){
  keras_model_sequential() %>%
    layer_conv_1d(32, 3, activation="relu", input_shape=input_shape) %>%
    layer_lstm(32) %>%
    layer_dense(32, activation="relu") %>%
    layer_dense(2, activation="linear") %>%
    compile(loss="mse", optimizer=optimizer_adam(alpha))
}

model <- create_model(c(T_steps, p))

# ==========================================================
# 9Ô∏è‚É£ TRAINING LOOP
# ==========================================================
for(ep in 1:episodes){
  i <- sample(train_idx, 1)
  idx_i <- which(train_idx == i)
  
  state <- array_reshape(X_train[idx_i,,], c(1, T_steps, p))
  
  if(runif(1) < epsilon){
    action <- sample(0:1,1)
  } else{
    qvals <- model %>% predict(state, verbose=0)
    action <- which.max(qvals)-1
  }
  
  reward <- if(action==1) mean(Y1_cop[i,]) else mean(Y0_cop[i,])
  
  qvals <- model %>% predict(state, verbose=0)
  qvals[1, action+1] <- reward
  
  model %>% fit(state, qvals, epochs=1, verbose=0)
  
  epsilon <- max(epsilon*epsilon_decay, epsilon_min)
}

# ==========================================================
# üîü POLICY EVALUATION
# ==========================================================
q_test <- model %>% predict(X_test, verbose=0)
pi_hat <- apply(q_test,1,which.max)-1

# Preserve matrix structure for multivariate reward
Y_policy_mat <- Y0_cop[test_idx, ]
Y_policy_mat[pi_hat==1, ] <- Y1_cop[test_idx, ][pi_hat==1, ]

R_policy <- rowMeans(Y_policy_mat)
R_opt <- rowMeans(pmax(Y0_cop[test_idx, ], Y1_cop[test_idx, ]))

cat("Mean Policy Reward:", mean(R_policy), "\n")
cat("Mean Regret:", mean(R_opt - R_policy), "\n")

# ==========================================================
# 1Ô∏è‚É£2Ô∏è‚É£ IPS ESTIMATOR
# ==========================================================
Y_obs_scalar <- rowMeans(Y_obs_mat)

IPS <- mean(
  ifelse(pi_hat==W[test_idx],
         Y_obs_scalar[test_idx] / ifelse(W[test_idx]==1, propensity[test_idx], 1-propensity[test_idx]),
         0)
)

cat("IPS Estimate:", IPS, "\n")

# ==========================================================
# 1Ô∏è‚É£3Ô∏è‚É£ DOUBLY ROBUST ESTIMATOR
# ==========================================================
DR_vec <- numeric(K_out)

for(k in 1:K_out){
  df_out <- data.frame(
    Y = Y_obs_mat[,k],
    rm_avg = rm_avg,
    lstat_avg = lstat_avg,
    crim_avg = crim_avg,
    W = W
  )
  
  outcome_model <- lm(Y ~ rm_avg + lstat_avg + crim_avg + W, data=df_out)
  
  mu1 <- predict(outcome_model,
                 newdata = data.frame(
                   rm_avg = rm_avg[test_idx],
                   lstat_avg = lstat_avg[test_idx],
                   crim_avg = crim_avg[test_idx],
                   W = 1))
  
  mu0 <- predict(outcome_model,
                 newdata = data.frame(
                   rm_avg = rm_avg[test_idx],
                   lstat_avg = lstat_avg[test_idx],
                   crim_avg = crim_avg[test_idx],
                   W = 0))
  
  DR_vec[k] <- mean(
    ifelse(pi_hat==1, mu1, mu0) +
      ifelse(pi_hat==W[test_idx],
             (Y_obs_mat[test_idx,k] - ifelse(W[test_idx]==1, mu1, mu0)) /
               ifelse(W[test_idx]==1, propensity[test_idx], 1-propensity[test_idx]),
             0)
  )
}

DR_estimate <- mean(DR_vec)
cat("DR Estimate (multivariate):", DR_estimate, "\n")

# ==========================================================
# 1Ô∏è‚É£4Ô∏è‚É£ BASELINES
# ==========================================================
always_treat <- mean(rowMeans(Y1_cop[test_idx, ]))
never_treat  <- mean(rowMeans(Y0_cop[test_idx, ]))

rand_choice <- rbinom(length(test_idx), 1, 0.5)
Y_rand_mat <- Y0_cop[test_idx, ]
Y_rand_mat[rand_choice==1, ] <- Y1_cop[test_idx, ][rand_choice==1, ]
random_pol <- mean(rowMeans(Y_rand_mat))


# ----------------------------------------------------------
# PRINT RESULTS
# ----------------------------------------------------------
cat("Mean Policy Reward:", mean(R_policy), "\n")
cat("Mean Regret:", mean(R_opt - R_policy), "\n")
cat("IPS Estimate:", IPS, "\n")
cat("DR Estimate (multivariate):", DR_estimate, "\n")
cat("Always Treat:", always_treat, "\n")
cat("Never Treat:", never_treat, "\n")
cat("Random Policy:", random_pol, "\n")

# ----------------------------------------------------------
# VISUALIZATIONS
# ----------------------------------------------------------
policy_rewards <- data.frame(
  Policy = c("Always Treat","Never Treat","Random","Learned Policy"),
  Reward = c(always_treat, never_treat, random_pol, mean(R_policy))
)

ggplot(policy_rewards, aes(x=Policy, y=Reward, fill=Policy)) +
  geom_bar(stat="identity") +
  theme_minimal() +
  ggtitle("Mean Policy Rewards (Boston Housing)") +
  ylab("Mean Reward") +
  theme(legend.position="none")

estimates <- data.frame(
  Estimator = c("IPS","Doubly Robust"),
  Value = c(IPS, DR_estimate)
)

ggplot(estimates, aes(x=Estimator, y=Value, fill=Estimator)) +
  geom_bar(stat="identity") +
  theme_minimal() +
  ggtitle("Policy Value Estimates (Boston Housing)") +
  ylab("Estimated Reward") +
  theme(legend.position="none")

regret_vals <- R_opt - R_policy
ggplot(data.frame(Regret=regret_vals), aes(x=Regret)) +
  geom_histogram(bins=20, fill="steelblue", color="black", alpha=0.7) +
  theme_minimal() +
  ggtitle("Histogram of Regret (Boston Housing)") +
  xlab("Regret") +
  ylab("Frequency")

# ==========================================================
# 2Ô∏è‚É£ Track episode-wise rewards (optional)
# ==========================================================
# For demonstration, we can track mean reward every 20 episodes
episode_rewards <- numeric(episodes)
epsilon_tmp <- epsilon
for(ep in 1:episodes){
  i <- sample(train_idx,1)
  idx_i <- which(train_idx==i)
  
  state <- array_reshape(X_train[idx_i,,], c(1,T_steps,p))
  if(runif(1)<epsilon_tmp){
    action <- sample(0:1,1)
  } else{
    qvals <- model %>% predict(state, verbose=0)
    action <- which.max(qvals)-1
  }
  
  reward <- if(action==1) mean(Y1_cop[i,]) else mean(Y0_cop[i,])
  episode_rewards[ep] <- reward
  
  epsilon_tmp <- max(epsilon_tmp*epsilon_decay, epsilon_min)
}


episode_df <- data.frame(
  Episode = 1:episodes,
  Reward = episode_rewards
)

ggplot(episode_df, aes(x=Episode, y=Reward)) +
  geom_line(color="blue") +
  geom_smooth(method="loess", color="red", se=FALSE) +
  ggtitle("Episode-wise Reward Learning (Boston Housing)") +
  theme_minimal()


############################################################
## Multivariate Contextual Bandit
## DR + IPS + Empirical Copula
## Wine Quality Dataset
############################################################

library(data.table)
library(keras)
library(tensorflow)
library(tidyverse)

set.seed(123)

# ----------------------------------------------------------
# PARAMETERS
# ----------------------------------------------------------
n <- 500
T_steps <- 10
episodes <- 400
epsilon <- 1
epsilon_decay <- 0.995
epsilon_min <- 0.05
alpha <- 0.001
rho <- 0.7          # AR(1)
K_out <- 4          # number of outcomes

# ----------------------------------------------------------
# LOAD WINE QUALITY DATA
# ----------------------------------------------------------
wine <- fread("winequality-red.csv", sep=";")
Y_target <- as.numeric(wine$quality >= 6)
wine[, quality := NULL]  # remove the column

if(nrow(wine) > 3000)  # downsample for speed
  wine <- wine[sample(1:nrow(wine), 3000), ]

p <- ncol(wine)

# Sample n rows
df_sampled <- wine[sample(1:nrow(wine), n, replace=TRUE), ]

# ----------------------------------------------------------
# AR(1) TEMPORAL PROCESS
# ----------------------------------------------------------
X_long <- array(0, c(n,T_steps,p))
X_long[,1,] <- as.matrix(df_sampled)

for(t in 2:T_steps){
  noise <- matrix(rnorm(n*p,0,0.1), n)
  X_long[,t,] <- rho*X_long[,t-1,] + noise
}

# Global standardization
X_scaled <- scale(array_reshape(X_long,c(n*T_steps,p)))
X_scaled <- array(X_scaled,c(n,T_steps,p))

# ----------------------------------------------------------
# MULTI-CONFOUNDER PROPENSITY MODEL (using first 3 features)
# ----------------------------------------------------------
feat1_avg <- apply(X_scaled[,,1],1,mean)
feat2_avg <- apply(X_scaled[,,2],1,mean)
feat3_avg <- apply(X_scaled[,,3],1,mean)

lin_pred <- 1.2*feat1_avg - 0.8*feat2_avg + 0.5*feat3_avg
propensity <- plogis(lin_pred)

W <- rbinom(n,1,propensity)

# ----------------------------------------------------------
# NONLINEAR MULTIVARIATE POTENTIAL OUTCOMES
# ----------------------------------------------------------
Y0_mat <- matrix(0,n,K_out)
Y1_mat <- matrix(0,n,K_out)

for(k in 1:K_out){
  base_k <- sin(feat1_avg + k*0.2) +
    log(abs(feat2_avg)+1) +
    rnorm(n,0,0.2)
  
  tau_k <- 1.5 + 0.5*k +
    0.8*(feat1_avg^2) -
    0.3*feat2_avg
  
  Y0_mat[,k] <- base_k
  Y1_mat[,k] <- base_k + tau_k
}

# ----------------------------------------------------------
# EMPIRICAL COPULA TRANSFORMATION
# ----------------------------------------------------------
empirical_copula <- function(Y){
  apply(Y,2,function(col){
    rank(col,ties.method="average")/length(col)
  })
}

Y0_cop <- empirical_copula(Y0_mat)
Y1_cop <- empirical_copula(Y1_mat)

# Observed outcomes
Y_obs_mat <- matrix(0, n, K_out)
for(i in 1:n){
  Y_obs_mat[i,] <- if(W[i]==1) Y1_cop[i,] else Y0_cop[i,]
}

# Scalar reward for training
reward_scalar <- rowMeans(Y_obs_mat)

# ----------------------------------------------------------
# TRAIN / TEST SPLIT
# ----------------------------------------------------------
train_idx <- sample(1:n, 0.7*n)
test_idx  <- setdiff(1:n, train_idx)

X_train <- X_scaled[train_idx,,]
X_test  <- X_scaled[test_idx,,]

# ----------------------------------------------------------
# CONTEXTUAL BANDIT NETWORK
# ----------------------------------------------------------
create_model <- function(input_shape){
  keras_model_sequential() %>%
    layer_conv_1d(32,3,activation="relu", input_shape=input_shape) %>%
    layer_lstm(32) %>%
    layer_dense(32,activation="relu") %>%
    layer_dense(2,activation="linear") %>%
    compile(loss="mse", optimizer=optimizer_adam(alpha))
}

model <- create_model(c(T_steps,p))

# ----------------------------------------------------------
# TRAINING LOOP
# ----------------------------------------------------------
episode_rewards <- numeric(episodes)  # track per-episode reward

for(ep in 1:episodes){
  i <- sample(train_idx,1)
  idx_i <- which(train_idx==i)
  
  state <- array_reshape(X_train[idx_i,,], c(1,T_steps,p))
  
  if(runif(1)<epsilon){
    action <- sample(0:1,1)
  } else{
    qvals <- model %>% predict(state,verbose=0)
    action <- which.max(qvals)-1
  }
  
  reward <- if(action==1) mean(Y1_cop[i,]) else mean(Y0_cop[i,])
  episode_rewards[ep] <- reward
  
  qvals <- model %>% predict(state,verbose=0)
  qvals[1,action+1] <- reward
  
  model %>% fit(state, qvals, epochs=1, verbose=0)
  
  epsilon <- max(epsilon*epsilon_decay, epsilon_min)
}


# ----------------------------------------------------------
# POLICY EVALUATION
# ----------------------------------------------------------
q_test <- model %>% predict(X_test, verbose=0)
pi_hat <- apply(q_test,1,which.max)-1  # greedy policy

# Assign outcomes according to policy
Y1_test <- Y1_cop[test_idx,,drop=FALSE]
Y0_test <- Y0_cop[test_idx,,drop=FALSE]
Y_policy_mat <- matrix(0, nrow=length(test_idx), ncol=K_out)

for(j in 1:length(test_idx)){
  Y_policy_mat[j,] <- if(pi_hat[j]==1) Y1_test[j,] else Y0_test[j,]
}

# Policy reward
R_policy <- rowMeans(Y_policy_mat)
# Oracle reward
R_opt <- rowMeans(pmax(Y0_test, Y1_test))

cat("Mean Policy Reward:", mean(R_policy), "\n")
cat("Mean Regret:", mean(R_opt - R_policy), "\n")

# ----------------------------------------------------------
# IPS ESTIMATOR
# ----------------------------------------------------------
Y_obs_scalar <- rowMeans(Y_obs_mat)

IPS <- mean(
  ifelse(pi_hat==W[test_idx],
         Y_obs_scalar[test_idx]/
           ifelse(W[test_idx]==1, propensity[test_idx], 1-propensity[test_idx]),
         0)
)

cat("IPS Estimate:", IPS, "\n")

# ----------------------------------------------------------
# DOUBLY ROBUST ESTIMATOR
# ----------------------------------------------------------
DR_vec <- numeric(K_out)

for(k in 1:K_out){
  df_out <- data.frame(
    Y = Y_obs_mat[,k],
    feat1_avg = feat1_avg,
    feat2_avg = feat2_avg,
    feat3_avg = feat3_avg,
    W = W
  )
  
  outcome_model <- lm(Y ~ feat1_avg + feat2_avg + feat3_avg + W, data=df_out)
  
  mu1 <- predict(outcome_model, newdata=data.frame(
    feat1_avg = feat1_avg[test_idx],
    feat2_avg = feat2_avg[test_idx],
    feat3_avg = feat3_avg[test_idx],
    W = 1
  ))
  
  mu0 <- predict(outcome_model, newdata=data.frame(
    feat1_avg = feat1_avg[test_idx],
    feat2_avg = feat2_avg[test_idx],
    feat3_avg = feat3_avg[test_idx],
    W = 0
  ))
  
  DR_vec[k] <- mean(
    ifelse(pi_hat==1, mu1, mu0) +
      ifelse(pi_hat==W[test_idx],
             (Y_obs_mat[test_idx,k] - ifelse(W[test_idx]==1, mu1, mu0)) /
               ifelse(W[test_idx]==1, propensity[test_idx], 1-propensity[test_idx]),
             0)
  )
}

DR_estimate <- mean(DR_vec)
cat("DR Estimate (multivariate):", DR_estimate, "\n")


# ==========================================================
# 1Ô∏è‚É£4Ô∏è‚É£ BASELINES
# ==========================================================
always_treat <- mean(rowMeans(Y1_cop[test_idx, ]))
never_treat  <- mean(rowMeans(Y0_cop[test_idx, ]))

# Correct random policy
rand_choice <- rbinom(length(test_idx), 1, 0.5)
Y_rand_mat <- Y0_cop[test_idx, ]
Y_rand_mat[rand_choice == 1, ] <- Y1_cop[test_idx, ][rand_choice == 1, ]
random_pol <- mean(rowMeans(Y_rand_mat))

cat("Always Treat:", always_treat, "\n")
cat("Never Treat:", never_treat, "\n")
cat("Random Policy:", random_pol, "\n")

# ----------------------------------------------------------
# IPS ESTIMATOR
# ----------------------------------------------------------
IPS <- mean(
  ifelse(pi_hat==W[test_idx],
         reward_scalar[test_idx]/ifelse(W[test_idx]==1,propensity[test_idx],1-propensity[test_idx]),
         0)
)

# ----------------------------------------------------------
# DOUBLY ROBUST ESTIMATOR
# ----------------------------------------------------------
DR_vec <- numeric(K_out)

for(k in 1:K_out){
  df_out <- data.frame(
    Y = Y_obs_mat[,k],
    feat1_avg = feat1_avg,
    feat2_avg = feat2_avg,
    feat3_avg = feat3_avg,
    W = W
  )
  
  outcome_model <- lm(Y ~ feat1_avg + feat2_avg + feat3_avg + W, data=df_out)
  
  mu1 <- predict(outcome_model, newdata=data.frame(
    feat1_avg = feat1_avg[test_idx],
    feat2_avg = feat2_avg[test_idx],
    feat3_avg = feat3_avg[test_idx],
    W = 1
  ))
  
  mu0 <- predict(outcome_model, newdata=data.frame(
    feat1_avg = feat1_avg[test_idx],
    feat2_avg = feat2_avg[test_idx],
    feat3_avg = feat3_avg[test_idx],
    W = 0
  ))
  
  DR_vec[k] <- mean(
    ifelse(pi_hat==1, mu1, mu0) +
      ifelse(pi_hat==W[test_idx],
             (Y_obs_mat[test_idx,k] - ifelse(W[test_idx]==1, mu1, mu0)) /
               ifelse(W[test_idx]==1, propensity[test_idx], 1-propensity[test_idx]),
             0)
  )
}

DR_estimate <- mean(DR_vec)
cat("DR Estimate (multivariate):", DR_estimate, "\n")

# ----------------------------------------------------------
# PRINT RESULTS
# ----------------------------------------------------------
cat("Mean Policy Reward:", mean(R_policy), "\n")
cat("Mean Regret:", mean(R_opt - R_policy), "\n")
cat("IPS Estimate:", IPS, "\n")
cat("DR Estimate (multivariate):", DR_estimate, "\n")
cat("Always Treat:", always_treat, "\n")
cat("Never Treat:", never_treat, "\n")
cat("Random Policy:", random_pol, "\n")

# ----------------------------------------------------------
# VISUALIZATIONS
# ----------------------------------------------------------
policy_rewards <- data.frame(
  Policy = c("Always Treat","Never Treat","Random","Learned Policy"),
  Reward = c(always_treat, never_treat, random_pol, mean(R_policy))
)

ggplot(policy_rewards, aes(x=Policy, y=Reward, fill=Policy)) +
  geom_bar(stat="identity") +
  theme_minimal() +
  ggtitle("Mean Policy Rewards (Wine Quality Dataset)") +
  ylab("Mean Reward") +
  theme(legend.position="none")

estimates <- data.frame(
  Estimator = c("IPS","Doubly Robust"),
  Value = c(IPS, DR_estimate)
)

ggplot(estimates, aes(x=Estimator, y=Value, fill=Estimator)) +
  geom_bar(stat="identity") +
  theme_minimal() +
  ggtitle("Policy Value Estimates (Wine Quality Dataset)") +
  ylab("Estimated Reward") +
  theme(legend.position="none")

regret_vals <- R_opt - R_policy
ggplot(data.frame(Regret=regret_vals), aes(x=Regret)) +
  geom_histogram(bins=20, fill="steelblue", color="black", alpha=0.7) +
  theme_minimal() +
  ggtitle("Histogram of Regret (Wine Quality Dataset)") +
  xlab("Regret") +
  ylab("Frequency")

# ==========================================================
# 2Ô∏è‚É£ Track episode-wise rewards (optional)
# ==========================================================
# For demonstration, we can track mean reward every 20 episodes
episode_rewards <- numeric(episodes)
epsilon_tmp <- epsilon
for(ep in 1:episodes){
  i <- sample(train_idx,1)
  idx_i <- which(train_idx==i)
  
  state <- array_reshape(X_train[idx_i,,], c(1,T_steps,p))
  if(runif(1)<epsilon_tmp){
    action <- sample(0:1,1)
  } else{
    qvals <- model %>% predict(state, verbose=0)
    action <- which.max(qvals)-1
  }
  
  reward <- if(action==1) mean(Y1_cop[i,]) else mean(Y0_cop[i,])
  episode_rewards[ep] <- reward
  
  epsilon_tmp <- max(epsilon_tmp*epsilon_decay, epsilon_min)
}


episode_df <- data.frame(
  Episode = 1:episodes,
  Reward = episode_rewards
)

ggplot(episode_df, aes(x=Episode, y=Reward)) +
  geom_line(color="blue") +
  geom_smooth(method="loess", color="red", se=FALSE) +
  ggtitle("Episode-wise Reward Learning (Wine Quality Dataset)") +
  theme_minimal()

############################################################
## Multivariate Contextual Bandit
## DR + IPS + Empirical Copula (Adult Dataset)
############################################################

library(data.table)
library(dplyr)
library(keras)
library(tensorflow)
library(tidyverse)

set.seed(123)

# ==========================================================
# 1Ô∏è‚É£ PARAMETERS
# ==========================================================
n <- 500
T_steps <- 10
episodes <- 400
epsilon <- 1
epsilon_decay <- 0.995
epsilon_min <- 0.05
alpha <- 0.001
rho <- 0.7          # AR(1) temporal correlation
K_out <- 4          # number of outcomes

# ==========================================================
# 2Ô∏è‚É£ Load Adult Dataset
# ==========================================================
adult <- fread(
  "adult.data.txt",
  header = FALSE,
  sep = ",",
  na.strings = "?",
  strip.white = TRUE
)

colnames(adult) <- c(
  "age","workclass","fnlwgt","education","education_num",
  "marital_status","occupation","relationship","race","sex",
  "capital_gain","capital_loss","hours_per_week","native_country","income"
)

adult <- na.omit(adult)
adult[, income_binary := ifelse(trimws(income) == ">50K", 1, 0)]

if(nrow(adult) > 30000)
  adult <- adult[sample(.N, 30000)]

# ==========================================================
# 3Ô∏è‚É£ Feature Engineering
# ==========================================================
numeric_covs <- c("age","fnlwgt","education_num",
                  "capital_gain","capital_loss","hours_per_week")

cat_covs <- setdiff(names(adult),
                    c(numeric_covs,"income","income_binary"))

for(col in cat_covs)
  adult[, (col) := as.numeric(factor(get(col)))]

feature_cols <- c(numeric_covs, cat_covs)
X_num <- scale(as.matrix(adult[, ..feature_cols]))
p <- ncol(X_num)

# ==========================================================
# 4Ô∏è‚É£ AR(1) TEMPORAL PROCESS
# ==========================================================
X_long <- array(0, c(n, T_steps, p))
X_long[,1,] <- X_num[sample(1:nrow(X_num), n), ]

for(t in 2:T_steps){
  noise <- matrix(rnorm(n*p, 0, 0.1), n)
  X_long[,t,] <- rho*X_long[,t-1,] + noise
}

X_scaled <- array_reshape(scale(array_reshape(X_long, c(n*T_steps, p))),
                          c(n,T_steps,p))

# ==========================================================
# 5Ô∏è‚É£ MULTI-CONFOUNDER PROPENSITY MODEL
# ==========================================================
rm_avg    <- apply(X_scaled[,,which(colnames(X_num)=="age")],1,mean)
lstat_avg <- apply(X_scaled[,,which(colnames(X_num)=="education_num")],1,mean)
crim_avg  <- apply(X_scaled[,,which(colnames(X_num)=="fnlwgt")],1,mean)

lin_pred <- 1.2*rm_avg - 1.0*lstat_avg + 0.7*crim_avg
propensity <- plogis(lin_pred)

W <- rbinom(n,1,propensity)

# ==========================================================
# 6Ô∏è‚É£ NONLINEAR MULTIVARIATE POTENTIAL OUTCOMES
# ==========================================================
Y0_mat <- matrix(0, n, K_out)
Y1_mat <- matrix(0, n, K_out)

for(k in 1:K_out){
  base_k <- sin(rm_avg + k*0.2) +
    log(abs(lstat_avg)+1) +
    rnorm(n,0,0.2)
  
  tau_k <- 1.5 + 0.5*k +
    0.8*(rm_avg^2) -
    0.3*lstat_avg
  
  Y0_mat[,k] <- base_k
  Y1_mat[,k] <- base_k + tau_k
}

# ==========================================================
# 7Ô∏è‚É£ EMPIRICAL COPULA TRANSFORMATION
# ==========================================================
empirical_copula <- function(Y){
  apply(Y, 2, function(col){
    rank(col, ties.method="average")/length(col)
  })
}

Y0_cop <- empirical_copula(Y0_mat)
Y1_cop <- empirical_copula(Y1_mat)

# Observed outcomes (preserve matrix structure)
Y_obs_mat <- Y0_cop
Y_obs_mat[W==1, ] <- Y1_cop[W==1, ]

reward_scalar <- rowMeans(Y_obs_mat)

# ==========================================================
# 8Ô∏è‚É£ TRAIN / TEST SPLIT
# ==========================================================
train_idx <- sample(1:n, 0.7*n)
test_idx  <- setdiff(1:n, train_idx)

X_train <- X_scaled[train_idx,,]
X_test  <- X_scaled[test_idx,,]

# ==========================================================
# 9Ô∏è‚É£ CONTEXTUAL BANDIT NETWORK
# ==========================================================
create_model <- function(input_shape){
  keras_model_sequential() %>%
    layer_conv_1d(32,3,activation="relu", input_shape=input_shape) %>%
    layer_lstm(32) %>%
    layer_dense(32,activation="relu") %>%
    layer_dense(2,activation="linear") %>%
    compile(loss="mse", optimizer=optimizer_adam(alpha))
}

model <- create_model(c(T_steps,p))

# ==========================================================
# üîü TRAINING LOOP
# ==========================================================
for(ep in 1:episodes){
  i <- sample(train_idx,1)
  idx_i <- which(train_idx==i)
  
  state <- array_reshape(X_train[idx_i,,], c(1,T_steps,p))
  
  if(runif(1)<epsilon){
    action <- sample(0:1,1)
  } else{
    qvals <- model %>% predict(state,verbose=0)
    action <- which.max(qvals)-1
  }
  
  reward <- if(action==1)
    mean(Y1_cop[i,]) else mean(Y0_cop[i,])
  
  qvals <- model %>% predict(state,verbose=0)
  qvals[1,action+1] <- reward
  
  model %>% fit(state, qvals, epochs=1, verbose=0)
  
  epsilon <- max(epsilon*epsilon_decay, epsilon_min)
}

# ==========================================================
# 1Ô∏è‚É£1Ô∏è‚É£ POLICY EVALUATION
# ==========================================================
q_test <- model %>% predict(X_test, verbose=0)
pi_hat <- apply(q_test,1,which.max)-1

Y_policy_mat <- Y0_cop[test_idx, ]
Y_policy_mat[pi_hat==1, ] <- Y1_cop[test_idx, ][pi_hat==1, ]

R_policy <- rowMeans(Y_policy_mat)
R_opt <- rowMeans(pmax(Y0_cop[test_idx, ], Y1_cop[test_idx, ]))

cat("Mean Policy Reward:", mean(R_policy), "\n")
cat("Mean Regret:", mean(R_opt - R_policy), "\n")

# ==========================================================
# 1Ô∏è‚É£2Ô∏è‚É£ IPS ESTIMATOR
# ==========================================================
Y_obs_scalar <- rowMeans(Y_obs_mat)

IPS <- mean(
  ifelse(pi_hat==W[test_idx],
         Y_obs_scalar[test_idx] /
           ifelse(W[test_idx]==1, propensity[test_idx], 1-propensity[test_idx]),
         0)
)

cat("IPS Estimate:", IPS, "\n")

# ==========================================================
# 1Ô∏è‚É£3Ô∏è‚É£ DOUBLY ROBUST ESTIMATOR
# ==========================================================
DR_vec <- numeric(K_out)

for(k in 1:K_out){
  df_out <- data.frame(
    Y = Y_obs_mat[,k],
    rm_avg = rm_avg,
    lstat_avg = lstat_avg,
    crim_avg = crim_avg,
    W = W
  )
  
  outcome_model <- lm(Y ~ rm_avg + lstat_avg + crim_avg + W, data=df_out)
  
  mu1 <- predict(outcome_model,
                 newdata=data.frame(
                   rm_avg = rm_avg[test_idx],
                   lstat_avg = lstat_avg[test_idx],
                   crim_avg = crim_avg[test_idx],
                   W = 1))
  
  mu0 <- predict(outcome_model,
                 newdata=data.frame(
                   rm_avg = rm_avg[test_idx],
                   lstat_avg = lstat_avg[test_idx],
                   crim_avg = crim_avg[test_idx],
                   W = 0))
  
  DR_vec[k] <- mean(
    ifelse(pi_hat==1, mu1, mu0) +
      ifelse(pi_hat==W[test_idx],
             (Y_obs_mat[test_idx,k] - ifelse(W[test_idx]==1, mu1, mu0)) /
               ifelse(W[test_idx]==1, propensity[test_idx], 1-propensity[test_idx]),
             0)
  )
}

DR_estimate <- mean(DR_vec)
cat("DR Estimate (multivariate):", DR_estimate, "\n")

# ==========================================================
# 1Ô∏è‚É£4Ô∏è‚É£ BASELINES
# ==========================================================
always_treat <- mean(rowMeans(Y1_cop[test_idx, ]))
never_treat  <- mean(rowMeans(Y0_cop[test_idx, ]))

# Correct random policy
rand_choice <- rbinom(length(test_idx), 1, 0.5)
Y_rand_mat <- Y0_cop[test_idx, ]
Y_rand_mat[rand_choice == 1, ] <- Y1_cop[test_idx, ][rand_choice == 1, ]
random_pol <- mean(rowMeans(Y_rand_mat))

cat("Always Treat:", always_treat, "\n")
cat("Never Treat:", never_treat, "\n")
cat("Random Policy:", random_pol, "\n")


# ----------------------------------------------------------
# PRINT RESULTS
# ----------------------------------------------------------
cat("Mean Policy Reward:", mean(R_policy), "\n")
cat("Mean Regret:", mean(R_opt - R_policy), "\n")
cat("IPS Estimate:", IPS, "\n")
cat("DR Estimate (multivariate):", DR_estimate, "\n")
cat("Always Treat:", always_treat, "\n")
cat("Never Treat:", never_treat, "\n")
cat("Random Policy:", random_pol, "\n")

# ----------------------------------------------------------
# VISUALIZATIONS
# ----------------------------------------------------------
policy_rewards <- data.frame(
  Policy = c("Always Treat","Never Treat","Random","Learned Policy"),
  Reward = c(always_treat, never_treat, random_pol, mean(R_policy))
)

ggplot(policy_rewards, aes(x=Policy, y=Reward, fill=Policy)) +
  geom_bar(stat="identity") +
  theme_minimal() +
  ggtitle("Mean Policy Rewards (Adult Dataset)") +
  ylab("Mean Reward") +
  theme(legend.position="none")

estimates <- data.frame(
  Estimator = c("IPS","Doubly Robust"),
  Value = c(IPS, DR_estimate)
)

ggplot(estimates, aes(x=Estimator, y=Value, fill=Estimator)) +
  geom_bar(stat="identity") +
  theme_minimal() +
  ggtitle("Policy Value Estimates (Adult Dataset)") +
  ylab("Estimated Reward") +
  theme(legend.position="none")

regret_vals <- R_opt - R_policy
ggplot(data.frame(Regret=regret_vals), aes(x=Regret)) +
  geom_histogram(bins=20, fill="steelblue", color="black", alpha=0.7) +
  theme_minimal() +
  ggtitle("Histogram of Regret (Adult Dataset)") +
  xlab("Regret") +
  ylab("Frequency")

# ==========================================================
# 2Ô∏è‚É£ Track episode-wise rewards (optional)
# ==========================================================
# For demonstration, we can track mean reward every 20 episodes
episode_rewards <- numeric(episodes)
epsilon_tmp <- epsilon
for(ep in 1:episodes){
  i <- sample(train_idx,1)
  idx_i <- which(train_idx==i)
  
  state <- array_reshape(X_train[idx_i,,], c(1,T_steps,p))
  if(runif(1)<epsilon_tmp){
    action <- sample(0:1,1)
  } else{
    qvals <- model %>% predict(state, verbose=0)
    action <- which.max(qvals)-1
  }
  
  reward <- if(action==1) mean(Y1_cop[i,]) else mean(Y0_cop[i,])
  episode_rewards[ep] <- reward
  
  epsilon_tmp <- max(epsilon_tmp*epsilon_decay, epsilon_min)
}


episode_df <- data.frame(
  Episode = 1:episodes,
  Reward = episode_rewards
)

ggplot(episode_df, aes(x=Episode, y=Reward)) +
  geom_line(color="blue") +
  geom_smooth(method="loess", color="red", se=FALSE) +
  ggtitle("Episode-wise Reward Learning (Adult Dataset)") +
  theme_minimal()
