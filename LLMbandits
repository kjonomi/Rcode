# mathematics-3772632 (August 04, 2025) Accepted by Mathematics Journal.	
# https://www.mdpi.com/2227-7390/13/15/2523
# Title: LLM-Guided Ensemble Learning for Contextual Bandits with Copula and Gaussian Process Models
# --- Required Libraries ---
library(rugarch)      # for fGARCH simulation
library(copula)       # copula modeling
library(VineCopula)   # vine copula fitting
library(laGP)         # Gaussian Process reward models
library(dplyr)
library(ggplot2)
library(httr)
library(jsonlite)
library(knitr)
library(DT)

# --- Cache Environments ---
.llm_reward_cache <- new.env(parent = emptyenv())
.llm_policy_cache <- new.env(parent = emptyenv())

# --- GP Model Fitting ---
fit_gp <- function(X, y) {
  tryCatch(laGP::newGPsep(X, y, d = 0.5, g = 1e-6, dK = TRUE), error = function(e) NULL)
}

# --- Bandit Policies ---
thompson_sampling <- function(x_row, gp_models) {
  preds <- sapply(gp_models, function(gp) {
    p <- tryCatch(predGPsep(gp, matrix(x_row, 1)), error = function(e) NULL)
    if (is.null(p) || is.null(p$mean) || is.null(p$s2) || p$s2 <= 0) return(-Inf)
    val <- rnorm(1, p$mean, sqrt(p$s2))
    min(max(val, 0), 1)
  })
  which.max(preds)
}

epsilon_greedy_policy <- function(x_row, t, gp_models, epsilon, n_arms) {
  if (runif(1) < epsilon || t <= n_arms) return(sample(1:n_arms, 1))
  preds <- sapply(gp_models, function(gp) {
    p <- tryCatch(predGPsep(gp, matrix(x_row, 1)), error = function(e) NULL)
    if (is.null(p) || is.null(p$mean)) return(-Inf)
    p$mean
  })
  which.max(preds)
}

ucb_policy <- function(x_row, t, gp_models) {
  preds <- sapply(gp_models, function(gp) {
    p <- tryCatch(predGPsep(gp, matrix(x_row, 1)), error = function(e) NULL)
    if (is.null(p) || is.null(p$mean) || is.null(p$s2) || p$s2 <= 0) return(-Inf)
    p$mean + sqrt(2 * log(t + 1) * p$s2)
  })
  which.max(preds)
}

# --- LLM Reward and Policy ---
query_llm_reward <- function(x_row, arm_idx, history_text = "", api_key = Sys.getenv("OPENAI_API_KEY"), mock = TRUE) {
  key <- paste0(paste(round(x_row, 3), collapse = ","), "_arm", arm_idx)
  if (exists(key, envir = .llm_reward_cache)) return(get(key, envir = .llm_reward_cache))
  pred <- if (mock) runif(1, 0, 1) else NA  # Placeholder for real API call
  assign(key, pred, envir = .llm_reward_cache)
  return(pred)
}

query_llm_policy <- function(summary_text, api_key = Sys.getenv("OPENAI_API_KEY"), mock = TRUE) {
  key <- paste0("policy_", substr(summary_text, 1, 100))
  if (exists(key, envir = .llm_policy_cache)) return(get(key, envir = .llm_policy_cache))
  choice <- if (mock) sample(c("thompson", "ucb", "epsilon-greedy", "llm-reward"), 1) else "thompson"
  assign(key, choice, envir = .llm_policy_cache)
  return(choice)
}

# --- Simulated Data Generation ---
generate_simulated_data <- function(T = 300, n_ctx = 10, n_arms = 3) {
  spec <- ugarchspec(
    variance.model = list(model = "sGARCH", garchOrder = c(1,1)),
    mean.model = list(armaOrder = c(0,0), include.mean = FALSE),
    distribution.model = "norm",
    fixed.pars = list(omega = 0.1, alpha1 = 0.15, beta1 = 0.7)
  )
  vol_matrix <- matrix(NA, nrow = T, ncol = n_ctx)
  innovations <- matrix(rnorm(T * n_ctx), nrow = T)
  for (j in 1:n_ctx) vol_matrix[, j] <- ugarchpath(spec, n.sim = T)@path$sigmaSim
  hetero_contexts <- innovations * vol_matrix
  U_ctx <- apply(hetero_contexts, 2, function(x) rank(x) / (length(x) + 1))
  vine_fit <- RVineStructureSelect(U_ctx, familyset = c(1, 3, 4, 5), cores = 2)
  U_trans <- RVinePIT(U_ctx, vine_fit)
#  reward_mat <- qbeta(U_trans[, 1:n_arms], shape1 = 2, shape2 = 5)
  reward_mat <- qbeta(U_trans[, 1:n_arms], shape1 = 5, shape2 = 2)
  list(X = U_trans, Y = reward_mat)
}

# --- Simulation Runner ---
run_bandit_simulation <- function(X, Y, epsilon = 0.1, mock = TRUE) {
  T_test <- nrow(X)
  n_arms <- ncol(Y)
  train_idx <- 1:floor(0.7 * T_test)
  test_idx <- (max(train_idx) + 1):T_test
  
  gp_models <- lapply(1:n_arms, function(j) fit_gp(X[train_idx, ], Y[train_idx, j]))
  X_test <- X[test_idx, ]
  Y_test <- Y[test_idx, ]
  T_test <- nrow(X_test)
  
  rewards <- regrets <- numeric(T_test)
  policies <- character(T_test)
  arms <- integer(T_test)
  history_regrets <- c()
  
  for (t in 1:T_test) {
    x <- X_test[t, , drop = FALSE]
    best_reward <- max(Y_test[t, ])
    summary_text <- paste("t =", t, "| rewards:", paste(round(tail(rewards[1:(t - 1)], 5), 2), collapse = ", "))
    policy <- if (t > 10) query_llm_policy(summary_text, mock = mock) else "thompson"
    arm <- switch(policy,
                  "thompson" = thompson_sampling(x, gp_models),
                  "ucb" = ucb_policy(x, t, gp_models),
                  "epsilon-greedy" = epsilon_greedy_policy(x, t, gp_models, epsilon, n_arms),
                  "llm-reward" = which.max(sapply(1:n_arms, function(a) query_llm_reward(x, a, mock = mock))),
                  sample(1:n_arms, 1))
    reward <- Y_test[t, arm]
    rewards[t] <- reward
    regrets[t] <- ifelse(t == 1, best_reward - reward, regrets[t - 1] + (best_reward - reward))
    policies[t] <- policy
    arms[t] <- arm
    history_regrets <- c(history_regrets, regrets[t])
  }
  
  data.frame(time = 1:T_test, reward = rewards, regret = regrets, policy = policies, arm = arms)
}

# --- Wrapper for Replication ---
run_multiple_reps <- function(n_reps = 10000, T = 100, n_ctx = 10, n_arms = 3, ...) {
  sim_data <- generate_simulated_data(T = T, n_ctx = n_ctx, n_arms = n_arms)
  results <- lapply(seq_len(n_reps), function(i) {
    message("▶️ Replication ", i)
    run_bandit_simulation(sim_data$X, sim_data$Y, ...)
  })
  bind_rows(results, .id = "rep")
}

# --- Run simulation ---
set.seed(123)
all_results <- run_multiple_reps(
  n_reps = 10000, T = 100, n_ctx = 10, n_arms = 3,
  epsilon = 0.1, mock = TRUE
)

n_reps <- length(unique(all_results$rep))
final_time <- max(all_results$time)

# --- Summarize and CI function ---
summarize_with_ci <- function(df, value_col) {
  df %>%
    group_by(time, policy) %>%
    summarise(
      mean = mean(.data[[value_col]]),
      sd = sd(.data[[value_col]]),
      .groups = "drop"
    ) %>%
    mutate(
      ci_lower = mean - 1.96 * sd / sqrt(n_reps),
      ci_upper = mean + 1.96 * sd / sqrt(n_reps)
    )
}

# --- Regret and Reward summaries ---
summary_regret <- summarize_with_ci(all_results, "regret")
summary_reward <- summarize_with_ci(all_results, "reward")

# --- Plotting helper ---
plot_ci <- function(summary_df, y_var, y_label, title) {
  ggplot(summary_df, aes(x = time, y = .data[[y_var]], color = policy, fill = policy)) +
    geom_line(linewidth = 1) +
    geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), alpha = 0.2, color = NA) +
    labs(title = title, x = "Time", y = y_label) +
    theme_minimal(base_size = 14)
}

# --- CI Plots ---
print(plot_ci(summary_regret, "mean", "Cumulative Regret", "Cumulative Regret Over Time (Mean ± 95% CI)"))
print(plot_ci(summary_reward, "mean", "Reward", "Average Reward Over Time (Mean ± 95% CI)"))

# --- SD Plots ---
plot_sd <- function(summary_df, sd_col, y_label, title) {
  ggplot(summary_df, aes(x = time, y = .data[[sd_col]], color = policy)) +
    geom_line(linewidth = 1) +
    labs(title = title, x = "Time", y = y_label) +
    theme_minimal(base_size = 14)
}

print(plot_sd(summary_regret, "sd", "Regret SD", "Regret Standard Deviation Over Time by Policy"))
print(plot_sd(summary_reward, "sd", "Reward SD", "Reward Standard Deviation Over Time by Policy"))

# --- Final-time summary table ---
summary_final_stats <- all_results %>%
  filter(time == final_time) %>%
  group_by(policy) %>%
  summarise(
    mean_reward = mean(reward),
    sd_reward = sd(reward),
    mean_regret = mean(regret),
    sd_regret = sd(regret),
    .groups = "drop"
  ) %>%
  arrange(mean_regret)

print(summary_final_stats)
