# Paper titled "Quantum-Inspired Deep Q-Learning for Multivalued Treatment Policy Optimization" under review.
# Weighted model corresponds to a "quantum-inspired" weighted sampling approach to improve exploration/exploitation, 
# Uniform model is standard Q-learning with uniform random sampling during training.

# Simulation setup

# --- Libraries ---
library(data.table)
library(keras)
library(tensorflow)
library(tidyverse)
library(boot)

# --- Parameters ---
T_steps <- 10
n <- 200
episodes <- 1000
alpha <- 0.001
epsilon_start <- 1.0
epsilon_decay <- 0.99
epsilon_min <- 0.1

# --- Pure Simulation Setup ---
set.seed(123)
p <- 6  # number of covariates

# Simulate covariates with time-evolving noise
X_long <- array(0, dim = c(n, T_steps, p))
X_base <- matrix(rnorm(n * p), nrow = n, ncol = p)
for (t in 1:T_steps) {
  noise <- matrix(rnorm(n * p, 0, 0.1), nrow = n)
  X_long[, t, ] <- X_base + noise
}

# Normalize each feature across samples and time
X_scaled <- X_long
for (j in 1:p) {
  for (t in 1:T_steps) {
    mean_val <- mean(X_scaled[, t, j])
    sd_val <- sd(X_scaled[, t, j])
    X_scaled[, t, j] <- (X_scaled[, t, j] - mean_val) / sd_val
  }
}

# --- Treatment Assignment and Outcome Simulation ---
x1_avg <- apply(X_scaled[, , 1], 1, mean)
x2_avg <- apply(X_scaled[, , 2], 1, mean)

# Sharper logits for treatment assignment
logits <- cbind(3.5 * x1_avg, 3.5 * x2_avg, -3.5 * x1_avg)
exp_logits <- exp(logits)
prob_mat <- exp_logits / rowSums(exp_logits)
W <- apply(prob_mat, 1, function(p) sample(0:2, 1, prob = p))

# Simulate nonlinear heterogeneous treatment effects
base_outcome <- sin(apply(X_scaled[, , 1:3], 1, mean)) * 3 + rnorm(n, 0, 0.5)
tau0 <- 0
tau1 <- 1.5 + 2 * x2_avg - 1 * x1_avg
tau2 <- -2 + 3.5 * x1_avg - 0.5 * x2_avg

Y0 <- base_outcome + tau0
Y1 <- base_outcome + tau1
Y2 <- base_outcome + tau2

Y_obs <- ifelse(W == 0, Y0, ifelse(W == 1, Y1, Y2))

# --- Train/Test Split ---
train_idx <- sample(1:n, floor(0.7 * n))
test_idx <- setdiff(1:n, train_idx)

X_train <- X_scaled[train_idx, , ]
X_test <- X_scaled[test_idx, , ]
Y0_test <- Y0[test_idx]
Y1_test <- Y1[test_idx]
Y2_test <- Y2[test_idx]

# --- Quantum-inspired Sampling Weights ---
get_sampling_weights <- function(model, X_input) {
  q_vals <- predict(model, X_input)
  prob_qvals <- t(apply(q_vals, 1, softmax))
  entropy <- -rowSums(prob_qvals * log(prob_qvals + 1e-8))
  qvar <- apply(q_vals, 1, var)
  combined_uncertainty <- entropy + qvar
  weights <- combined_uncertainty / max(combined_uncertainty)
  return(weights)
}

# --- Softmax Utility ---
softmax <- function(x) {
  exp_x <- exp(x - max(x))
  exp_x / sum(exp_x)
}

# --- DQN Model Definition ---
build_model <- function(input_shape, output_dim = 3) {
  model <- keras_model_sequential() %>%
    layer_conv_1d(filters = 32, kernel_size = 3, activation = "relu", input_shape = input_shape) %>%
    layer_lstm(units = 32, return_sequences = FALSE) %>%
    layer_dense(units = 32, activation = "relu") %>%
    layer_dense(units = output_dim)
  model %>% compile(optimizer = optimizer_adam(learning_rate = alpha), loss = "mse")
  return(model)
}

# --- DQN Training Function ---
train_dqn <- function(X, Y0, Y1, Y2, idx, episodes, alpha,
                      gamma = 0.95,
                      epsilon_start = 1.0,
                      epsilon_decay = 0.99,
                      epsilon_min = 0.1,
                      weighted_sampling = FALSE) {
  n <- length(idx)
  epsilon <- epsilon_start
  model <- build_model(input_shape = dim(X)[2:3])
  reward_log <- numeric(episodes)
  
  for (ep in 1:episodes) {
    X_ep <- X[idx, , ]
    
    # Apply quantum-inspired sampling weights if requested
    if (weighted_sampling && ep > 1) {
      weights <- get_sampling_weights(model, X_ep)
      sample_idx <- sample(1:n, n, replace = TRUE, prob = weights)
    } else {
      sample_idx <- sample(1:n, n, replace = TRUE)
    }
    
    X_batch <- X_ep[sample_idx, , ]
    actions <- sample(0:2, n, replace = TRUE)
    
    # Generate true rewards
    Y <- ifelse(actions == 0, Y0, ifelse(actions == 1, Y1, Y2))[sample_idx]
    
    # Predict current Q-values
    Q_pred <- predict(model, X_batch)
    Q_target <- Q_pred
    
    for (i in 1:n) {
      a <- actions[i] + 1  # R is 1-indexed
      Q_target[i, a] <- Y[i]
    }
    
    history <- model %>% fit(X_batch, Q_target, epochs = 1, verbose = 0)
    reward_log[ep] <- mean(Y)
    
    epsilon <- max(epsilon * epsilon_decay, epsilon_min)
  }
  
  return(list(model = model, rewards = reward_log))
}

# --- Train both models ---
set.seed(123)

res_uniform <- train_dqn(X_train, Y0[train_idx], Y1[train_idx], Y2[train_idx],
                         idx = 1:nrow(X_train),
                         episodes = episodes,
                         alpha = alpha,
                         weighted_sampling = FALSE)
set.seed(123)

res_weighted <- train_dqn(X_train, Y0[train_idx], Y1[train_idx], Y2[train_idx],
                          idx = 1:nrow(X_train),
                          episodes = episodes,
                          alpha = alpha,
                          weighted_sampling = TRUE)


# --- Evaluation function ---
evaluate_model <- function(model, X_test, Y0_test, Y1_test, Y2_test) {
  q_preds <- model %>% predict(X_test, verbose = 0)
  policy_actions <- apply(q_preds, 1, which.max) - 1
  rewards_pred <- sapply(seq_along(policy_actions), function(i) {
    a <- policy_actions[i]
    if (a == 0) return(Y0_test[i])
    if (a == 1) return(Y1_test[i])
    if (a == 2) return(Y2_test[i])
  })
  
  list(
    total_reward = sum(rewards_pred),
    average_reward = mean(rewards_pred),
    action_distribution = table(policy_actions),
    rewards_pred = rewards_pred,
    policy_actions = policy_actions
  )
}

eval_uniform <- evaluate_model(res_uniform$model, X_test, Y0_test, Y1_test, Y2_test)
eval_weighted <- evaluate_model(res_weighted$model, X_test, Y0_test, Y1_test, Y2_test)

cat("Uniform Sampling Model Performance:\n")
print(eval_uniform[1:3])

cat("\nWeighted Sampling Model Performance:\n")
print(eval_weighted[1:3])

# --- Statistical Tests on training rewards ---
cat("\n--- Statistical Tests on Training Rewards ---\n")
t_test_res <- t.test(res_uniform$rewards, res_weighted$rewards, paired = TRUE)
cat("Paired t-test:\n")
print(t_test_res)

# --- Diagnostic Plots ---
df_rewards <- data.frame(
  Episode = rep(1:episodes, 2),
  Reward = c(res_uniform$rewards, res_weighted$rewards),
  Model = factor(rep(c("Uniform", "Weighted"), each = episodes))
)

ggplot(df_rewards, aes(x = Model, y = Reward, fill = Model)) +
  geom_boxplot(alpha = 0.7) +
  labs(title = "Boxplot of Rewards Per Episode by Model", y = "Reward") +
  theme_minimal()

df_test_rewards <- data.frame(
  Reward = c(eval_uniform$rewards_pred, eval_weighted$rewards_pred),
  Model = factor(rep(c("Uniform", "Weighted"), each = length(eval_uniform$rewards_pred)))
)

ggplot(df_test_rewards, aes(x = Reward, fill = Model)) +
  geom_density(alpha = 0.5) +
  labs(title = "Density Plot of Predicted Rewards on Test Set", x = "Reward", y = "Density") +
  theme_minimal()

bootstrap_cum_rewards <- function(rewards, R = 10000) {
  boot_obj <- boot(rewards, statistic = function(data, i) cumsum(data[i]), R = R)
  boot_ci_lower <- apply(boot_obj$t, 2, quantile, probs = 0.025)
  boot_ci_upper <- apply(boot_obj$t, 2, quantile, probs = 0.975)
  list(lower = boot_ci_lower, upper = boot_ci_upper)
}

ci_uniform <- bootstrap_cum_rewards(res_uniform$rewards)
ci_weighted <- bootstrap_cum_rewards(res_weighted$rewards)

df_compare <- data.frame(
  Episode = 1:episodes,
  Uniform = cumsum(res_uniform$rewards),
  Weighted = cumsum(res_weighted$rewards),
  Uniform_lower = ci_uniform$lower,
  Uniform_upper = ci_uniform$upper,
  Weighted_lower = ci_weighted$lower,
  Weighted_upper = ci_weighted$upper
)

df_long <- pivot_longer(df_compare, cols = c("Uniform", "Weighted"),
                        names_to = "Model", values_to = "CumulativeReward")

ggplot(df_long, aes(x = Episode, y = CumulativeReward, color = Model)) +
  geom_line(size = 1) +
  geom_ribbon(data = df_compare, aes(x = Episode, ymin = Uniform_lower, ymax = Uniform_upper),
              fill = "blue", alpha = 0.15, inherit.aes = FALSE) +
  geom_ribbon(data = df_compare, aes(x = Episode, ymin = Weighted_lower, ymax = Weighted_upper),
              fill = "red", alpha = 0.15, inherit.aes = FALSE) +
  labs(title = "Cumulative Reward with 95% CI by Model", y = "Cumulative Reward") +
  theme_minimal()

# --- Comparison Summary Table ---
comparison_table <- data.frame(
  Metric = c(
    "Total Reward (Test Set)",
    "Average Reward (Test Set)",
    "Training Reward Mean",
    "Training Reward SD",
    "Action 0 Freq (Test Set)",
    "Action 1 Freq (Test Set)",
    "Action 2 Freq (Test Set)"
  ),
  Uniform_Model = c(
    eval_uniform$total_reward,
    eval_uniform$average_reward,
    mean(res_uniform$rewards),
    sd(res_uniform$rewards),
    eval_uniform$action_distribution["0"],
    eval_uniform$action_distribution["1"],
    eval_uniform$action_distribution["2"]
  ),
  Weighted_Model = c(
    eval_weighted$total_reward,
    eval_weighted$average_reward,
    mean(res_weighted$rewards),
    sd(res_weighted$rewards),
    eval_weighted$action_distribution["0"],
    eval_weighted$action_distribution["1"],
    eval_weighted$action_distribution["2"]
  )
)

comparison_table[is.na(comparison_table)] <- 0
cat("\nTwo-Model Comparison Summary:\n")
print(comparison_table)


# Wine Quality

# --- Libraries ---
library(data.table)
library(keras)
library(tensorflow)
library(tidyverse)
library(boot)

# --- Parameters ---
T_steps <- 10
n <- 1000
episodes <- 30
alpha <- 0.001
epsilon_start <- 1.0
epsilon_decay <- 0.99
epsilon_min <- 0.1

# --- Load Wine Quality Dataset ---
wine_url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
wine <- fread(wine_url, sep = ";")

# --- Prepare Data ---
p <- ncol(wine)
set.seed(123)
df_sampled <- wine[sample(1:nrow(wine), n, replace = TRUE), ]

# Simulate time series structure by adding small noise per time step
X_long <- array(0, dim = c(n, T_steps, p))
for (t in 1:T_steps) {
  noise <- matrix(rnorm(n * p, 0, 0.01), nrow = n)
  X_long[, t, ] <- as.matrix(df_sampled) + noise
}

# Normalize each feature across samples and time
X_scaled <- X_long
for (j in 1:p) {
  for (t in 1:T_steps) {
    mean_val <- mean(X_scaled[, t, j])
    sd_val <- sd(X_scaled[, t, j])
    X_scaled[, t, j] <- (X_scaled[, t, j] - mean_val) / sd_val
  }
}

# --- Treatment Assignment and Outcome Simulation ---
alcohol_avg <- apply(X_scaled[, , which(colnames(wine) == "alcohol")], 1, mean)
ph_avg <- apply(X_scaled[, , which(colnames(wine) == "pH")], 1, mean)

# Sharper logits for treatment assignment
logits <- cbind(3.5 * alcohol_avg, 3.5 * ph_avg, -3.5 * alcohol_avg)
exp_logits <- exp(logits)
prob_mat <- exp_logits / rowSums(exp_logits)
W <- apply(prob_mat, 1, function(p) sample(0:2, 1, prob = p))

# Simulate nonlinear heterogeneous treatment effects
base_outcome <- sin(apply(X_scaled[, , 1:3], 1, mean)) * 3 + rnorm(n, 0, 0.5)
tau0 <- 0
tau1 <- 1.5 + 2 * ph_avg - 1 * alcohol_avg
tau2 <- -2 + 3.5 * alcohol_avg - 0.5 * ph_avg

Y0 <- base_outcome + tau0
Y1 <- base_outcome + tau1
Y2 <- base_outcome + tau2

Y_obs <- ifelse(W == 0, Y0, ifelse(W == 1, Y1, Y2))

# --- Train/Test Split ---
train_idx <- sample(1:n, floor(0.7 * n))
test_idx <- setdiff(1:n, train_idx)

X_train <- X_scaled[train_idx, , ]
X_test <- X_scaled[test_idx, , ]
Y0_test <- Y0[test_idx]
Y1_test <- Y1[test_idx]
Y2_test <- Y2[test_idx]

# --- Quantum-inspired Sampling Weights ---
get_sampling_weights <- function(model, X_input) {
  q_vals <- predict(model, X_input)
  prob_qvals <- t(apply(q_vals, 1, softmax))
  entropy <- -rowSums(prob_qvals * log(prob_qvals + 1e-8))
  qvar <- apply(q_vals, 1, var)
  combined_uncertainty <- entropy + qvar
  weights <- combined_uncertainty / max(combined_uncertainty)
  return(weights)
}

# --- Softmax Utility ---
softmax <- function(x) {
  exp_x <- exp(x - max(x))
  exp_x / sum(exp_x)
}

# --- DQN Model Definition ---
build_model <- function(input_shape, output_dim = 3) {
  model <- keras_model_sequential() %>%
    layer_conv_1d(filters = 32, kernel_size = 3, activation = "relu", input_shape = input_shape) %>%
    layer_lstm(units = 32, return_sequences = FALSE) %>%
    layer_dense(units = 32, activation = "relu") %>%
    layer_dense(units = output_dim)
  model %>% compile(optimizer = optimizer_adam(learning_rate = alpha), loss = "mse")
  return(model)
}

# --- DQN Training Function ---
train_dqn <- function(X, Y0, Y1, Y2, idx, episodes, alpha,
                      gamma = 0.95,
                      epsilon_start = 1.0,
                      epsilon_decay = 0.99,
                      epsilon_min = 0.1,
                      weighted_sampling = FALSE) {
  n <- length(idx)
  epsilon <- epsilon_start
  model <- build_model(input_shape = dim(X)[2:3])
  reward_log <- numeric(episodes)
  
  for (ep in 1:episodes) {
    X_ep <- X[idx, , ]
    
    # Apply quantum-inspired sampling weights if requested
    if (weighted_sampling && ep > 1) {
      weights <- get_sampling_weights(model, X_ep)
      sample_idx <- sample(1:n, n, replace = TRUE, prob = weights)
    } else {
      sample_idx <- sample(1:n, n, replace = TRUE)
    }
    
    X_batch <- X_ep[sample_idx, , ]
    actions <- sample(0:2, n, replace = TRUE)
    
    # Generate true rewards
    Y <- ifelse(actions == 0, Y0, ifelse(actions == 1, Y1, Y2))[sample_idx]
    
    # Predict current Q-values
    Q_pred <- predict(model, X_batch)
    Q_target <- Q_pred
    
    for (i in 1:n) {
      a <- actions[i] + 1  # R is 1-indexed
      Q_target[i, a] <- Y[i]
    }
    
    history <- model %>% fit(X_batch, Q_target, epochs = 1, verbose = 0)
    reward_log[ep] <- mean(Y)
    
    epsilon <- max(epsilon * epsilon_decay, epsilon_min)
  }
  
  return(list(model = model, rewards = reward_log))
}

# --- Train both models ---
set.seed(123)

res_uniform <- train_dqn(X_train, Y0[train_idx], Y1[train_idx], Y2[train_idx],
                         idx = 1:nrow(X_train),
                         episodes = episodes,
                         alpha = alpha,
                         weighted_sampling = FALSE)
set.seed(123)

res_weighted <- train_dqn(X_train, Y0[train_idx], Y1[train_idx], Y2[train_idx],
                          idx = 1:nrow(X_train),
                          episodes = episodes,
                          alpha = alpha,
                          weighted_sampling = TRUE)


# --- Evaluation function ---
evaluate_model <- function(model, X_test, Y0_test, Y1_test, Y2_test) {
  q_preds <- model %>% predict(X_test, verbose = 0)
  policy_actions <- apply(q_preds, 1, which.max) - 1
  rewards_pred <- sapply(seq_along(policy_actions), function(i) {
    a <- policy_actions[i]
    if (a == 0) return(Y0_test[i])
    if (a == 1) return(Y1_test[i])
    if (a == 2) return(Y2_test[i])
  })
  
  list(
    total_reward = sum(rewards_pred),
    average_reward = mean(rewards_pred),
    action_distribution = table(policy_actions),
    rewards_pred = rewards_pred,
    policy_actions = policy_actions
  )
}

eval_uniform <- evaluate_model(res_uniform$model, X_test, Y0_test, Y1_test, Y2_test)
eval_weighted <- evaluate_model(res_weighted$model, X_test, Y0_test, Y1_test, Y2_test)

cat("Uniform Sampling Model Performance:\n")
print(eval_uniform[1:3])

cat("\nWeighted Sampling Model Performance:\n")
print(eval_weighted[1:3])

# --- Statistical Tests on training rewards ---
cat("\n--- Statistical Tests on Training Rewards ---\n")
t_test_res <- t.test(res_uniform$rewards, res_weighted$rewards, paired = TRUE)
cat("Paired t-test:\n")
print(t_test_res)

# --- Diagnostic Plots ---
df_rewards <- data.frame(
  Episode = rep(1:episodes, 2),
  Reward = c(res_uniform$rewards, res_weighted$rewards),
  Model = factor(rep(c("Uniform", "Weighted"), each = episodes))
)

ggplot(df_rewards, aes(x = Model, y = Reward, fill = Model)) +
  geom_boxplot(alpha = 0.7) +
  labs(title = "Boxplot of Rewards Per Episode by Model", y = "Reward") +
  theme_minimal()

df_test_rewards <- data.frame(
  Reward = c(eval_uniform$rewards_pred, eval_weighted$rewards_pred),
  Model = factor(rep(c("Uniform", "Weighted"), each = length(eval_uniform$rewards_pred)))
)

ggplot(df_test_rewards, aes(x = Reward, fill = Model)) +
  geom_density(alpha = 0.5) +
  labs(title = "Density Plot of Predicted Rewards on Test Set", x = "Reward", y = "Density") +
  theme_minimal()

bootstrap_cum_rewards <- function(rewards, R = 10000) {
  boot_obj <- boot(rewards, statistic = function(data, i) cumsum(data[i]), R = R)
  boot_ci_lower <- apply(boot_obj$t, 2, quantile, probs = 0.025)
  boot_ci_upper <- apply(boot_obj$t, 2, quantile, probs = 0.975)
  list(lower = boot_ci_lower, upper = boot_ci_upper)
}

ci_uniform <- bootstrap_cum_rewards(res_uniform$rewards)
ci_weighted <- bootstrap_cum_rewards(res_weighted$rewards)

df_compare <- data.frame(
  Episode = 1:episodes,
  Uniform = cumsum(res_uniform$rewards),
  Weighted = cumsum(res_weighted$rewards),
  Uniform_lower = ci_uniform$lower,
  Uniform_upper = ci_uniform$upper,
  Weighted_lower = ci_weighted$lower,
  Weighted_upper = ci_weighted$upper
)

df_long <- pivot_longer(df_compare, cols = c("Uniform", "Weighted"),
                        names_to = "Model", values_to = "CumulativeReward")

ggplot(df_long, aes(x = Episode, y = CumulativeReward, color = Model)) +
  geom_line(size = 1) +
  geom_ribbon(data = df_compare, aes(x = Episode, ymin = Uniform_lower, ymax = Uniform_upper),
              fill = "blue", alpha = 0.15, inherit.aes = FALSE) +
  geom_ribbon(data = df_compare, aes(x = Episode, ymin = Weighted_lower, ymax = Weighted_upper),
              fill = "red", alpha = 0.15, inherit.aes = FALSE) +
  labs(title = "Cumulative Reward with 95% CI by Model", y = "Cumulative Reward") +
  theme_minimal()

# --- Comparison Summary Table ---
comparison_table <- data.frame(
  Metric = c(
    "Total Reward (Test Set)",
    "Average Reward (Test Set)",
    "Training Reward Mean",
    "Training Reward SD",
    "Action 0 Freq (Test Set)",
    "Action 1 Freq (Test Set)",
    "Action 2 Freq (Test Set)"
  ),
  Uniform_Model = c(
    eval_uniform$total_reward,
    eval_uniform$average_reward,
    mean(res_uniform$rewards),
    sd(res_uniform$rewards),
    eval_uniform$action_distribution["0"],
    eval_uniform$action_distribution["1"],
    eval_uniform$action_distribution["2"]
  ),
  Weighted_Model = c(
    eval_weighted$total_reward,
    eval_weighted$average_reward,
    mean(res_weighted$rewards),
    sd(res_weighted$rewards),
    eval_weighted$action_distribution["0"],
    eval_weighted$action_distribution["1"],
    eval_weighted$action_distribution["2"]
  )
)

comparison_table[is.na(comparison_table)] <- 0
cat("\nTwo-Model Comparison Summary:\n")
print(comparison_table)

## Boston Housing
# --- Libraries ---
library(data.table)
library(keras)
library(tensorflow)
library(tidyverse)
library(boot)
library(MASS)  # For Boston dataset

# --- Parameters ---
T_steps <- 10
n <- 1000
episodes <- 30
alpha <- 0.001
epsilon_start <- 1.0
epsilon_decay <- 0.99
epsilon_min <- 0.1

# --- Load Boston Housing Dataset ---
data("Boston")
df <- Boston
p <- ncol(df)
set.seed(123)
df_sampled <- df[sample(1:nrow(df), n, replace = TRUE), ]

# Simulate time series structure by adding small noise per time step
X_long <- array(0, dim = c(n, T_steps, p))
for (t in 1:T_steps) {
  noise <- matrix(rnorm(n * p, 0, 0.01), nrow = n)
  X_long[, t, ] <- as.matrix(df_sampled) + noise
}

# Normalize each feature across samples and time
X_scaled <- X_long
for (j in 1:p) {
  for (t in 1:T_steps) {
    mean_val <- mean(X_scaled[, t, j])
    sd_val <- sd(X_scaled[, t, j])
    X_scaled[, t, j] <- (X_scaled[, t, j] - mean_val) / sd_val
  }
}

# --- Treatment Assignment and Outcome Simulation ---
rm_avg <- apply(X_scaled[, , which(colnames(df) == "rm")], 1, mean)
ptratio_avg <- apply(X_scaled[, , which(colnames(df) == "ptratio")], 1, mean)

# Sharper logits for treatment assignment
logits <- cbind(3.5 * rm_avg, 3.5 * ptratio_avg, -3.5 * rm_avg)
exp_logits <- exp(logits)
prob_mat <- exp_logits / rowSums(exp_logits)
W <- apply(prob_mat, 1, function(p) sample(0:2, 1, prob = p))

# Simulate nonlinear heterogeneous treatment effects
base_outcome <- sin(apply(X_scaled[, , 1:3], 1, mean)) * 3 + rnorm(n, 0, 0.5)
tau0 <- 0
tau1 <- 1.5 + 2 * ptratio_avg - 1 * rm_avg
tau2 <- -2 + 3.5 * rm_avg - 0.5 * ptratio_avg

Y0 <- base_outcome + tau0
Y1 <- base_outcome + tau1
Y2 <- base_outcome + tau2

Y_obs <- ifelse(W == 0, Y0, ifelse(W == 1, Y1, Y2))

# --- Train/Test Split ---
train_idx <- sample(1:n, floor(0.7 * n))
test_idx <- setdiff(1:n, train_idx)

X_train <- X_scaled[train_idx, , ]
X_test <- X_scaled[test_idx, , ]
Y0_test <- Y0[test_idx]
Y1_test <- Y1[test_idx]
Y2_test <- Y2[test_idx]

# --- Quantum-inspired Sampling Weights ---
get_sampling_weights <- function(model, X_input) {
  q_vals <- predict(model, X_input)
  prob_qvals <- t(apply(q_vals, 1, softmax))
  entropy <- -rowSums(prob_qvals * log(prob_qvals + 1e-8))
  qvar <- apply(q_vals, 1, var)
  combined_uncertainty <- entropy + qvar
  weights <- combined_uncertainty / max(combined_uncertainty)
  return(weights)
}

# --- Softmax Utility ---
softmax <- function(x) {
  exp_x <- exp(x - max(x))
  exp_x / sum(exp_x)
}

# --- DQN Model Definition ---
build_model <- function(input_shape, output_dim = 3) {
  model <- keras_model_sequential() %>%
    layer_conv_1d(filters = 32, kernel_size = 3, activation = "relu", input_shape = input_shape) %>%
    layer_lstm(units = 32, return_sequences = FALSE) %>%
    layer_dense(units = 32, activation = "relu") %>%
    layer_dense(units = output_dim)
  model %>% compile(optimizer = optimizer_adam(learning_rate = alpha), loss = "mse")
  return(model)
}

# --- DQN Training Function ---
train_dqn <- function(X, Y0, Y1, Y2, idx, episodes, alpha,
                      gamma = 0.95,
                      epsilon_start = 1.0,
                      epsilon_decay = 0.99,
                      epsilon_min = 0.1,
                      weighted_sampling = FALSE) {
  n <- length(idx)
  epsilon <- epsilon_start
  model <- build_model(input_shape = dim(X)[2:3])
  reward_log <- numeric(episodes)
  
  for (ep in 1:episodes) {
    X_ep <- X[idx, , ]
    
    # Apply quantum-inspired sampling weights if requested
    if (weighted_sampling && ep > 1) {
      weights <- get_sampling_weights(model, X_ep)
      sample_idx <- sample(1:n, n, replace = TRUE, prob = weights)
    } else {
      sample_idx <- sample(1:n, n, replace = TRUE)
    }
    
    X_batch <- X_ep[sample_idx, , ]
    actions <- sample(0:2, n, replace = TRUE)
    
    # Generate true rewards
    Y <- ifelse(actions == 0, Y0, ifelse(actions == 1, Y1, Y2))[sample_idx]
    
    # Predict current Q-values
    Q_pred <- predict(model, X_batch)
    Q_target <- Q_pred
    
    for (i in 1:n) {
      a <- actions[i] + 1  # R is 1-indexed
      Q_target[i, a] <- Y[i]
    }
    
    history <- model %>% fit(X_batch, Q_target, epochs = 1, verbose = 0)
    reward_log[ep] <- mean(Y)
    
    epsilon <- max(epsilon * epsilon_decay, epsilon_min)
  }
  
  return(list(model = model, rewards = reward_log))
}

# --- Train both models ---
set.seed(123)

res_uniform <- train_dqn(X_train, Y0[train_idx], Y1[train_idx], Y2[train_idx],
                         idx = 1:nrow(X_train),
                         episodes = episodes,
                         alpha = alpha,
                         weighted_sampling = FALSE)
set.seed(123)

res_weighted <- train_dqn(X_train, Y0[train_idx], Y1[train_idx], Y2[train_idx],
                          idx = 1:nrow(X_train),
                          episodes = episodes,
                          alpha = alpha,
                          weighted_sampling = TRUE)

# --- Evaluation ---
evaluate_model <- function(model, X_test, Y0_test, Y1_test, Y2_test) {
  q_preds <- model %>% predict(X_test, verbose = 0)
  policy_actions <- apply(q_preds, 1, which.max) - 1
  rewards_pred <- sapply(seq_along(policy_actions), function(i) {
    a <- policy_actions[i]
    if (a == 0) return(Y0_test[i])
    if (a == 1) return(Y1_test[i])
    if (a == 2) return(Y2_test[i])
  })
  
  list(
    total_reward = sum(rewards_pred),
    average_reward = mean(rewards_pred),
    action_distribution = table(policy_actions),
    rewards_pred = rewards_pred,
    policy_actions = policy_actions
  )
}

eval_uniform <- evaluate_model(res_uniform$model, X_test, Y0_test, Y1_test, Y2_test)
eval_weighted <- evaluate_model(res_weighted$model, X_test, Y0_test, Y1_test, Y2_test)

cat("Uniform Sampling Model Performance:\n")
print(eval_uniform[1:3])

cat("\nWeighted Sampling Model Performance:\n")
print(eval_weighted[1:3])

# --- Statistical Tests ---
cat("\n--- Statistical Tests on Training Rewards ---\n")
t_test_res <- t.test(res_uniform$rewards, res_weighted$rewards, paired = TRUE)
cat("Paired t-test:\n"); print(t_test_res)

# --- Diagnostic Plots ---
df_rewards <- data.frame(
  Episode = rep(1:episodes, 2),
  Reward = c(res_uniform$rewards, res_weighted$rewards),
  Model = factor(rep(c("Uniform", "Weighted"), each = episodes))
)

ggplot(df_rewards, aes(x = Model, y = Reward, fill = Model)) +
  geom_boxplot(alpha = 0.7) +
  labs(title = "Boxplot of Rewards Per Episode by Model", y = "Reward") +
  theme_minimal()

df_test_rewards <- data.frame(
  Reward = c(eval_uniform$rewards_pred, eval_weighted$rewards_pred),
  Model = factor(rep(c("Uniform", "Weighted"), each = length(eval_uniform$rewards_pred)))
)

ggplot(df_test_rewards, aes(x = Reward, fill = Model)) +
  geom_density(alpha = 0.5) +
  labs(title = "Density Plot of Predicted Rewards on Test Set", x = "Reward", y = "Density") +
  theme_minimal()

bootstrap_cum_rewards <- function(rewards, R = 1000) {
  boot_obj <- boot(rewards, statistic = function(data, i) cumsum(data[i]), R = R)
  boot_ci_lower <- apply(boot_obj$t, 2, quantile, probs = 0.025)
  boot_ci_upper <- apply(boot_obj$t, 2, quantile, probs = 0.975)
  list(lower = boot_ci_lower, upper = boot_ci_upper)
}

ci_uniform <- bootstrap_cum_rewards(res_uniform$rewards)
ci_weighted <- bootstrap_cum_rewards(res_weighted$rewards)

df_compare <- data.frame(
  Episode = 1:episodes,
  Uniform = cumsum(res_uniform$rewards),
  Weighted = cumsum(res_weighted$rewards),
  Uniform_lower = ci_uniform$lower,
  Uniform_upper = ci_uniform$upper,
  Weighted_lower = ci_weighted$lower,
  Weighted_upper = ci_weighted$upper
)

df_long <- pivot_longer(df_compare, cols = c("Uniform", "Weighted"),
                        names_to = "Model", values_to = "CumulativeReward")

ggplot(df_long, aes(x = Episode, y = CumulativeReward, color = Model)) +
  geom_line(size = 1) +
  geom_ribbon(data = df_compare, aes(x = Episode, ymin = Uniform_lower, ymax = Uniform_upper),
              fill = "blue", alpha = 0.15, inherit.aes = FALSE) +
  geom_ribbon(data = df_compare, aes(x = Episode, ymin = Weighted_lower, ymax = Weighted_upper),
              fill = "red", alpha = 0.15, inherit.aes = FALSE) +
  labs(title = "Cumulative Reward with 95% CI by Model", y = "Cumulative Reward") +
  theme_minimal()

# --- Summary Table ---
comparison_table <- data.frame(
  Metric = c(
    "Total Reward (Test Set)",
    "Average Reward (Test Set)",
    "Training Reward Mean",
    "Training Reward SD",
    "Action 0 Freq (Test Set)",
    "Action 1 Freq (Test Set)",
    "Action 2 Freq (Test Set)"
  ),
  Uniform_Model = c(
    eval_uniform$total_reward,
    eval_uniform$average_reward,
    mean(res_uniform$rewards),
    sd(res_uniform$rewards),
    eval_uniform$action_distribution["0"],
    eval_uniform$action_distribution["1"],
    eval_uniform$action_distribution["2"]
  ),
  Weighted_Model = c(
    eval_weighted$total_reward,
    eval_weighted$average_reward,
    mean(res_weighted$rewards),
    sd(res_weighted$rewards),
    eval_weighted$action_distribution["0"],
    eval_weighted$action_distribution["1"],
    eval_weighted$action_distribution["2"]
  )
)
comparison_table[is.na(comparison_table)] <- 0
cat("\nTwo-Model Comparison Summary:\n")
print(comparison_table)
