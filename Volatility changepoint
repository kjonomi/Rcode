## Applied Economics Letters (under review) 
## Bitcoin 5 minutes
# Required libraries
library(fda)
library(fdapace)
library(sde)
library(fda)
library(fdapace)
library(MASS)
library(ggplot2)
library(copula)
library(fGarch)
library(CompGLM)  # For Conway-Maxwell-Poisson distribution

# Step 1: Read CSV data (replace path if needed)
df <- read.csv("BIT5min.csv", stringsAsFactors = FALSE)

# Step 2: Convert time to POSIXct
df$Time <- as.POSIXct(df$Time, format = "%Y-%m-%d %H:%M:%OS", tz = "UTC")

# Step 3: Choose column - use either Buys or Sells
count_series <- df$Buys  # Or df$Sells
count_series <- df$Sells

# Step 4: Determine number of days and intervals (assuming 283 intervals per day)
L <- 283
N <- floor(length(count_series) / L)
h<-1
p <- 2                   # Number of functional principal components
nbasis <- 200            # Number of B-spline basis functions
lambda <- 1e-4           # Smoothing parameter

# Step 5: Reshape into N x L matrix
count_matrix <- matrix(count_series[1:(L*N)], nrow = N, byrow = TRUE)


# Step 6: Compute raw differences
y <- t(apply(count_matrix, 1, function(x) diff(x)))  # N x (L-1) matrix


# Copula-based Functional GARCH modeling function
copula_fgarch_estimation <- function(y, p, L, N, lambda, nbasis, model_type = "poisson") {
  sigma_out_squared = matrix(0, N-2, L)
  basis = create.bspline.basis(rangeval = c(0,1), nbasis = nbasis, norder = 5)
  fd_par_obj = fdPar(basis, lambda = lambda, Lfdobj = 2)
  log_likelihoods = numeric(N-2)
  
  for (out in 2:(N-1)) {
    y_current = y[-out, , drop = FALSE]
    y_sq_fdsmooth = smooth.basis(argvals = seq(0, 1, length.out = L), 
                                 y = matrix(t(y_current^2), nrow = L), 
                                 fdParobj = fd_par_obj)
    
    functional_y_squared = y_sq_fdsmooth$fd
    functional_y_squared_matrix = functional_y_squared$coefs
    
    # Perform Functional PCA
    functional_y_squared_recreated = fd(coef = functional_y_squared_matrix, basisobj = basis)
    principal_components = pca.fd(fdobj = functional_y_squared_recreated, nharm = p)
    eigenfunctions = principal_components$harmonics
    mean_fd = mean.fd(functional_y_squared_recreated)
    
    # Compute functional principal component scores
    Z = matrix(0, p, ncol(functional_y_squared_matrix))
    for (i in 1:p) {
      Z[i, ] = sapply(1:ncol(functional_y_squared_matrix), function(k) {
        if (k > ncol(functional_y_squared_matrix)) return(NA)
        inprod(fd(coef = functional_y_squared_matrix[, k, drop = FALSE], basisobj = basis), eigenfunctions[i]) - 
          inprod(mean_fd, eigenfunctions[i])
      })
    }
    Z = Z[, colSums(is.na(Z)) == 0]  # Remove NA values
    
    # Apply Copula-based modeling
    if (ncol(Z) > 1) {
      u_data = pobs(t(Z))  # Transform to pseudo-observations (Uniform [0,1] margins)
      
      # Fit Gaussian and Clayton Copulas
      fit_gaussian = fitCopula(normalCopula(dim = p), u_data, method = "ml")
      fit_clayton = fitCopula(claytonCopula(dim = p), u_data, method = "ml")
      emp_copula = empCopula(u_data)
      
      # Simulate from fitted copulas
      sim_gaussian = rCopula(ncol(Z), fit_gaussian@copula)
      sim_clayton = rCopula(ncol(Z), fit_clayton@copula)
      sim_empirical = rCopula(ncol(Z), emp_copula)
      
      # Combine copula-simulated values
      combined_copula_effect = rowMeans(cbind(sim_gaussian[,1], sim_clayton[,1], sim_empirical[,1]))
      
      # Apply GARCH model based on model type
      garch_model <- if (model_type == "poisson") {
        garchFit(~ garch(1,1), data = count_data[, out-1], trace = FALSE)
      } else if (model_type == "nb") {
        garchFit(~ garch(1,1), data = count_data[, out-1] + rnbinom(L, size=10, mu=10), trace = FALSE)
      } else {
        garchFit(~ garch(1,1), data = count_data[, out-1] + rcomp(L, lam = 10, nu = 1.5, sumTo = 100L), trace = FALSE)
      }
      garch_volatility = fitted(garch_model)
      log_likelihoods[out-1] = garch_model@fit$llh  # Extract log-likelihood correctly
      
      # Fix dimension mismatch
      if (length(combined_copula_effect) != L) {
        combined_copula_effect = rep(combined_copula_effect, length.out = L)
      }
      
      # Compute estimated volatility using copula-adjusted GARCH scores
      estimated_delta = eval.fd(seq(0, 1, length.out = L), mean_fd) * 
        rep_len(combined_copula_effect, L) * 
        rep_len(garch_volatility, L)
      sigma_out_squared[out-1, ] = estimated_delta
    }
  }
  aic = -2 * sum(log_likelihoods) + 2 * length(log_likelihoods)
  bic = -2 * sum(log_likelihoods) + length(log_likelihoods) * log(N-2)
  return(list(sigma_out_squared = sigma_out_squared, AIC = aic, BIC = bic, logLik = sum(log_likelihoods)))
}

# Apply models without log-transformed data
results_poisson = copula_fgarch_estimation(y, p, L, N, lambda, nbasis, model_type = "poisson")
results_nb = copula_fgarch_estimation(y, p, L, N, lambda, nbasis, model_type = "nb")
results_cmp = copula_fgarch_estimation(y, p, L, N, lambda, nbasis, model_type = "cmp")

# Compare AIC, BIC, and Log-Likelihood
comparison_df = data.frame(Model = c("Poisson", "Negative Binomial", "Conway-Maxwell-Poisson"),
                           AIC = c(results_poisson$AIC, results_nb$AIC, results_cmp$AIC),
                           BIC = c(results_poisson$BIC, results_nb$BIC, results_cmp$BIC),
                           LogLikelihood = c(results_poisson$logLik, results_nb$logLik, results_cmp$logLik))
print(comparison_df)

# Load necessary libraries
library(ggplot2)
library(changepoint)

# ---------- Function: 2-Sigma Control Chart with Out-of-Control Count ----------
control_chart_2sigma <- function(estimated_volatility, dist_name, changepoints_list = NULL) {
  mean_volatility <- mean(estimated_volatility)
  sd_volatility <- sd(estimated_volatility)
  
  upper_limit <- mean_volatility + 2 * sd_volatility
  lower_limit <- mean_volatility - 2 * sd_volatility
  
  df <- data.frame(
    Time = seq_along(estimated_volatility),
    Volatility = estimated_volatility,
    OutOfControl = estimated_volatility > upper_limit | estimated_volatility < lower_limit
  )
  
  out_of_control_count <- sum(df$OutOfControl)
  
  p <- ggplot(df, aes(x = Time, y = Volatility)) +
    geom_line(color = "blue") +
    geom_point(data = subset(df, OutOfControl), aes(x = Time, y = Volatility), color = "orange", size = 1.5) +
    geom_hline(yintercept = mean_volatility, linetype = "dashed", color = "red") +
    geom_hline(yintercept = upper_limit, linetype = "dashed", color = "green") +
    geom_hline(yintercept = lower_limit, linetype = "dashed", color = "green") +
    annotate("text", x = Inf, y = Inf, label = paste("Out-of-control points:", out_of_control_count),
             hjust = 1.1, vjust = 2, size = 4, color = "black", fontface = "bold") +
    labs(title = paste("2-Sigma Control Chart for", dist_name, "Distribution"),
         x = "Time", y = "Volatility") +
    theme_minimal()
  
  # Add text labels on out-of-control points (showing their indices)
  p <- p + geom_text(data = subset(df, OutOfControl), aes(x = Time, y = Volatility, label = Time), 
                     color = "orange", size = 3, vjust = -0.5, fontface = "bold")
  
  # Add changepoints if provided
  if (!is.null(changepoints_list)) {
    method_colors <- c(PELT = "purple", BinSeg = "darkred", SegNeigh = "brown")
    for (method in names(changepoints_list)) {
      cps <- changepoints_list[[method]]
      for (cp in cps) {
        p <- p + geom_vline(xintercept = cp, linetype = "dotted", color = method_colors[method], alpha = 0.7)
      }
    }
  }
  
  print(p)
  
  return(list(upper_limit = upper_limit, lower_limit = lower_limit, data = df))
}

# ---------- Function: ARL Calculation ----------
calculate_arl <- function(estimated_volatility, upper_limit, lower_limit) {
  out_of_control_points <- which(estimated_volatility > upper_limit | estimated_volatility < lower_limit)
  arl_values <- diff(c(0, out_of_control_points))
  arl_mean <- mean(arl_values)
  arl_sd <- sd(arl_values)
  return(list(ARL_Mean = arl_mean, ARL_SD = arl_sd))
}

# ---------- Function: Multi-Method Changepoint Detection ----------
changepoint_detection_all <- function(series, Q = 5) {
  methods <- list(
    PELT = cpt.meanvar(series, method = "PELT"),
    BinSeg = cpt.meanvar(series, method = "BinSeg", Q = Q),
    SegNeigh = suppressWarnings(cpt.meanvar(series, method = "SegNeigh", Q = Q, penalty = "BIC"))
  )
  result <- lapply(methods, function(x) cpts(x))
  return(result)
}

# ---------- Function: Run Complete Pipeline ----------
run_pipeline <- function(sigma_matrix, dist_name) {
  N <- nrow(sigma_matrix)
  volatility <- sigma_matrix[N - 2, ]
  
  changepoints <- changepoint_detection_all(volatility)
  control_output <- control_chart_2sigma(volatility, dist_name, changepoints)
  arl <- calculate_arl(volatility, control_output$upper_limit, control_output$lower_limit)
  
  cat("\n=========== Results for", dist_name, "============\n")
  print(arl)
  cat("Changepoints - PELT:\n"); print(changepoints$PELT)
  cat("Changepoints - BinSeg:\n"); print(changepoints$BinSeg)
  cat("Changepoints - SegNeigh:\n"); print(changepoints$SegNeigh)
}

# ---------- Apply to All Three Distributions ----------
# Assumes these three result objects are already loaded in your environment:
# - results_poisson
# - results_nb
# - results_cmp

run_pipeline(results_poisson$sigma_out_squared, "Poisson")
run_pipeline(results_nb$sigma_out_squared, "Negative Binomial")
run_pipeline(results_cmp$sigma_out_squared, "COM-Poisson")
